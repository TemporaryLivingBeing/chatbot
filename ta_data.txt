URL: https://www.furman.edu/academics/mathematics/program-overview/data-analytics-minor/
The Data Analytics interdisciplinary minor consists of 20 credit hours typically split between three required foundational courses and two electives chosen from a list representing ten different departments.

REQUIRED COURSES
12 Credits
The three foundational courses offer students experience in statistics, introduction to programming in R and Python, and exposure to applications of analytics techniques in a variety of real-world settings.

CSC-121 Introduction to Computer Programming
CSC-272 Introduction to Data Mining
MTH-245 Statistical Methods I with R
At a minimum, students will need a background in Statistics (BIO-222, ECN-120, HSC-201, MTH-120, PSY-202, or SOC-302) for MTH-245.

 

ELECTIVE COURSES: DOMAIN-SPECIFIC APPROACHES
8 credit hours, at least 4 credit hours must be from a course not labeled MTH or CSC
ACC-350 Accounting Information Systems
BIO-222 Research and Analysis
BUS-230 Introduction to Data Analysis
BUS-337 Intermediate Business Analytics
CHM-240 Experimental Techniques
CHM-330 Analytical Chemistry
CSC-341 Database Management Systems
CSC-343 Artificial Intelligence
CSC-372 Machine Learning with Big Data
ECN-331 Empirical Methods in Economics
ECN-475 Senior Seminar in Economics (by approval of the oversight committee)
EES-201 Geographic Information Systems
EES-301 Remote Sensing of the Environment
HSC-201 Research and Evaluation in Health Sciences
HSC-401 Epidemiology
HST-322 Simulating Historic Communities in Virtual Space
MTH-335 Mathematical Models and Applications
MTH-337 Operations Research
MTH-340 Probability
MTH-341 Mathematical Statistics
MTH-345 Statistical Methods II with R
PSY-202 Research Methods and Statistics II
SOC-303 Quantitative Research Seminar
SOC-470 Qualitative Research Seminar
SUS-242 Dynamic Systems Modeling
INDEPENDENT STUDY ELECTIVES AND SUMMER RESEARCH EXPERIENCES
Independent Study, Independent Research, and Directed Independent Study (Courses numbered 501, 502, and 504) may be used as electives with approval of the oversight committee. Likewise, directed summer research experiences (TFA-002) may be considered for use as electives with approval, along with the submission of a substantive research paper to be reviewed by the committee. Faculty-supervised independent study and research offer excellent learning opportunities and should be encouraged when the work is empirical in nature, directly involving data analytics if possible, and congruent with the objectives for the electives above.

 

FURMAN ENGAGED
For successful completion of the minor, students must give a presentation at Furman Engaged. This may take the form of an oral presentation or a poster, and should detail a significant data analytics project undertaken by the student in the course of their minor studies.

 

DATA ANALYTICS MINOR OVERSIGHT COMMITTEE
To declare this minor, contact the Data Analytics Minor Chair, Dr. Kevin Hutson, or contact any member of the Data Analytics Minor Oversight Committee to discuss this minor:

Dr. Chris Alvin (Computer Science)
Dr. Liz Bouzarth (Mathematics)
Dr. Roy Bower (Mathematics)
Dr. Kevin Hutson (Mathematics), Chair
Dr. Taha Kasim (Economics)
Dr. Marion McHugh (Business and Accounting)
Dr. John Quinn (Biology)
Dr. Fahad Sultan (Computer Science)
Dr. Kevin Treu(Computer Science. He is the founder of the data analytics minor.)


URL: https://fahadsultan.com/csc272/index.html#


CSC-272 Data Mining (Fall 2024)

Hi! Welcome to the CSC-272 Data Mining course website 👋🏾

I am excited to talk math 🔢, programming 💻 and all-things data 📊 in this course with you!

👈🏾 From the sidebar on the left, you should be able to navigate to any topic relevant to the course. If that does not help, there should also be a search icon 🔍 in the top right corner ↗️

👇🏾 Below, you can find important links and important announcements.

Important

Final Exam is on Friday, December 8th at 8:30 AM in class

Important

Final Project Report is due before Final exam Friday, Dec 8th, 8:30 AM

Tip

Drop by my office: Riley Hall 200-D. I am in my office between 9 AM - 5 PM most weekdays and have an open-door policy! Alternatively, send me an email or schedule an appointment

See also

Course page on Moodle; used mostly for submitting assignments and uploading grades



URL: https://fahadsultan.com/csc272/calendar.html


Calendar

Note

Please note that this is a tentative plan and is subject to change.




#

	

TOPIC

	

READING

	

MEETING 1

	

MEETING 2

	

WRITTEN ASSIGNMENT

	

PROGRAMMING ASSIGNMENT




1

	

Vectorized Operations

	

Skiena Ch.1

	

Introduction

	

Vectorized Operations


	

	

PA 1





2

	

Vectorized Operations

	

McKinney Ch.5, 8, 10

	

Pandas I

	

Pandas II

	

WA 1

	

PA 2
PA 3





3

	

Vectorized Operations

	

McKinney Ch.5, 8, 10

	

Pandas III

	

	

WA 2

	

PA 4




4

	

Encoding and Representation

	

McKinney Ch.7

	

Feature Types

	

Exam 1

	

	

Project phase 1 due
EXAM 1




#

	

TOPIC

	

READING

	

MEETING 1

	

MEETING 2

	

WRITTEN ASSIGNMENT

	

PROGRAMMING ASSIGNMENT




5

	

Encoding and Representation

	

Skiena Ch. 3 (3.1.2, 3.3)

	

Modalities

	

Formats

	

	

PA 5




6

	

Data Visualization

	

Skiena Ch.6

	

Univariate

	

Multivariate
+ Dos and Donts

	

	

PA 6




7

	

Probability

	

Skiena Ch.2

	

Probability

	

	

WA 3

	




8

	

Stats + Logs

	

Skiena Ch.2

	

Statistics

	

Logarithms

	

	

PA 7

Project phase 2 due
EXAM 2




#

	

TOPIC

	

READING

	

MEETING 1

	

MEETING 2

	

WRITTEN ASSIGNMENT

	

PROGRAMMING ASSIGNMENT




9

	

Classification
+ Naive Bayes

	

SLP 3rd edition Ch.4

	

Supervised Learning

	

Classification + Naive Bayes

	

	

PA 8




10

	

Linear Algebra + Nearest Neighbor

	

Skiena Ch. 8 and Ch. 10

	

Vectors

	

Matrices

	

	

PA 9

Project phase 3 due
EXAM 3




#

	

TOPIC

	

READING

	

MEETING 1

	

MEETING 2

	

WRITTEN ASSIGNMENT

	

PROGRAMMING ASSIGNMENT




12

	

Clustering

	

Skiena Ch.10
(10.5)

	

DBSCAN

	

K-Means

	

	




13

	

Regression

	

Skiena Ch.9
(9.1-9.5)

	

Regression

	

	

	

Project phase 4 due
FINAL EXAM



URL: https://fahadsultan.com/csc272/syllabus/index.html


Syllabus
Pre-requisites

CSC-121 Introduction to Computer Programming

Fulfills requirements

Core course requirement for Data Analytics minor

Human Behavior (HB) General Education Requirement (GER)

Course Description

This course focuses on the algorithms and computing tools fundamental to data science: the process of extracting accurate and generalizable models from data via machine learning. Topics will include the prediction of outcomes, the discovery of associations, and the identification of similar groups. Students will complete a project related to human behavior, starting with data collection and cleaning, culminating in the presentation of a model and visualization of results.

Course Goals

On successful completion of the course, the students should have the ability to identify and apply appropriate data mining and/or machine learning techniques towards solving problems of pattern recogition, learning and prediction. The course also aims to instill in students a deep sensitivity of issues of algorithmic bias and fairness in data mining.



URL: https://fahadsultan.com/csc272/syllabus/grading.html


Grading
Grade Breakdown

Component

	

Percentage




Project

	

30%




Exam 1

	

10%




Exam 2

	

10%




Exam 3

	

10%




Exam 4

	

10%




Written Assignments

	

10%




Programming Assignments

	

10%




Class Participation

	

10%

Grading Scale

(+/- at instructor’s discretion)

Letter Grade

	

Range




A

	

> 90 %




B

	

80 - 90 %




C

	

70 - 80 %




D

	

60 - 70 %




F

	

< 60 %

Minimum Requirements

In order to pass this class, you must

Earn a passing grade

Submit at least 80% of assignments and labs

Take all exams.

Meet the minimum requirements for the course project.

Simply, you cannot blow off an entire aspect of the course and pass this class! Note that this basic requirement is necessary but not sufficient to pass the class.



URL: https://fahadsultan.com/csc272/syllabus/textbook.html


Textbooks

The Data Science Design Manual by Steven S. Skiena

Python for Data Analysis by Wes McKinney (Free, Open Access)

Other Recommended (but advanced) Textbooks

Mining of Massive Datasets by Jure Leskovec, Anand Rajaraman, Jeffrey D. Ullman

Probabilistic Programming & Bayesian Methods for Hackers by Cameron Davidson-Pilon

Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani ]

Other Resources

Jupyter Notebook User Guide

Python Data Science Handbook by Jake VanderPlas

Python: How to Think Like a Computer Scientist (Swarthmore Edition) by Jeffrey Elkner, Allen B. Downey, and Chris Meyers (free, open textbook)



URL: https://fahadsultan.com/csc272/syllabus/mental_health.html


Mental Health
The Counseling Center

Empowering and equipping students to manage their mental health and academic success, the Counseling Center’s stepped care model offers an array of evidence based services.

The resources listed below are free, confidential and accessible to all enrolled students. Go to the Counseling Center Website for details.

Counseling On Demand

Furman University Counseling Center Mental Health and Crisis Support Line – Call the Counseling Center at 864-294-3031, press #3 (confidential, available 24/7/365 from anywhere).

Self Help

Headspace – a mindfulness app that helps decrease stress and improve focus and mind-wandering, sponsored by SGA and PHOKUS. Students may enroll using their Furman email.

TAO Connect – a self-help platform (anonymous and confidential, 24/7) sponsored by the Counseling Center and accessible to students, faculty and staff. Enroll with a Furman email.

Peer Support

Paladin Peer Support is a student peer mentoring organization focused on wellness and self-efficacy. Follow them on Instagram and connect for support in reaching personal well-being goals.

Skill Building Groups and Workshops

Rotating evidence-based psycho-education and skill building groups for anxiety and emotional regulation

Consultation and Treatment Services

Start Strong and Finish Strong Walk-in Clinics (first and last two weeks of every semester)

Brief individual counseling (in person and online), which may include psychiatric and nutrition consults where clinically indicated.

Single Session Consultations

Group Counseling and Skill Building Workshops

Spiritual Life

The Office for Spiritual Life provides individual confidential counseling for students, faculty and staff in person and online

Groups and workshops that are theme-focused and interpersonal

Contact OSL@furman.edu, 864-294-2133, or contact a chaplain directly: vaughn.crowetipton@furman.edu, kate.taber@furman.edu.



URL: https://fahadsultan.com/csc272/syllabus/academic_success.html


Center for Academic Success

Peer Tutors are available free of charge for many classes and may be requested by dropping by CAS (LIB 002) or on the Center for Academic Success website. Tutors are typically recommended by faculty and have performed well in the class.


The Writing & Media Lab (WML) is staffed by student Consultants who are trained to help you improve your writing and multimodal communication skills. The consultation process is non-directive and intended to allow students to maintain ownership of their work. In addition to helping with the nuts and bolts, WML Consultants also support you in developing your own ideas thoughtfully and critically, whether you’re writing an essay or planning a video or other multimedia project. You may drop into the WML during its regular hours (LIB 002; 9 AM to 10 PM) or visit the **Writing and Media Lab website to make an appointment online.

Professional Academic Assistance Staff in CAS can provide students assistance with time management, study skills, and organizational skills.

The Writing and ESL Specialist provides professional writing support as well as support for students whose primary language is not English.



URL: https://fahadsultan.com/csc272/syllabus/accomodations.html


Accomodations

Furman University recognizes a student with a disability as anyone whose impairment substantially limits one or more major life activity. Students may receive a variety of services including classroom accommodations such as extended time on tests, test proctoring, note-taking assistance and access to assistive technology. However, receipt of reasonable accommodations cannot guarantee success–all students are responsible for meeting academic standards. Students with a diagnosed disability may be entitled to accommodations under the Americans with Disabilities Act (ADA).



Please visit Student Office for Accessibility Resources for more info.



URL: https://fahadsultan.com/csc272/syllabus/integrity.html


Academic Integrity

Academic Integrity standards are important to our Furman community and will be upheld in this class. Students should review the Academic Integrity Pledge and other resources available on the Academic Integrity page on the Furman website. Pay special attention to definitions of cheating, plagiarism, unacceptable collaboration, facilitating misconduct and other types of misrepresentation. All those apply in this course.

For programming assignments/homeworks and labs, follow the 50 foot policy in its spirit.

Fig. 1 50 foot policy

In this class, the grade penalty for an academic integrity violation is an F for the course. Academic Discipline procedures will be followed through the Office of the Academic Dean.



URL: https://fahadsultan.com/csc272/syllabus/title_9.html


Nondiscrimination Policy and Sexual Misconduct

Furman University and its faculty are committed to supporting our students and seeking an environment that is free of bias, discrimination, and harassment. Furman does not unlawfully discriminate on the basis of race, color, national origin, sex, sexual orientation, gender identity, pregnancy, disability, age, religion, veteran status, or any other characteristic or status protected by applicable local, state, or federal law in admission, treatment, or access to, or employment in, its programs and activities.

If you have encountered any form of discrimination or harassment, including sexual misconduct (e.g. sexual assault, sexual harassment or gender-based harassment, sexual exploitation or intimidation, stalking, intimate partner violence), we encourage you to report this to the institution. If you wish to report such an incident of misconduct, you may contact Furman’s Title IX Coordinator, Melissa Nichols (Trone Center, Suite 215; Melissa.nichols@furman.edu; 864.294.2221).

If you would like to speak with someone who can advise you but maintain complete confidentiality, you can talk with a counselor, a professional in the Student Health Center or someone in the Office of Spiritual Life. If you speak with a faculty member, understand that as a “Responsible Employee” of the University, the faculty member MUST report to the University’s Title IX Coordinator what you share to help ensure that your safety and welfare are being addressed, consistent with the requirements of the law.

Additional information about Furman’s Sexual Misconduct Policy, how to report sexual misconduct and your rights can be found at the Furman Title IX Webpage. You do not have to go through the experience alone.



URL: https://fahadsultan.com/csc272/project/project.html


Course Project

In this course, you are expected to plan, execute, and present the results of a semester-long group project of your choosing.

Take the time to investigate multiple options during the proposal phase. You should have gotten some preliminary results by then, enough to provide confidence you will be able to successfully complete the project.

Groups

Working alone on the project is strongly discouraged. Group sizes should range from two to three students. It is possible for multiple groups working on the same data set. However, such groups must work independently and are not allowed to share code or results. Each dataset leaves enough room to pursue different directions so I expect to see variety among the submissions from each group.

To ensure consistent progress, there is a project deadline roughly every month, before each of the four exams. This will allow you to apply the techniques you learn in class to your project in a timely manner.

Data Science Lifecycle

The steps of your project are to mirror the general data science lifecycle pipeline.

The data science lifecycle is a high-level overview of the data science workflow. It’s a cycle of stages that a data scientist should explore as they conduct a thorough analysis of a data-driven problem.

There are many variations of the key ideas present in the data science lifecycle. Here, we visualize the stages of the lifecycle using a flow diagram.

Fig. 2 Data Science Lifecycle

1. Ask a Question and Obtain Data

Important

Due before Exam 1

Whether by curiosity or necessity, data scientists will constantly ask questions. For example, in the business world, data scientists may be interested in predicting the profit generated by a certain investment. In the field of medicine, they may ask whether some patients are more likely than others to benefit from a treatment.

Posing questions is one of the primary ways the data science lifecycle begins. It helps to fully define the question. Here are some things you should ask yourself before framing a question.

What do we want to know?

A question that is too ambiguous may lead to confusion.

What problems are we trying to solve?

The goal of asking a question should be clear in order to justify your efforts to stakeholders.

What are the hypotheses we want to test?

This gives a clear perspective from which to analyze final results.

What are the metrics for our success?

This gives a clear point to know when to finish the project.

	




	




1. Ask a Question

	

2. Obtain Data

The second entry point to the lifecycle is by obtaining data. A careful analysis of any problem requires the use of data. Data may be readily available to us, or we may have to embark on a process to collect it. When doing so, its crucial to ask the following:

What data do we have and what data do we need?

Define the units of the data (people, cities, points in time, etc.) and what features to measure.

How will we sample more data?

Scrape the web, collect manually, etc.

Is our data representative of the population we want to study?

If our data is not representative of our population of interest, then we can come to incorrect conclusions.

Project Phase 1

Identify or collect a publicly available data set that you will use for your project. A list of possible datasets and project ideas are provided here but you are allowed to propose your own project and data set. Just be mindful of the scale of the data: it should be large enough to be appropriate for usage in Machine Learning, but not so large that it is unwieldy.

Key procedures: Data Acquisition, Data Cleaning

2. Understand the Data

Important

Due before Exam 2

Raw data itself is not inherently useful. It’s impossible to discern all the patterns and relationships between variables without carefully investigating them. Therefore, translating pure data to actionable insights is a key job of a data scientist. For example, we may choose to ask:

How is our data organized and what does it contain?

Knowing what the data says about the world helps us better understand the world.

Do we have relevant data?

If the data we have collected is not useful to the question at hand, then we must collected more data.

What are the biases, anomalies, or other issues with the data?

These can lead to many false conclusions if ignored, so data scientists must always be aware of these issues.

How do we transform the data to enable effective analysis?

Data is not always easy to interpret at first glance, so a data scientist should reveal these hidden insights.

Project Phase 2

Perform some exploratory analysis to help you get acquainted with the data. This may include data visualization and basic preliminary analysis. Identify interesting aspects of the data set that would be useful for downstream prediction: correlations, outliers, missing values, etc.

Key procedures: Exploratory data analysis, Data visualization.

3. Understand the World 1

Important

Due before Exam 3

After observing the patterns in our data, we can begin answering our question. This may require that we predict a quantity (machine learning), or measure the effect of some treatment (inference).

From here, we may choose to report our results, or possibly conduct more analysis. We may not be satisfied by our findings, or our initial exploration may have brought up new questions that require a new data.

What does the data say about the world?

Given our models, the data will lead us to certain conclusions about the real world.

Does it answer our questions or accurately solve the problem?

If our model and data can not accomplish our goals, then we must reform our question, model, or both.

How robust are our conclusions and can we trust the predictions?

Inaccurate models can lead to untrue conclusions.

Project Phase 3

Preprocess data to change raw feature vectors into a representation that is more suitable for the downstream analysis. This may include data cleaning, calculating derivative or second-order variables, feature extraction, and feature selection.

Implement baseline models covered in class and report their performance.

Key procedures: Model Creation, Prediction, Inference.

4. Understand the World 2

Important

Due before Exam 4 (Finals)

Project Phase 4

Identify, implement and apply as many models as relevant from class to predict some aspect of the data. This must be a supervised learning model. This may include, model selection, and model evaluation.

You must compare your results to a number of baselines, including random predictor, major class classifier and/or autocorrelation model. An example table is shown below:

Model

	

Accuracy

	

Precision

	

Recall

	

F1-score




Random baseline

	

0.5

	

0.52

	

0.55

	

0.53




Majority class / Autocorrelation

	

0.75

	

0.5

	

1

	

0.66




Model 1

	

0.77

	

0.72

	

0.74

	

0.73




Model 2

	

0.85

	

0.79

	

0.89

	

0.88

Discuss the results of your analysis. This must include stating the assumptions of the model, the limitations of the model, a thorough error analysis and future directions.

Key procedures: Model Creation, Prediction, Inference, Model Selection, Error Analysis.



URL: https://fahadsultan.com/csc272/project/data.html


Data sets and Project ideas

Following data sets and ideas are only there to give you a starting point. You are free to propose a data set or project idea not listed here.

Tip

Google’s Dataset Search, Kaggle and Awesome datasets are good places to look for data sets

Data sets

Movies: i) Scripts data ii) Subtitles data iii) IMDB Dataset

Music: i) Million Song Dataset ii) Last.fm Dataset iii) Spotify Dataset iv) Lyrics data

TV series: i) TV Series Dataset ii) Subtitles data iii) IMDB Dataset

Books: i) Goodreads Dataset ii) Book Reviews Dataset iii) Book Summaries Dataset

Socio-Economic: i) S&P 500 ii) World Development Indicators

Environment: i) Earth Surface Temperature ii) US Pollution Data

Sports: i) College Basketball ii) FIFA Soccer Rankings iii) Cricket

Project Ideas

Implement a Recommendation System using the data sets above. Unsupervised

Predict the Genre / Artist of the media using the data sets above. Supervised

Predict the Rating / Popularity / Revenue of the media using the data sets above. Regression

Use historical trend to predict future value of an indicator. Time Series



URL: https://fahadsultan.com/csc272/intro.html


Introduction
About Me

Name: Syed Fahad Sultan

سید فہد سلطان

Pronunciation: sæjjɪd fah(aː)d solˈtˤɑːn

Just call me “Dr. Sultan” (Pronounced: Sool-tahn 🔈)

Fig. 3 Names work differently in different parts of the world!

I am originally from Lahore, Pakistan (Fun fact: Population of Lahore > Population of New York City) and joined Furman University in Fall 2022 after earning my Ph.D. in Computer Science from State University of New York at Stony Brook.

Fig. 4 Badshahi Mosque, Lahore, Pakistan.

Fresh out of college, I worked as a professional video game developer for a startup that later got acquired by the Japanese gaming giant DeNA. During this time, I was part of the team that built TapFish, the top-grossing game worldwide, for two weeks in 2011, on both the App Store and Google Play.

I then went on to work at Technology for People Initiative, an applied research lab in my where I mined social media and cell phone data for proxies of socio-economic indicators that allowed more inclusive policy-making for marginalized communities. During these years, I also dabbled in data journalism and helped organize a boot camp on using data for journalists with the support of the International Center for Journalists (ICFJ) and the Knight Foundation.

In 2015, I moved to Mecca, Saudi Arabia to work for the GIS Innovation Center (now Wadi Makkah). There I worked on innovative urban sensing techniques for better crowd control during the annual pilgrimage to the city, the largest human gathering in the world every year.

During my PhD, Fahad worked at the intersection of computational neuroscience, bioinformatics and machine learning. My work focused on identifying neurological and genetic biomarkers linking type-2 diabetes with cognitive disorders such as Alzheimer’s and other dementias.

	

	




	

	




1. Video games developer

	

2. City Planner

	

3. Neuroscientist

Teaching Assistant

Fig. 5 If your code has bugs, Mister Cat will find them!

How to Reach Me

Office: Riley Hall 200-D

Email: fahad.sultan@furman.edu

Office hours:

Monday: 1:30 PM – 4:30 PM

Friday: 9:30 AM – 11:30 AM

Drop by office, for any other time

Open door policy, when not in class or meeting

Fig. 6 Computer Science Department Suite in Riley Hall.

If you don’t like your odds for catching me in my office, schedule a meeting for 15 minutes or 30 minutes in advance.

Fig. 7 Course week. I am in my office most weekdays from 9 AM - 5 PM and have an open-door policy.

About the Course

Course website: https://fahadsultan.com/csc272

The Syllabus is available on the course website. In particular, please make sure to read the Grading, Academic Integrity and Textbook sections carefully.

Last but not least, please go over the Course Project in detail. The project is a major component of the course and will be due at the end of the semester.

All of the course content will be posted on this website.

Important announcements will be made on both the course website homepage and on Moodle

All assignments and exams should be submitted on Moodle. All grades will be uploaded on Moodle

Exams

There will be four exams in the course, one roughly every month, including the final. Exams constitute 40% of your course grade.

All exams will be on the computer, with a large programming component. Questions will be posted on Moodle and you will have to submit your solutions on Moodle, just like assignments.

You will be evaluated on your ability to apply knowledge to new problems and not just on your ability to retain and recall information.

The exams (and the project), more than the assignments, are primarily going to determine your grade.

All exams are going to be cumulative, with focus on the topics covered since last exam.

You will be assigned an interim grade on Workday after every Exam.

Diligent work on the homework and assignments will be rewarded here.

Fig. 8 Consistent effort and regular feedback.

Assignments

Approach assignments purely as opportunities to learn, prepare for exams and to prepare for your career.

It is not worth cheating on assignments. Just come talk to me if you are struggling with an assignment. I will literally just tell you the answer.

On assignments, expect near maximal flexibility from me. For every assignment, the soft (recommended) deadline will be a week after the assignment is posted. The hard deadline will be before the next exam.

In other words, as long as you submit the assignment before the next exam, you will get full credit.

Written Assignments:

Written assignments are to help you build a deeper understanding of algorithms and math covered in class.

These could simply be math problems or involve tracing algorithms and dry-runs.

Both handwritten or typed submissions are acceptable. Submissions, as always, on Moodle.

Programming Assignments:

Programming assignments are going to be posted at the start of the lab session each week and will be due before the start of the next lab session.

You should expect similar questions on the exams.

Class Participation

I may periodically give out class participation points during class for answering or asking a question

Given the glut of information accessible online and otherwise in this day and age, meaningful interactions with your peers and teachers is essentially what you are paying college tuition for.

Please come to class, labs and office hours

Please ask questions during class

Please answer questions and participate in discussions during class

Giant Asterisk *

Everything is tentative and subject to change

Fig. 9 Complaints Box on Moodle

This is my first teaching this course. Any and all feedback is welcome!

I have created an anonymous feedback poll on Moodle. Please use this to anonymously share any feedback.

Share any changes you want me to make in the course, at any point in the semester. You can submit multiple times over the span of the semester.

Think of it as a Complaints Box for the course.

What is Data Mining?

“Data Mining” is a term from the 1990s, back when it was an exciting and popular new field. Around 2010, people instead started to speak of “big data”. Today, the popular term is “data science”. There are some who even regard data mining as synonymous with machine learning. There is no question that some data mining appropriately uses algorithms from machine learning. However, during all this time, the concept remained the same: use the most powerful hardware, the most powerful programming systems, and the most efficient algorithms to solve problems in science, commerce, healthcare, government, the humanities, and many other fields of human endeavor.

Fig. 10 Drew Conway’s Data Science Venn Diagram

From the Venn Diagram, the course content is going to cover ✅ Hacking Skills and ✅ Math & Statistics in detail but not ☐ Substantive Expertise. For that missing piece, I strongly encourage you to bring in knowledge from your GERs and other Non-CS department courses into this class and the term project in particular. Nothing would make me happier than to see projects that combines CS with your other interests.

Expect lots of Programming and lots of Math in the Course!

“But wait, I am not a Math Person!” you say!

There is no such thing as a “Math Person”. I do recognize, however, that Math Anxiety is a real thing and is very common. It is a feeling of fear based on a belief that one is not good at math or that math is inherently difficult.

Please use this course as an opportunity to overcome your Math anxiety!

In this course, the code you write will be mostly math. Most modern “AI” is just that: math, in code.

This presents a unique opportunity for you to overcome your Math anxiety. You will be able to see the math in action, be able to visualize the results and have a conversation with it.

Trust me, there is a tremendous amount of beauty and joy to be found in mathematics. And if beauty and joy aren’t really your thing, then let me also assure you there is a lot of money to be made these days by being good at coding math. Either way, the rewards are well worth the effort!

Central Dogma of Data Science

Originally, “data mining” or “data dredging” was a derogatory term referring to attempts to extract information that was not supported by the data. Later, around the same time the iPhone was released and social networks started to take off, “data mining” took on a more positive meaning. Data was declared to be the “new oil” and the job of a Data Scientist was declared as “The Sexiest Job of the 21st Century”.

Today, I argue that the term “data mining” has mixed connotations. At the heart of the ambivalence towards data mining is the central dogma of data science:

Fig. 11 Commonly known as Central Dogma of Statistics.

The central dogma of data science, simply put, is that general claims about a population can be made from a sample of data. This raises all sorts of questions and concerns about the sampling process such as the representativeness of the sample, the size of the sample, the sampling bias, etc. Which in turn raises concerns about potential negative effects of the claims made based on questionable data.

I’ll end here. I hope you are excited about the course and the term project.

In the next lecture, we will begin by introducing one of the most important tools in exploratory data analysis: pandas.

Assignment 0

Go over course syllabus

Course website homepage

Written Assignment 0: Pre-course Survey (On Moodle)

Please complete before next class



URL: https://fahadsultan.com/csc272/1_programming/10_dataops.html


1. Vectorized Operations on Data

Computer Science is all about getting computers to perform tasks for us efficiently, reliably and correctly.

When we talk about efficiency, we are talking both space (memory) and time (speed).

Given we are living through the data deluge of the 21st century, when working with data, the issues of time and space are particularly relevant.

Let’s talk about speed first.

Time Complexity

Computer scientists are a little paranoid when it comes to speed. We always assume the worst!

Wait, how do we even measure speed?

And what are we even measuring the speed of?

Should we be measuring the amount of time it takes for our code to run?

We know computers are much faster today then they were only a few years ago and my Dad’s computer today is much slower than mine.

Fig. 1.1 Improvement in Processors over the years

So if we are measuring the speed of a computer program, whose computer should we measure the speed on?

Should we measure speed on an “average computer” available to buy in the market today? Identifying such an average computer sounds like a lot of work!

What do we do when newer faster computers come out in the future?

We also know some programming languages are faster than others.

It sounds like what we need is an abstract theoretical framework for measuring speed that is independent of any particular hardware specifications and any programming language.

Thankfully, this is a solved problem.

Computer Scientists long ago decided to measure speed of an algorithm, as opposed to any particular program in any programming language. An Algorithm if you recall is simply a sequence of instructions that maps inputs to outputs.

Five basic instructions of an algorithm:

Input

Output

Math

Branching

Repetition

Guess which of the five above is most relevant to measuring speed of an algorithm?

It is number 5: Repetitions! Also known as LOOPS!

That is, in computer science, we measure the speed of an algorithm in terms of iterations of a loop.

If an algorithm does not have any loops in it, it is said to be a constant time algorithm, even if that has a million input/output/math/branching instructions. Algorithms are classified using the big Big O-Notation. A constant time algorithm in O(1).

Your average algorithm has atleast one loop that iterates over some particular data structure. So what label do we give to an algorithm with one loop that follows the basic template:

sequence = [3, 1, 4, 1, 5, 9, 2, 6, 5]
i = 0 
while i < len(sequence):
    sequence[i] = sequence[i] + 2
    print(i, sequence)
    i = i + 1

0 [5, 1, 4, 1, 5, 9, 2, 6, 5]
1 [5, 3, 4, 1, 5, 9, 2, 6, 5]
2 [5, 3, 6, 1, 5, 9, 2, 6, 5]
3 [5, 3, 6, 3, 5, 9, 2, 6, 5]
4 [5, 3, 6, 3, 7, 9, 2, 6, 5]
5 [5, 3, 6, 3, 7, 11, 2, 6, 5]
6 [5, 3, 6, 3, 7, 11, 4, 6, 5]
7 [5, 3, 6, 3, 7, 11, 4, 8, 5]
8 [5, 3, 6, 3, 7, 11, 4, 8, 7]


Fig. 1.2 Serial execution of loops: each iteration must wait for the previous one to finish.

Such algorithms are given the label linear time or, in Big-O notation, O(n).

The letter n here is the size of the data structure i.e. len(arr). Length of a data structure also happens to be the number of iterations of the loop.

In summary, we measure the speed of an algorithm in relation to:

size of data

number of iterations in all loops in the algorithm

Fig. 1.3 If an iteration takes time t, then the total time taken by the algorithm is n*t where n is length of data

This is a good theoretical framework because it measures how the algorithm’s speed independent of hardware considerations.

Space Complexity

I don’t have a lot to say on the issue of space or memory consumption but it is critical.

Improving speed of an algorithm often comes at the cost of more memory consumption. That is, improving speed of an algorithm often necessitates creation or use of data structures that allow faster data retrieval and processing times. In that sense, speed and memory have an inherent tension in Computer Science. In this course, we are going to use such data structures that are going to result in very fast code. But the price we have to pay as a result is that we can only work with data sets that can fit within our memory. For most people it is going to be on the order of 8-16 GB.

Datasets of today are often much larger than this and require alternative paradigms of computer programming such as Distributed Computing, Parallel Programming, Concurrent Programming etc.

Vectorization

Vectorization is the process of applying operations to entire arrays at once, rather than their individual elements sequentially. Vectorization is also called Array Programming. It is usually much faster than using loops.

In this course, we will be minimizing the use of loops.

	




	




	




1. Serial / Sequential Operations

	

2. Vectorized Operations

Vectorization can only be applied in situations when operations at individual elements are independent of each other. For example, if we want to add two arrays, we can do so by adding each element of the first array to the corresponding element of the second array. This is a vectorized operation.

Fig. 1.4 Adding two arrays

However, for problems such as the Fibonacci sequence, where the value of an element depends on the values of the previous two elements, we cannot vectorize the operation. Similarly, finding minimum or maximum of an array cannot be vectorized.

Fig. 1.5 Vectorized operations, more generally, can be application of any function f to each element of an array.

Fig. 1.6 Vectorized operations are also known as SIMD (Single Instruction Multiple Data) operations in the context of computer architecture. In contrast, scalar operations are known as SISD (Single Instruction Single Data) operations.

Filtering certain entries in a vectorized array is also very easy using boolean indexing or boolean maps of the same length. Elements corresponding to True values are kept, while elements corresponding to False values are discarded.

Fig. 1.7 Filtering a vectorized array using a boolean map

pandas and pretty much all modern data science and AI libraries heavily rely on Vectorized operations to process and operate on large datasets.



URL: https://fahadsultan.com/csc272/1_programming/pandas1.html


1.1. Pandas I: Preliminaries

Pandas is a powerful Python library that is widely used in data science and data analysis. It provides data structures and functions that make working with tabular data easy and intuitive.

It is generally accepted in the data science community as the industry- and academia-standard tool for manipulating tabular data.

1.1.1. Dimensionality of Data

Dimensionality, in the context of data, refers to the number of axes or directions in which data can be represented. The most common dimensions are 0, 1, 2, and n.

Scalars (0-dimensional data; values) are single numbers. They can be integers, real numbers, or complex numbers. Scalars are the simplest objects in linear algebra. In Python, we can represent scalars using the built-in int and float data types. For example, 3 and 3.0 are both scalars.

Vectors (1-dimensional data, collection of values) are one-dimensional arrays of scalars. They are used to represent quantities that have both magnitude and direction. In native Python, we can represent vectors using lists or tuples. For example, [1, 2, 3] is a vector.

Fig. 1.8 Data can be represented in different dimensions. The most common dimensions are 0, 1, 2, and n.

Matrices (2-dimensional data, collection of vectors) are two-dimensional arrays of scalars. They are used to represent linear transformations from one vector space to another. In native Python, we can represent matrices using lists of lists. For example, [[1, 2], [3, 4]] is a matrix.

Tensors (n-dimensional data, collection of matrices) are n-dimensional arrays of scalars. They are used to represent multi-dimensional data.

1.1.2. Tabular (2-dimensional) Data

Tables are one of the most common ways to organize data. This is in large part due to the simplicity and flexibility of tables. Tables allow us to represent each observation, or instance of collecting data from an individual, as its own row. We can record distinct characteristics, or features, of each observation in separate columns.

Fig. 1.9 A table is a collection of rows and columns. Each row represents an observation, and each column represents a feature of the observation.

To see this in action, we’ll explore the elections dataset, which stores information about political candidates who ran for president of the United States in various years.

The first few rows of elections dataset in CSV format are as follows:

Year,Candidate,Party,Popular vote,Result,%\n
1824,Andrew Jackson,Democratic-Republican,151271,loss,57.21012204\n
1824,John Quincy Adams,Democratic-Republican,113142,win,42.78987796\n
1828,Andrew Jackson,Democratic,642806,win,56.20392707\n
1828,John Quincy Adams,National Republican,500897,loss,43.79607293\n
1832,Andrew Jackson,Democratic,702735,win,54.57478905\n


This dataset is stored in Comma Separated Values (CSV) format. CSV files due to their simplicity and readability are one of the most common ways to store tabular data. Each line in a CSV file (file extension: .csv) represents a row in the table. In other words, each row is separated by a newline character \n. Within each row, each column is separated by a comma ,, hence the name Comma Separated Values.

1.1.3. Reading Data

To begin our studies in pandas, we must first import the library into our Python environment using import pandas as pd statement. pd is a common alias for pandas. The import statement will allow us to use pandas data structures and methods in our code.

Fig. 1.10 Pandas can read from and to a variety of file formats, including CSV, Excel, and SQL databases.

CSV files can be in pandas using read_csv. The following code cell imports pandas as pd, the conventional alias for Pandas and then reads the elections.csv file.

# `pd` is the conventional alias for Pandas
import pandas as pd

url = "https://raw.githubusercontent.com/fahadsultan/csc272/main/data/elections.csv"
elections = pd.read_csv(url)
elections

	Year	Candidate	Party	Popular vote	Result	%
0	1824	Andrew Jackson	Democratic-Republican	151271	loss	57.210122
1	1824	John Quincy Adams	Democratic-Republican	113142	win	42.789878
2	1828	Andrew Jackson	Democratic	642806	win	56.203927
3	1828	John Quincy Adams	National Republican	500897	loss	43.796073
4	1832	Andrew Jackson	Democratic	702735	win	54.574789
...	...	...	...	...	...	...
177	2016	Jill Stein	Green	1457226	loss	1.073699
178	2020	Joseph Biden	Democratic	81268924	win	51.311515
179	2020	Donald Trump	Republican	74216154	loss	46.858542
180	2020	Jo Jorgensen	Libertarian	1865724	loss	1.177979
181	2020	Howard Hawkins	Green	405035	loss	0.255731

182 rows × 6 columns

Let’s dissect the code above.

We first import the pandas library into our Python environment, using the alias pd.  import pandas as pd

There are a number of ways to read data into a DataFrame. In this course, our datasets are typically stored in a CSV (comma-seperated values) file format. We can import a CSV file into a DataFrame by passing the data path as an argument to the following pandas function.  pd.read_csv("data/elections.csv")

This code stores our DataFrame object in the elections variable. We see that our elections DataFrame has 182 rows and 6 columns (Year, Candidate, Party, Popular Vote, Result, %). Each row represents a single record – in our example, a presedential candidate from some particular year. Each column represents a single attribute, or feature of the record.

In the example above, we constructed a DataFrame object using data from a CSV file. As we’ll explore in the next section, we can also create a DataFrame with data of our own.

In the elections dataset, each row represents one instance of a candidate running for president in a particular year. For example, the first row represents Andrew Jackson running for president in the year 1824. Each column represents one characteristic piece of information about each presidential candidate. For example, the column named Result stores whether or not the candidate won the election.

Some relevant arguments for read_csv are:

filepath_or_buffer: The path to the CSV file.

sep: The character that separates the values in the CSV file. By default, this is a comma ,.

header: The row number to use as the column names. By default, this is 0, which means the first row is used as the column names.

index_col: The column to use as the row labels of the DataFrame. By default, this is None, which means that the row labels are integers starting from 0.

error_bad_lines: If True, the parser will skip lines with too many fields rather than raising an error. By default, this is False.

1.1.4. .head() method

.head() is a method of a DataFrame that returns the first n rows of a DataFrame. By default, n is 5. This is useful when you want to quickly check the contents of a DataFrame.

elections.head()

	Year	Candidate	Party	Popular vote	Result	%
0	1824	Andrew Jackson	Democratic-Republican	151271	loss	57.210122
1	1824	John Quincy Adams	Democratic-Republican	113142	win	42.789878
2	1828	Andrew Jackson	Democratic	642806	win	56.203927
3	1828	John Quincy Adams	National Republican	500897	loss	43.796073
4	1832	Andrew Jackson	Democratic	702735	win	54.574789

Similarly, calling df.tail(n) allows us to extract the last n rows of the DataFrame.

elections.tail(3)

	Year	Candidate	Party	Popular vote	Result	%
179	2020	Donald Trump	Republican	74216154	loss	46.858542
180	2020	Jo Jorgensen	Libertarian	1865724	loss	1.177979
181	2020	Howard Hawkins	Green	405035	loss	0.255731
1.1.5. .shape attribute

.shape is an attribute of a DataFrame that returns a tuple representing the dimensions of the DataFrame.

elections.shape

(182, 6)


The first element of the tuple is the number of rows, and the second element is the number of columns.

1.1.6. .dtypes attribute

.dtypes is an attribute of a DataFrame that returns the data type of each column. The data types are returned as a Series with the column names as the index labels.

elections.dtypes

Year              int64
Candidate        object
Party            object
Popular vote      int64
Result           object
%               float64
dtype: object


In pandas, object is the data type used for string columns, while int64 and float64 are used for integer and floating-point columns, respectively.

1.1.7. Writing Data

pandas can also write data to a variety of file formats, including CSV, Excel, and SQL databases. The following code cell writes the elections dataset to a CSV file named elections.csv.

Fig. 1.11 Pandas can write to a variety of file formats, including CSV, Excel, XML, JSON and SQL. To write to a format, use the to_<format> method on a DataFrame with the desired file name as an argument.

pd.to_csv('elections_new.csv')

1.1.8. DataFrame, Series and Index

There are three fundamental data structures in pandas:

Series: 1D labeled array data; best thought of as columnar data

DataFrame: 2D tabular data with rows and columns

Index: A sequence of row/column labels

DataFrames, Series, and Indices can be represented visually in the following diagram, which considers the first few rows of the elections dataset.

Fig. 1.12 Three fundamental pandas data structures: Series, DataFrame, Index

Notice how the DataFrame is a two-dimensional object – it contains both rows and columns. The Series above is a singular column of this DataFrame, namely, the Result column. Both contain an Index, or a shared list of row labels (here, the integers from 0 to 4, inclusive).

Fig. 1.13 Schematic of a pandas DataFrame and Series

Fig. 1.14 Each column of a pandas DataFrame df is a Series s where s.index == df.index

Fig. 1.15 Each row of a pandas DataFrame df is a Series s where s.index == df.columns



URL: https://fahadsultan.com/csc272/1_programming/pandas2.html


1.2. Pandas II - Data Manipulation and Wrangling

Let’s read in the same elections data from the previous exercise and do some data manipulation and wrangling using pandas.

import pandas as pd 
url = "https://raw.githubusercontent.com/fahadsultan/csc272/main/data/elections.csv"
elections = pd.read_csv(url)

1.2.1. Data Alignment

pandas can make it much simpler to work with objects that have different indexes. For example, when you add objects, if any index pairs are not the same, the respective index in the result will be the union of the index pairs. Let’s look at an example:

s1 = pd.Series([7.3, -2.5, 3.4, 1.5], index=["a", "c", "d", "e"])

s2 = pd.Series([-2.1, 3.6, -1.5, 4, 3.1], index=["a", "c", "e", "f", "g"])

s1, s2, s1 + s2

(a    7.3
 c   -2.5
 d    3.4
 e    1.5
 dtype: float64,
 a   -2.1
 c    3.6
 e   -1.5
 f    4.0
 g    3.1
 dtype: float64,
 a    5.2
 c    1.1
 d    NaN
 e    0.0
 f    NaN
 g    NaN
 dtype: float64)


The internal data alignment introduces missing values in the label locations that don’t overlap. Missing values will then propagate in further arithmetic computations.

In the case of DataFrame, alignment is performed on both rows and columns:

df1 = pd.DataFrame({"A": [1, 2], "B":[3, 4]})
df2 = pd.DataFrame({"B": [5, 6], "D":[7, 8]})
df1 + df2

1.2.2. Math Operations

In native Python, we have a number of operators that we can use to manipulate data. Most, if not all, of these operators can be used with Pandas Series and DataFrames and are applied element-wise in parallel. A summary of the operators supported by Pandas is shown below:

Category

	

Operators

	

Supported by Pandas

	

Comments




Arithmetic

	

+, -, *, /, %, //, **

	

✅

	

Assuming comparable shapes (equal length)




Assignment

	

=, +=, -=, *=, /=, %=, //=, **=

	

✅

	

Assuming comparable shapes




Comparison

	

==, !=, >, <, >=, <=

	

✅

	

Assuming comparable shapes




Logical

	

and, or, not

	

❌

	

Use &, |, ~ instead




Identity

	

is, is not

	

✅

	

Assuming comparable data type/structure




Membership

	

in, not in

	

❌

	

Use isin() method instead




Bitwise

	

&, |, ^, ~, <<, >>

	

❌

	

The most significant difference is that logical operators and, or, and not are NOT used with Pandas Series and DataFrames. Instead, we use &, |, and ~ respectively.

Membership operators in and not in are also not used with Pandas Series and DataFrames. Instead, we use the isin() method.

1.2.3. Creating new columns

Creating new columns in a DataFrame is a common task when working with data. In this notebook, we will see how to create new columns in a DataFrame based on existing columns or other values.

Fig. 1.16 Creating a new column in a DataFrame

New columns can be created by assigning a value to a new column name. For example, to create a new column named new_column with a constant value 10, we can use the following code:

elections['constant'] = 10

elections.head()

	Year	Candidate	Party	Popular vote	Result	%	constant
0	1824	Andrew Jackson	Democratic-Republican	151271	loss	57.210122	10
1	1824	John Quincy Adams	Democratic-Republican	113142	win	42.789878	10
2	1828	Andrew Jackson	Democratic	642806	win	56.203927	10
3	1828	John Quincy Adams	National Republican	500897	loss	43.796073	10
4	1832	Andrew Jackson	Democratic	702735	win	54.574789	10



If we want to create a new column based on an existing column, we can refer to the existing column by its name, within the square brackets, on the right side of the assignment operator. For example, to create a new column named new_column with the values of the existing column column1, we can use the following code:



elections['total_voters'] = ((elections['Popular vote']* 100) / elections['%']).astype(int)

elections.head()

	Year	Candidate	Party	Popular vote	Result	%	constant	total_voters
0	1824	Andrew Jackson	Democratic-Republican	151271	loss	57.210122	10	264413
1	1824	John Quincy Adams	Democratic-Republican	113142	win	42.789878	10	264412
2	1828	Andrew Jackson	Democratic	642806	win	56.203927	10	1143702
3	1828	John Quincy Adams	National Republican	500897	loss	43.796073	10	1143703
4	1832	Andrew Jackson	Democratic	702735	win	54.574789	10	1287655

Note that the new column will have the same length as the DataFrame and the calculations are element-wise. That is, the value of the new column at row i will be calculated based on the value of the existing column at row i.

Fig. 1.17 Element-wise operations on existing columns to create a new column

These element-wise operations are vectorized and are very efficient.

1.2.4. .apply(f, axis=0/1)

A frequent operation in pandas is applying a function on to either each column or row of a DataFrame.

DataFrame’s apply method does exactly this.

Let’s say we wanted to count the number of unique values that each column takes on. We can use .apply to answer that question:

def count_unique(col):
    return len(set(col))

elections.apply(count_unique, axis="index") # function is passed an individual column

Year             50
Candidate       132
Party            36
Popular vote    182
Result            2
%               182
dtype: int64

1.2.4.1. Column-wise: axis=0 (default)

data.apply(f, axis=0) applies the function f to each column of the DataFrame data.

Fig. 1.18 df.apply(f, axis=0) results in 
𝐷
 concurrent calls to 
𝑓
. Each function call is passed a column.

For example, if we wanted to find the number of unique values in each column of a DataFrame data, we could use the following code:

def count_unique(column):
    return len(column.unique())

elections.apply(count_unique, axis=0)

Year             50
Candidate       132
Party            36
Popular vote    182
Result            2
%               182
dtype: int64

1.2.4.2. Row-wise: axis=1

data.apply(f, axis=1) applies the function f to each row of the DataFrame data.

Fig. 1.19 df.apply(f, axis=1) results in 
𝑁
 concurrent calls to 
𝑓
. Each function call is passed a row.

For instance, let’s say we wanted to count the total number of voters in an election.

We can use .apply to answer that question using the following formula:

total
×
%
100
=
Popular vote
def compute_total(row):
    return int(row['Popular vote']*100/row['%'])

elections.apply(compute_total, axis=1)

0         264413
1         264412
2        1143702
3        1143703
4        1287655
         ...    
177    135720167
178    158383403
179    158383403
180    158383401
181    158383402
Length: 182, dtype: int64

1.2.5. Summary Statistics

In data science, we often want to compute summary statistics. pandas provides a number of built-in methods for this purpose.

Fig. 1.20 Aggregating data using one or more operations

For example, we can use the .mean(), .median() and .std() methods to compute the mean, median, and standard deviation of a column, respectively.

elections['%'].mean(), elections['%'].median(), elections['%'].std()

(27.470350372043967, 37.67789306, 22.96803399144419)


Similarly, we can use the .max() and .min() methods to compute the maximum and minimum values of a Series or DataFrame.

elections['%'].max(), elections['%'].min()

(61.34470329, 0.098088334)


The .sum() method computes the sum of all the values in a Series or DataFrame.

Fig. 1.21 Reduction operations

The .describe() method computes summary statistics for a Series or DataFrame. It computes the mean, standard deviation, minimum, maximum, and the quantiles of the data.

elections['%'].describe()

count    182.000000
mean      27.470350
std       22.968034
min        0.098088
25%        1.219996
50%       37.677893
75%       48.354977
max       61.344703
Name: %, dtype: float64

elections.describe()

	Year	Popular vote	%
count	182.000000	1.820000e+02	182.000000
mean	1934.087912	1.235364e+07	27.470350
std	57.048908	1.907715e+07	22.968034
min	1824.000000	1.007150e+05	0.098088
25%	1889.000000	3.876395e+05	1.219996
50%	1936.000000	1.709375e+06	37.677893
75%	1988.000000	1.897775e+07	48.354977
max	2020.000000	8.126892e+07	61.344703
1.2.6. Other Handy Utility Functions

pandas contains an extensive library of functions that can help shorten the process of setting and getting information from its data structures. In the following section, we will give overviews of each of the main utility functions that will help us in in this course.

Discussing all functionality offered by pandas could take an entire semester! We will walk you through the most commonly-used functions, and encourage you to explore and experiment on your own.

.shape

.size

.dtypes

.astype()

.describe()

.sample()

.value_counts()

.unique()

.sort_values()

The pandas documentation will be a valuable resource.

1.2.6.1. .astype()

Cast a pandas object to a specified dtype

elections['%'].astype(int)

0      57
1      42
2      56
3      43
4      54
       ..
177     1
178    51
179    46
180     1
181     0
Name: %, Length: 182, dtype: int64

1.2.6.2. .describe()

If many statistics are required from a DataFrame (minimum value, maximum value, mean value, etc.), then .describe() can be used to compute all of them at once.

elections.describe()

	Year	Popular vote	%
count	182.000000	1.820000e+02	182.000000
mean	1934.087912	1.235364e+07	27.470350
std	57.048908	1.907715e+07	22.968034
min	1824.000000	1.007150e+05	0.098088
25%	1889.000000	3.876395e+05	1.219996
50%	1936.000000	1.709375e+06	37.677893
75%	1988.000000	1.897775e+07	48.354977
max	2020.000000	8.126892e+07	61.344703

A different set of statistics will be reported if .describe() is called on a Series.

elections["Party"].describe()

count            182
unique            36
top       Democratic
freq              47
Name: Party, dtype: object

elections["Popular vote"].describe().astype(int)

count         182
mean     12353635
std      19077149
min        100715
25%        387639
50%       1709375
75%      18977751
max      81268924
Name: Popular vote, dtype: int64

1.2.6.3. .sample()

As we will see later in the semester, random processes are at the heart of many data science techniques (for example, train-test splits, bootstrapping, and cross-validation). .sample() lets us quickly select random entries (a row if called from a DataFrame, or a value if called from a Series).

By default, .sample() selects entries without replacement. Pass in the argument replace=True to sample with replacement.

# Sample a single row
elections.sample()

	Year	Candidate	Party	Popular vote	Result	%
135	1988	George H. W. Bush	Republican	48886597	win	53.518845
# Sample 5 random rows
elections.sample(5)

	Year	Candidate	Party	Popular vote	Result	%
155	2000	Ralph Nader	Green	2882955	loss	2.741176
134	1984	Walter Mondale	Democratic	37577352	loss	40.729429
39	1884	Grover Cleveland	Democratic	4914482	win	48.884933
84	1928	Herbert Hoover	Republican	21427123	win	58.368524
177	2016	Jill Stein	Green	1457226	loss	1.073699
# Randomly sample 4 names from the year 2000, with replacement
elections[elections["Result"] == "win"].sample(4, replace = True)

	Year	Candidate	Party	Popular vote	Result	%
53	1896	William McKinley	Republican	7112138	win	51.213817
131	1980	Ronald Reagan	Republican	43903230	win	50.897944
2	1828	Andrew Jackson	Democratic	642806	win	56.203927
168	2012	Barack Obama	Democratic	65915795	win	51.258484
1.2.6.4. .value_counts()

The Series.value_counts() methods counts the number of occurrence of each unique value in a Series. In other words, it counts the number of times each unique value appears. This is often useful for determining the most or least common entries in a Series.

In the example below, we can determine the name with the most years in which at least one person has taken that name by counting the number of times each name appears in the "Name" column of elections.

elections["Party"].value_counts().head()

Democratic     47
Republican     41
Libertarian    12
Prohibition    11
Socialist      10
Name: Party, dtype: int64

1.2.6.5. .unique()

If we have a Series with many repeated values, then .unique() can be used to identify only the unique values. Here we return an array of all the names in elections.

elections["Result"].unique()

array(['loss', 'win'], dtype=object)

1.2.6.6. .drop_duplicates()

If we have a DataFrame with many repeated rows, then .drop_duplicates() can be used to remove the repeated rows.

Where .unique() only works with individual columns (Series) and returns an array of unique values, .drop_duplicates() can be used with multiple columns (DataFrame) and returns a DataFrame with the repeated rows removed.

elections[['Candidate', 'Party']].drop_duplicates()

	Candidate	Party
0	Andrew Jackson	Democratic-Republican
1	John Quincy Adams	Democratic-Republican
2	Andrew Jackson	Democratic
3	John Quincy Adams	National Republican
5	Henry Clay	National Republican
...	...	...
174	Evan McMullin	Independent
176	Hillary Clinton	Democratic
178	Joseph Biden	Democratic
180	Jo Jorgensen	Libertarian
181	Howard Hawkins	Green

141 rows × 2 columns

1.2.6.7. .sort_values()

Ordering a DataFrame can be useful for isolating extreme values. For example, the first 5 entries of a row sorted in descending order (that is, from highest to lowest) are the largest 5 values. .sort_values allows us to order a DataFrame or Series by a specified column. We can choose to either receive the rows in ascending order (default) or descending order.

# Sort the "Count" column from highest to lowest
elections.sort_values(by = "%", ascending=False).head()

	Year	Candidate	Party	Popular vote	Result	%
114	1964	Lyndon Johnson	Democratic	43127041	win	61.344703
91	1936	Franklin Roosevelt	Democratic	27752648	win	60.978107
120	1972	Richard Nixon	Republican	47168710	win	60.907806
79	1920	Warren Harding	Republican	16144093	win	60.574501
133	1984	Ronald Reagan	Republican	54455472	win	59.023326

We do not need to explicitly specify the column used for sorting when calling .value_counts() on a Series. We can still specify the ordering paradigm – that is, whether values are sorted in ascending or descending order.

# Sort the "Name" Series alphabetically
elections["Candidate"].sort_values(ascending=True).head()

75     Aaron S. Watkins
27      Abraham Lincoln
23      Abraham Lincoln
108     Adlai Stevenson
105     Adlai Stevenson
Name: Candidate, dtype: object

1.2.6.8. .set_index()

The .set_index() method is used to set the DataFrame index using existing columns.

elections.set_index("Candidate")

	Year	Party	Popular vote	Result	%
Candidate					
Andrew Jackson	1824	Democratic-Republican	151271	loss	57.210122
John Quincy Adams	1824	Democratic-Republican	113142	win	42.789878
Andrew Jackson	1828	Democratic	642806	win	56.203927
John Quincy Adams	1828	National Republican	500897	loss	43.796073
Andrew Jackson	1832	Democratic	702735	win	54.574789
...	...	...	...	...	...
Jill Stein	2016	Green	1457226	loss	1.073699
Joseph Biden	2020	Democratic	81268924	win	51.311515
Donald Trump	2020	Republican	74216154	loss	46.858542
Jo Jorgensen	2020	Libertarian	1865724	loss	1.177979
Howard Hawkins	2020	Green	405035	loss	0.255731

182 rows × 5 columns

1.2.6.9. .reset_index()

The .reset_index() method is used to reset the index of a DataFrame. By default, the original index is stored in a new column called index.

elections.reset_index()

	index	Year	Candidate	Party	Popular vote	Result	%
0	0	1824	Andrew Jackson	Democratic-Republican	151271	loss	57.210122
1	1	1824	John Quincy Adams	Democratic-Republican	113142	win	42.789878
2	2	1828	Andrew Jackson	Democratic	642806	win	56.203927
3	3	1828	John Quincy Adams	National Republican	500897	loss	43.796073
4	4	1832	Andrew Jackson	Democratic	702735	win	54.574789
...	...	...	...	...	...	...	...
177	177	2016	Jill Stein	Green	1457226	loss	1.073699
178	178	2020	Joseph Biden	Democratic	81268924	win	51.311515
179	179	2020	Donald Trump	Republican	74216154	loss	46.858542
180	180	2020	Jo Jorgensen	Libertarian	1865724	loss	1.177979
181	181	2020	Howard Hawkins	Green	405035	loss	0.255731

182 rows × 7 columns

1.2.6.10. .rename()

The .rename() method is used to rename the columns or index labels of a DataFrame.

elections.rename(columns={"Candidate":"Name"})

	Year	Name	Party	Popular vote	Result	%
0	1824	Andrew Jackson	Democratic-Republican	151271	loss	57.210122
1	1824	John Quincy Adams	Democratic-Republican	113142	win	42.789878
2	1828	Andrew Jackson	Democratic	642806	win	56.203927
3	1828	John Quincy Adams	National Republican	500897	loss	43.796073
4	1832	Andrew Jackson	Democratic	702735	win	54.574789
...	...	...	...	...	...	...
177	2016	Jill Stein	Green	1457226	loss	1.073699
178	2020	Joseph Biden	Democratic	81268924	win	51.311515
179	2020	Donald Trump	Republican	74216154	loss	46.858542
180	2020	Jo Jorgensen	Libertarian	1865724	loss	1.177979
181	2020	Howard Hawkins	Green	405035	loss	0.255731

182 rows × 6 columns



URL: https://fahadsultan.com/csc272/1_programming/pandas3.html


1.3. Pandas III: Selection, Filtering and Dropping

In this section, we will learn how to extract and remove a subset of rows and columns in pandas. The two primary operations of data extraction are:

Selection: Extracting subset of columns.

Filtering: Extracting subset of rows.

Let’s start by loading the dataset.

import pandas as pd 

url = "https://raw.githubusercontent.com/fahadsultan/csc272/main/data/elections.csv"

elections = pd.read_csv(url)

1.3.1. Selection: subset of columns

To select a column in a DataFrame, we can use the bracket notation. That is, name of the DataFrame followed by the column name in square brackets: df['column_name'].

Fig. 1.22 To select a column, we can use the bracket notation: df['column_name']. This extracts a single column as a Series.

To select multiple columns, we can pass a list of column names: df[['column_name1', 'column_name2']]. This extracts multiple columns as a DataFrame.

For example, to select a column named Candidate from the election DataFrame, we can use the following code:

candidates = elections['Candidate']
print(candidates)

0         Andrew Jackson
1      John Quincy Adams
2         Andrew Jackson
3      John Quincy Adams
4         Andrew Jackson
             ...        
177           Jill Stein
178         Joseph Biden
179         Donald Trump
180         Jo Jorgensen
181       Howard Hawkins
Name: Candidate, Length: 182, dtype: object


This extracts a single column as a Series. We can confirm this by checking the type of the output.

type(candidates)

pandas.core.series.Series


To select multiple columns, we can pass a list of column names. For example, to select both Candidate and Votes columns from the election DataFrame, we can use the following line of code:

elections[['Candidate', 'Party']]

	Candidate	Party
0	Andrew Jackson	Democratic-Republican
1	John Quincy Adams	Democratic-Republican
2	Andrew Jackson	Democratic
3	John Quincy Adams	National Republican
4	Andrew Jackson	Democratic
...	...	...
177	Jill Stein	Green
178	Joseph Biden	Democratic
179	Donald Trump	Republican
180	Jo Jorgensen	Libertarian
181	Howard Hawkins	Green

182 rows × 2 columns

This extracts multiple columns as a DataFrame. We can confirm as well this by checking the type of the output.

type(elections[['Candidate', 'Party']])


This is how we can select columns in a DataFrame. Next, let’s learn how to filter rows.

1.3.2. A Filtering Condition

Perhaps the most interesting (and useful) method of selecting data from a Series is with a filtering condition.

First, we apply a boolean condition to the Series. This create a new Series of boolean values.

series = pd.Series({'a': 1, 'b': 2, 'c': 3, 'd': 4})
series > 2

a    False
b    False
c     True
d     True
dtype: bool


We then use this boolean condition to index into our original Series. pandas will select only the entries in the original Series that satisfy the condition.

series[series > 2]

c    3
d    4
dtype: int64

1.3.3. Filtering: subset of rows

Extracting a subset of rows from a DataFrame is called filtering.

We can filter rows based on a boolean condition, similar to conditional statements (e.g., if, else) in Python.

Fig. 1.23 To filter rows based on a boolean condition, we can use the bracket notation: df[boolean_condition]. This extracts rows where the condition is True.

To filter rows based on multiple conditions, we can use the & operator for AND and the | operator for OR.

For example, to filter rows of candidates who ran for elections since 2010, we can use the following code:

condition = election['Year'] > 2010

election[condition]

	Year	Candidate	Party	Popular vote	Result	%
168	2012	Barack Obama	Democratic	65915795	win	51.258484
169	2012	Gary Johnson	Libertarian	1275971	loss	0.992241
170	2012	Jill Stein	Green	469627	loss	0.365199
171	2012	Mitt Romney	Republican	60933504	loss	47.384076
172	2016	Darrell Castle	Constitution	203091	loss	0.149640
173	2016	Donald Trump	Republican	62984828	win	46.407862
174	2016	Evan McMullin	Independent	732273	loss	0.539546
175	2016	Gary Johnson	Libertarian	4489235	loss	3.307714
176	2016	Hillary Clinton	Democratic	65853514	loss	48.521539
177	2016	Jill Stein	Green	1457226	loss	1.073699
178	2020	Joseph Biden	Democratic	81268924	win	51.311515
179	2020	Donald Trump	Republican	74216154	loss	46.858542
180	2020	Jo Jorgensen	Libertarian	1865724	loss	1.177979
181	2020	Howard Hawkins	Green	405035	loss	0.255731

To filter rows based on multiple conditions, we can use the & operator for AND and the | operator for OR.

For example, to filter rows of candidates who won the elections with less than 50% of the votes, we can use the following code:

condition = (election['Result'] == 'win') & (election['%'] < 50)

election[condition]

	Year	Candidate	Party	Popular vote	Result	%
1	1824	John Quincy Adams	Democratic-Republican	113142	win	42.789878
16	1848	Zachary Taylor	Whig	1360235	win	47.309296
20	1856	James Buchanan	Democratic	1835140	win	45.306080
23	1860	Abraham Lincoln	Republican	1855993	win	39.699408
33	1876	Rutherford Hayes	Republican	4034142	win	48.471624
36	1880	James Garfield	Republican	4453337	win	48.369234
39	1884	Grover Cleveland	Democratic	4914482	win	48.884933
43	1888	Benjamin Harrison	Republican	5443633	win	47.858041
47	1892	Grover Cleveland	Democratic	5553898	win	46.121393
70	1912	Woodrow Wilson	Democratic	6296284	win	41.933422
74	1916	Woodrow Wilson	Democratic	9126868	win	49.367987
100	1948	Harry Truman	Democratic	24179347	win	49.601536
117	1968	Richard Nixon	Republican	31783783	win	43.565246
140	1992	Bill Clinton	Democratic	44909806	win	43.118485
144	1996	Bill Clinton	Democratic	47400125	win	49.296938
152	2000	George W. Bush	Republican	50456002	win	47.974666
173	2016	Donald Trump	Republican	62984828	win	46.407862
1.3.4. Extracting subset of values

To extract a subset of values, we can use .loc[] or .iloc[] with row and column indices and labels respectively.

The .loc[] method is used to access a group of rows and columns by labels or a boolean array.

1.3.4.1. .loc[row_labels, col_labels]

The .loc operator selects rows and columns in a DataFrame by their row and column label(s), respectively. The row labels (commonly referred to as the indices) are the bold text on the far left of a DataFrame, while the column labels are the column names found at the top of a DataFrame.

Fig. 1.24 .loc[i, j] returns value(s) where row label(s)== i and column label(s)== j in the DataFrame.

To grab data with .loc, we must specify the row and column label(s) where the data exists. The row labels are the first argument to the .loc function; the column labels are the second. For example, we can select the the row labeled 0 and the column labeled Candidate from the elections DataFrame.

elections.loc[0, 'Candidate']


To select multiple rows and columns, we can use Python slice notation. Here, we select the rows from labels 0 to 3 and the columns from labels "Year" to "Popular vote".

elections.loc[0:3, 'Year':'Popular vote']


Suppose that instead, we wanted every column value for the first four rows in the elections DataFrame. The shorthand : is useful for this.

elections.loc[0:3, :]


There are a couple of things we should note. Firstly, unlike conventional Python, Pandas allows us to slice string values (in our example, the column labels). Secondly, slicing with .loc is inclusive. Notice how our resulting DataFrame includes every row and column between and including the slice labels we specified.

Equivalently, we can use a list to obtain multiple rows and columns in our elections DataFrame. elections.loc[[0, 1, 2, 3], [‘Year’, ‘Candidate’, ‘Party’, ‘Popular vote’]]

Lastly, we can interchange list and slicing notation. elections.loc[[0, 1, 2, 3], :]

1.3.4.2. .iloc[row_indices, col_indices]

The .iloc[] method is used to access a group of rows and columns by integer position.

Warning

If you find yourself needing to use .iloc then stop and think if you are about to implement a loop. If so, there is probably a better way to do it.

Slicing with .iloc works similarily to .loc, however, .iloc uses the index positions of rows and columns rather the labels (think to yourself: loc uses labels; iloc uses indices). The arguments to the .iloc function also behave similarly -– single values, lists, indices, and any combination of these are permitted.

Fig. 1.25 .iloc[i, j] returns value(s) where row location(s)== i and column location(s)== j in the DataFrame.

Let’s begin reproducing our results from above. We’ll begin by selecting for the first presidential candidate in our elections DataFrame:

# elections.loc[0, "Candidate"] - Previous approach
elections.iloc[0, 1]


Notice how the first argument to both .loc and .iloc are the same. This is because the row with a label of 0 is conveniently in the 0th index (equivalently, the first position) of the elections DataFrame. Generally, this is true of any DataFrame where the row labels are incremented in ascending order from 0.

However, when we select the first four rows and columns using .iloc, we notice something.

# elections.loc[0:3, 'Year':'Popular vote'] - Previous approach
elections.iloc[0:4, 0:4]


Slicing is no longer inclusive in .iloc -– it’s exclusive. In other words, the right-end of a slice is not included when using .iloc. This is one of the subtleties of pandas syntax; you will get used to it with practice.

#elections.loc[[0, 1, 2, 3], ['Year', 'Candidate', 'Party', 'Popular vote']] - Previous Approach
elections.iloc[[0, 1, 2, 3], [0, 1, 2, 3]]


This discussion begs the question: when should we use .loc vs .iloc? In most cases, .loc is generally safer to use. You can imagine .iloc may return incorrect values when applied to a dataset where the ordering of data can change.

1.3.4.3. []

The [] selection operator is the most baffling of all, yet the most commonly used. It only takes a single argument, which may be one of the following:

A slice of row numbers

A list of column labels

A single column label

That is, [] is context dependent. Let’s see some examples.

Say we wanted the first four rows of our elections DataFrame.

elections[0:4]

	Year	Candidate	Party	Popular vote	Result	%
0	1824	Andrew Jackson	Democratic-Republican	151271	loss	57.210122
1	1824	John Quincy Adams	Democratic-Republican	113142	win	42.789878
2	1828	Andrew Jackson	Democratic	642806	win	56.203927
3	1828	John Quincy Adams	National Republican	500897	loss	43.796073
1.3.5. Dropping rows and columns

To drop rows and columns in a DataFrame, we can use the drop() method.

For example, to drop the first row from the election DataFrame, we can use the following code:

elections.head()

	Year	Candidate	Party	Popular vote	Result	%
0	1824	Andrew Jackson	Democratic-Republican	151271	loss	57.210122
1	1824	John Quincy Adams	Democratic-Republican	113142	win	42.789878
2	1828	Andrew Jackson	Democratic	642806	win	56.203927
3	1828	John Quincy Adams	National Republican	500897	loss	43.796073
4	1832	Andrew Jackson	Democratic	702735	win	54.574789
elections.drop(columns=['Popular vote'])

	Year	Candidate	Party	Result	%
0	1824	Andrew Jackson	Democratic-Republican	loss	57.210122
1	1824	John Quincy Adams	Democratic-Republican	win	42.789878
2	1828	Andrew Jackson	Democratic	win	56.203927
3	1828	John Quincy Adams	National Republican	loss	43.796073
4	1832	Andrew Jackson	Democratic	win	54.574789
...	...	...	...	...	...
177	2016	Jill Stein	Green	loss	1.073699
178	2020	Joseph Biden	Democratic	win	51.311515
179	2020	Donald Trump	Republican	loss	46.858542
180	2020	Jo Jorgensen	Libertarian	loss	1.177979
181	2020	Howard Hawkins	Green	loss	0.255731

182 rows × 5 columns

# Drop the first row
elections.drop(index=0)

# Drop the first two rows
elections.drop(index=[0, 1])

	Year	Candidate	Party	Popular vote	Result	%
2	1828	Andrew Jackson	Democratic	642806	win	56.203927
3	1828	John Quincy Adams	National Republican	500897	loss	43.796073
4	1832	Andrew Jackson	Democratic	702735	win	54.574789
5	1832	Henry Clay	National Republican	484205	loss	37.603628
6	1832	William Wirt	Anti-Masonic	100715	loss	7.821583
...	...	...	...	...	...	...
177	2016	Jill Stein	Green	1457226	loss	1.073699
178	2020	Joseph Biden	Democratic	81268924	win	51.311515
179	2020	Donald Trump	Republican	74216154	loss	46.858542
180	2020	Jo Jorgensen	Libertarian	1865724	loss	1.177979
181	2020	Howard Hawkins	Green	405035	loss	0.255731

180 rows × 6 columns



URL: https://fahadsultan.com/csc272/1_programming/pandas4.html


1.4. Pandas IV: Aggregation

In this notebook, we will learn how to aggregate data using pandas. This generally entails grouping data by a certain column’s values and then applying a function to the groups.

1.4.1. Aggregation with .groupby

Up until this point, we have been working with individual rows of DataFrames. As data scientists, we often wish to investigate trends across a larger subset of our data. For example, we may want to compute some summary statistic (the mean, median, sum, etc.) for a group of rows in our DataFrame. To do this, we’ll use pandas GroupBy objects.

Fig. 1.26 GroupBy operation broken down into split-apply-combine steps.

Fig. 1.27 Value counts for a single column.

Fig. 1.28 Detail of the split-apply-combine operation.

A groupby operation involves some combination of splitting a DataFrame into grouped subframes, applying a function, and combining the results.

For some arbitrary DataFrame df below, the code df.groupby("year").sum() does the following:

Splits the DataFrame into sub-DataFrames with rows belonging to the same year.

Applies the sum function to each column of each sub-DataFrame.

Combines the results of sum into a single DataFrame, indexed by year.

Let’s say we had baby names for all years in a single DataFrame names

Show code cell source
	Name	Sex	Count	Year
0	Mary	F	7065	1880
1	Anna	F	2604	1880
2	Emma	F	2003	1880
3	Elizabeth	F	1939	1880
4	Minnie	F	1746	1880
...	...	...	...	...
31677	Zyell	M	5	2023
31678	Zyen	M	5	2023
31679	Zymirr	M	5	2023
31680	Zyquan	M	5	2023
31681	Zyrin	M	5	2023

2117219 rows × 4 columns

names.to_csv("../data/names.csv", index=False)


Now, if we wanted to aggregate all rows in names for a given year, we would need names.groupby("Year")

names.groupby("Year")

<pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f7a30b64bb0>


What does this strange output mean? Calling .groupby has generated a GroupBy object. You can imagine this as a set of “mini” sub-DataFrames, where each subframe contains all of the rows from names that correspond to a particular year.

The diagram below shows a simplified view of names to help illustrate this idea.

We can’t work with a GroupBy object directly – that is why you saw that strange output earlier, rather than a standard view of a DataFrame. To actually manipulate values within these “mini” DataFrames, we’ll need to call an aggregation method. This is a method that tells pandas how to aggregate the values within the GroupBy object. Once the aggregation is applied, pandas will return a normal (now grouped) DataFrame.

Aggregation functions (.min(), .max(), .mean(), .sum(), etc.) are the most common way to work with GroupBy objects. These functions are applied to each column of a “mini” grouped DataFrame. We end up with a new DataFrame with one aggregated row per subframe. Let’s see this in action by finding the sum of all counts for each year in names – this is equivalent to finding the number of babies born in each year.

names.groupby("Year").sum().head(5)

	Count
Year	
1880	201484
1881	192690
1882	221533
1883	216944
1884	243461

We can relate this back to the diagram we used above. Remember that the diagram uses a simplified version of names, which is why we see smaller values for the summed counts.

Calling .agg has condensed each subframe back into a single row. This gives us our final output: a DataFrame that is now indexed by "Year", with a single row for each unique year in the original names DataFrame.

You may be wondering: where did the "State", "Sex", and "Name" columns go? Logically, it doesn’t make sense to sum the string data in these columns (how would we add “Mary” + “Ann”?). Because of this, pandas will simply omit these columns when it performs the aggregation on the DataFrame. Since this happens implicitly, without the user specifying that these columns should be ignored, it’s easy to run into troubling situations where columns are removed without the programmer noticing. It is better coding practice to select only the columns we care about before performing the aggregation.

# Same result, but now we explicitly tell pandas to only consider the "Count" column when summing
names.groupby("Year")[["Count"]].sum().head(5)

	Count
Year	
1880	201484
1881	192690
1882	221533
1883	216944
1884	243461

There are many different aggregations that can be applied to the grouped data. The primary requirement is that an aggregation function must:

Take in a Series of data (a single column of the grouped subframe)

Return a single value that aggregates this Series

Because of this fairly broad requirement, pandas offers many ways of computing an aggregation.

In-built Python operations – such as sum, max, and min – are automatically recognized by pandas.

# What is the maximum count for each name in any year?
names.groupby("Name")[["Count"]].max().head()

	Count
Name	
Aaban	16
Aabha	9
Aabid	6
Aabidah	5
Aabir	5
# What is the minimum count for each name in any year?
names.groupby("Name")[["Count"]].min().head()

	Count
Name	
Aaban	5
Aabha	5
Aabid	5
Aabidah	5
Aabir	5
# What is the average count for each name across all years?
names.groupby("Name")[["Count"]].mean().head()

	Count
Name	
Aaban	10.000000
Aabha	6.375000
Aabid	5.333333
Aabidah	5.000000
Aabir	5.000000

pandas also offers a number of in-built functions for aggregation. Some examples include:

.sum()

.max()

.min()

.mean()

.first()

.last()

The latter two entries in this list – "first" and "last" – are unique to pandas. They return the first or last entry in a subframe column. Why might this be useful? Consider a case where multiple columns in a group share identical information. To represent this information in the grouped output, we can simply grab the first or last entry, which we know will be identical to all other entries.

Let’s illustrate this with an example. Say we add a new column to names that contains the first letter of each name.

# Imagine we had an additional column, "First Letter". We'll explain this code next week
names["First Letter"] = names["Name"].apply(lambda x: x[0])

# We construct a simplified DataFrame containing just a subset of columns
names_new = names[["Name", "First Letter", "Year"]]
names_new.head()

	Name	First Letter	Year
0	Mary	M	1880
1	Anna	A	1880
2	Emma	E	1880
3	Elizabeth	E	1880
4	Minnie	M	1880

If we form groups for each name in the dataset, "First Letter" will be the same for all members of the group. This means that if we simply select the first entry for "First Letter" in the group, we’ll represent all data in that group.

We can use a dictionary to apply different aggregation functions to each column during grouping.

names_new.groupby("Name").agg({"First Letter":"first", "Year":"max"}).head()

	First Letter	Year
Name		
Aaban	A	2019
Aabha	A	2021
Aabid	A	2018
Aabidah	A	2018
Aabir	A	2018

We can also define aggregation functions of our own! This can be done using either a def or lambda statement. Again, the condition for a custom aggregation function is that it must take in a Series and output a single scalar value.

def ratio_to_peak(series):
    return series.iloc[-1]/max(series)

names.groupby("Name")[["Year", "Count"]].apply(ratio_to_peak)

---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
Input In [2], in <cell line: 4>()
      1 def ratio_to_peak(series):
      2     return series.iloc[-1]/max(series)
----> 4 names.groupby("Name")[["Year", "Count"]].apply(ratio_to_peak)

NameError: name 'names' is not defined


Note

lambda functions are a special type of function that can be defined in a single line. They are also often refered to as “anonymous” functions because these functions don’t have a name. They are useful for simple functions that are not used elsewhere in your code.

# Alternatively, using lambda
names.groupby("Name")[["Year", "Count"]].agg(lambda s: s.iloc[-1]/max(s))

	Year	Count
Name		
Aaban	1.0	0.375000
Aabha	1.0	0.555556
Aabid	1.0	1.000000
Aabidah	1.0	1.000000
Aabir	1.0	1.000000
...	...	...
Zyvion	1.0	1.000000
Zyvon	1.0	1.000000
Zyyanna	1.0	1.000000
Zyyon	1.0	1.000000
Zzyzx	1.0	1.000000

101338 rows × 2 columns

1.4.1.1. Aggregation with lambda Functions

We’ll work with the elections DataFrame again.

import pandas as pd

url = "https://raw.githubusercontent.com/fahadsultan/csc272/main/data/elections.csv"
elections = pd.read_csv(url)
elections.head(5)

	Year	Candidate	Party	Popular vote	Result	%
0	1824	Andrew Jackson	Democratic-Republican	151271	loss	57.210122
1	1824	John Quincy Adams	Democratic-Republican	113142	win	42.789878
2	1828	Andrew Jackson	Democratic	642806	win	56.203927
3	1828	John Quincy Adams	National Republican	500897	loss	43.796073
4	1832	Andrew Jackson	Democratic	702735	win	54.574789

What if we wish to aggregate our DataFrame using a non-standard function – for example, a function of our own design? We can do so by combining .agg with lambda expressions.

Let’s first consider a puzzle to jog our memory. We will attempt to find the Candidate from each Party with the highest % of votes.

A naive approach may be to group by the Party column and aggregate by the maximum.

elections.groupby("Party").agg(max).head(10)

	Year	Candidate	Popular vote	Result	%
Party					
American	1976	Thomas J. Anderson	873053	loss	21.554001
American Independent	1976	Lester Maddox	9901118	loss	13.571218
Anti-Masonic	1832	William Wirt	100715	loss	7.821583
Anti-Monopoly	1884	Benjamin Butler	134294	loss	1.335838
Citizens	1980	Barry Commoner	233052	loss	0.270182
Communist	1932	William Z. Foster	103307	loss	0.261069
Constitution	2016	Michael Peroutka	203091	loss	0.152398
Constitutional Union	1860	John Bell	590901	loss	12.639283
Democratic	2020	Woodrow Wilson	81268924	win	61.344703
Democratic-Republican	1824	John Quincy Adams	151271	win	57.210122

This approach is clearly wrong – the DataFrame claims that Woodrow Wilson won the presidency in 2020.

Why is this happening? Here, the max aggregation function is taken over every column independently. Among Democrats, max is computing:

The most recent Year a Democratic candidate ran for president (2020)

The Candidate with the alphabetically “largest” name (“Woodrow Wilson”)

The Result with the alphabetically “largest” outcome (“win”)

Instead, let’s try a different approach. We will:

Sort the DataFrame so that rows are in descending order of %

Group by Party and select the first row of each sub-DataFrame

While it may seem unintuitive, sorting elections by descending order of % is extremely helpful. If we then group by Party, the first row of each groupby object will contain information about the Candidate with the highest voter %.

elections_sorted_by_percent = elections.sort_values("%", ascending=False)
elections_sorted_by_percent.head(5)

	Year	Candidate	Party	Popular vote	Result	%
114	1964	Lyndon Johnson	Democratic	43127041	win	61.344703
91	1936	Franklin Roosevelt	Democratic	27752648	win	60.978107
120	1972	Richard Nixon	Republican	47168710	win	60.907806
79	1920	Warren Harding	Republican	16144093	win	60.574501
133	1984	Ronald Reagan	Republican	54455472	win	59.023326
elections_sorted_by_percent.groupby("Party").agg(lambda x : x.iloc[0]).head(10)

# Equivalent to the below code
# elections_sorted_by_percent.groupby("Party").agg('first').head(10)

	Year	Candidate	Popular vote	Result	%
Party					
American	1856	Millard Fillmore	873053	loss	21.554001
American Independent	1968	George Wallace	9901118	loss	13.571218
Anti-Masonic	1832	William Wirt	100715	loss	7.821583
Anti-Monopoly	1884	Benjamin Butler	134294	loss	1.335838
Citizens	1980	Barry Commoner	233052	loss	0.270182
Communist	1932	William Z. Foster	103307	loss	0.261069
Constitution	2008	Chuck Baldwin	199750	loss	0.152398
Constitutional Union	1860	John Bell	590901	loss	12.639283
Democratic	1964	Lyndon Johnson	43127041	win	61.344703
Democratic-Republican	1824	Andrew Jackson	151271	loss	57.210122

Here’s an illustration of the process:

Notice how our code correctly determines that Lyndon Johnson from the Democratic Party has the highest voter %.

More generally, lambda functions are used to design custom aggregation functions that aren’t pre-defined by Python. The input parameter x to the lambda function is a GroupBy object. Therefore, it should make sense why lambda x : x.iloc[0] selects the first row in each groupby object.

In fact, there’s a few different ways to approach this problem. Each approach has different tradeoffs in terms of readability, performance, memory consumption, complexity, etc. We’ve given a few examples below.

Note: Understanding these alternative solutions is not required. They are given to demonstrate the vast number of problem-solving approaches in pandas.

# Using the idxmax function
best_per_party = elections.loc[elections.groupby('Party')['%'].idxmax()]
best_per_party.head(5)

	Year	Candidate	Party	Popular vote	Result	%
22	1856	Millard Fillmore	American	873053	loss	21.554001
115	1968	George Wallace	American Independent	9901118	loss	13.571218
6	1832	William Wirt	Anti-Masonic	100715	loss	7.821583
38	1884	Benjamin Butler	Anti-Monopoly	134294	loss	1.335838
127	1980	Barry Commoner	Citizens	233052	loss	0.270182
# Using the .drop_duplicates function
best_per_party2 = elections.sort_values('%').drop_duplicates(['Party'], keep='last')
best_per_party2.head(5)

	Year	Candidate	Party	Popular vote	Result	%
148	1996	John Hagelin	Natural Law	113670	loss	0.118219
164	2008	Chuck Baldwin	Constitution	199750	loss	0.152398
110	1956	T. Coleman Andrews	States' Rights	107929	loss	0.174883
147	1996	Howard Phillips	Taxpayers	184656	loss	0.192045
136	1988	Lenora Fulani	New Alliance	217221	loss	0.237804
1.4.1.2. Other GroupBy Features

There are many aggregation methods we can use with .agg. Some useful options are:

.mean: creates a new DataFrame with the mean value of each group

.sum: creates a new DataFrame with the sum of each group

.max and .min: creates a new DataFrame with the maximum/minimum value of each group

.first and .last: creates a new DataFrame with the first/last row in each group

.size: creates a new Series with the number of entries in each group

.count: creates a new DataFrame with the number of entries, excluding missing values.

Note the slight difference between .size() and .count(): while .size() returns a Series and counts the number of entries including the missing values, .count() returns a DataFrame and counts the number of entries in each column excluding missing values. Here’s an example:

df = pd.DataFrame({'letter':['A','A','B','C','C','C'], 
                   'num':[1,2,3,4,None,4], 
                   'state':[None, 'tx', 'fl', 'hi', None, 'ak']})
df

	letter	num	state
0	A	1.0	None
1	A	2.0	tx
2	B	3.0	fl
3	C	4.0	hi
4	C	NaN	None
5	C	4.0	ak
df.groupby("letter").size()

letter
A    2
B    1
C    3
dtype: int64

df.groupby("letter").count()

	num	state
letter		
A	2	1
B	1	1
C	2	2

You might recall that the value_counts() function in the previous note does something similar. It turns out value_counts() and groupby.size() are the same, except value_counts() sorts the resulting Series in descending order automatically.

df["letter"].value_counts()

C    3
A    2
B    1
Name: letter, dtype: int64


hese (and other) aggregation functions are so common that pandas allows for writing shorthand. Instead of explicitly stating the use of .agg, we can call the function directly on the GroupBy object.

For example, the following are equivalent:

elections.groupby("Candidate").agg(mean)

elections.groupby("Candidate").mean()

There are many other methods that pandas supports. You can check them out on the pandas documentation.



1.4.1.3. Filtering by Group

Another common use for GroupBy objects is to filter data by group.

groupby.filter takes an argument 
f
, where 
f
 is a function that:

Takes a DataFrame object as input

Returns a single True or False for the each sub-DataFrame

Sub-DataFrames that correspond to True are returned in the final result, whereas those with a False value are not. Importantly, groupby.filter is different from groupby.agg in that an entire sub-DataFrame is returned in the final DataFrame, not just a single row. As a result, groupby.filter preserves the original indices.

To illustrate how this happens, consider the following .filter function applied on some arbitrary data. Say we want to identify “tight” election years – that is, we want to find all rows that correspond to elections years where all candidates in that year won a similar portion of the total vote. Specifically, let’s find all rows corresponding to a year where no candidate won more than 45% of the total vote.

In other words, we want to:

Find the years where the maximum % in that year is less than 45%

Return all DataFrame rows that correspond to these years

For each year, we need to find the maximum % among all rows for that year. If this maximum % is lower than 45%, we will tell pandas to keep all rows corresponding to that year.

elections.groupby("Year").filter(lambda sf: sf["%"].max() < 45).head(9)

	Year	Candidate	Party	Popular vote	Result	%
23	1860	Abraham Lincoln	Republican	1855993	win	39.699408
24	1860	John Bell	Constitutional Union	590901	loss	12.639283
25	1860	John C. Breckinridge	Southern Democratic	848019	loss	18.138998
26	1860	Stephen A. Douglas	Northern Democratic	1380202	loss	29.522311
66	1912	Eugene V. Debs	Socialist	901551	loss	6.004354
67	1912	Eugene W. Chafin	Prohibition	208156	loss	1.386325
68	1912	Theodore Roosevelt	Progressive	4122721	loss	27.457433
69	1912	William Taft	Republican	3486242	loss	23.218466
70	1912	Woodrow Wilson	Democratic	6296284	win	41.933422

What’s going on here? In this example, we’ve defined our filtering function, 
f
, to be lambda sf: sf["%"].max() < 45. This filtering function will find the maximum "%" value among all entries in the grouped sub-DataFrame, which we call sf. If the maximum value is less than 45, then the filter function will return True and all rows in that grouped sub-DataFrame will appear in the final output DataFrame.

Examine the DataFrame above. Notice how, in this preview of the first 9 rows, all entries from the years 1860 and 1912 appear. This means that in 1860 and 1912, no candidate in that year won more than 45% of the total vote.

You may ask: how is the groupby.filter procedure different to the boolean filtering we’ve seen previously? Boolean filtering considers individual rows when applying a boolean condition. For example, the code elections[elections["%"] < 45] will check the "%" value of every single row in elections; if it is less than 45, then that row will be kept in the output. groupby.filter, in contrast, applies a boolean condition across all rows in a group. If not all rows in that group satisfy the condition specified by the filter, the entire group will be discarded in the output.

1.4.2. Aggregation with .pivot_table

We know now that .groupby gives us the ability to group and aggregate data across our DataFrame. The examples above formed groups using just one column in the DataFrame. It’s possible to group by multiple columns at once by passing in a list of column names to .groupby.

Let’s consider the names dataset. In this problem, we will find the total number of baby names associated with each sex for each year. To do this, we’ll group by both the "Year" and "Sex" columns.

names.head()

	Name	Sex	Count	Year	First Letter
0	Mary	F	7065	1880	M
1	Anna	F	2604	1880	A
2	Emma	F	2003	1880	E
3	Elizabeth	F	1939	1880	E
4	Minnie	F	1746	1880	M
# Find the total number of baby names associated with each sex for each year in the data
names.groupby(["Year", "Sex"])[["Count"]].sum().head(6)

		Count
Year	Sex	
1880	F	90994
M	110490
1881	F	91953
M	100737
1882	F	107847
M	113686

Notice that both "Year" and "Sex" serve as the index of the DataFrame (they are both rendered in bold). We’ve created a multi-index DataFrame where two different index values, the year and sex, are used to uniquely identify each row.

This isn’t the most intuitive way of representing this data – and, because multi-indexed DataFrames have multiple dimensions in their index, they can often be difficult to use.

Another strategy to aggregate across two columns is to create a pivot table. One set of values is used to create the index of the pivot table; another set is used to define the column names. The values contained in each cell of the table correspond to the aggregated data for each index-column pair.

The best way to understand pivot tables is to see one in action. Let’s return to our original goal of summing the total number of names associated with each combination of year and sex. We’ll call the pandas .pivot_table method to create a new table.

# The `pivot_table` method is used to generate a Pandas pivot table
names.pivot_table(
    index = "Year", 
    columns = "Sex", 
    values = "Count", 
    aggfunc = sum).head(5)

Sex	F	M
Year		
1880	90994	110490
1881	91953	100737
1882	107847	113686
1883	112319	104625
1884	129019	114442

Looks a lot better! Now, our DataFrame is structured with clear index-column combinations. Each entry in the pivot table represents the summed count of names for a given combination of "Year" and "Sex".

Let’s take a closer look at the code implemented above.

index = "Year" specifies the column name in the original DataFrame that should be used as the index of the pivot table

columns = "Sex" specifies the column name in the original DataFrame that should be used to generate the columns of the pivot table

values = "Count" indicates what values from the original DataFrame should be used to populate the entry for each index-column combination

aggfunc = sum tells pandas what function to use when aggregating the data specified by values. Here, we are summing the name counts for each pair of "Year" and "Sex"

Fig. 1.29 A pivot table is a way of summarizing data in a DataFrame for a particular purpose. It makes heavy use of the aggregation function. A pivot table is itself a DataFrame, where the rows represent one variable that you’re interested in, the columns another, and the cell’s some aggregate value. A pivot table also tends to include marginal values (like sums) for each row and column.

We can even include multiple values in the index or columns of our pivot tables.

names_pivot = names.pivot_table(
    index="Year",     # the rows (turned into index)
    columns="Sex",    # the column values
    values=["Count", "Name"], 
    aggfunc=max,   # group operation
)
names_pivot.head(6)

	Count	Name
Sex	F	M	F	M
Year				
1880	7065	9655	Zula	Zeke
1881	6919	8769	Zula	Zeb
1882	8148	9557	Zula	Zed
1883	8012	8894	Zula	Zeno
1884	9217	9388	Zula	Zollie
1885	9128	8756	Zula	Zollie



URL: https://fahadsultan.com/csc272/1_programming/pandas5.html


1.5. Pandas V: Concatenation and Merging
1.5.1. Concatenating DataFrames

Another way to combine DataFrames is to concatenate them. Concatenation is a bit different from joining. When we join two DataFrames, we are combining them horizontally – that is, we are adding new columns to an existing DataFrame. Concatenation, on the other hand, is generally a vertical operation – we are adding new rows to an existing DataFrame.

pd.concat is the pandas method used to concatenate DataFrames together. It takes as input a list of DataFrames to be concatenated and returns a new DataFrame containing all of the rows from each input DataFrame in the input list.

Let’s say we wanted to concatenate data from two different years in babynames. We can do so using the pd.concat method.

import pandas as pd 
url_template = "https://raw.githubusercontent.com/fahadsultan/csc272/main/data/names/yob%s.txt"

data_list = []
for year in range(1880, 2023):
    url = url_template % year
    data = pd.read_csv(url, header=None, names=['name', 'sex', 'count'])
    data['year'] = year
    data_list.append(data)
all_data = pd.concat(data_list)
all_data

	name	sex	count	year
0	Mary	F	7065	1880
1	Anna	F	2604	1880
2	Emma	F	2003	1880
3	Elizabeth	F	1939	1880
4	Minnie	F	1746	1880
...	...	...	...	...
31910	Zuberi	M	5	2022
31911	Zydn	M	5	2022
31912	Zylon	M	5	2022
31913	Zymeer	M	5	2022
31914	Zymeire	M	5	2022

2085158 rows × 4 columns

1.5.2. Merging DataFrames

When working on data science projects, we’re unlikely to have absolutely all the data we want contained in a single DataFrame – a real-world data scientist needs to grapple with data coming from multiple sources. If we have access to multiple datasets with related information, we can merge two or more tables into a single DataFrame.

To put this into practice, we’ll revisit the elections dataset.

elections.head(5)

	Year	Candidate	Party	Popular vote	Result	%
0	1824	Andrew Jackson	Democratic-Republican	151271	loss	57.210122
1	1824	John Quincy Adams	Democratic-Republican	113142	win	42.789878
2	1828	Andrew Jackson	Democratic	642806	win	56.203927
3	1828	John Quincy Adams	National Republican	500897	loss	43.796073
4	1832	Andrew Jackson	Democratic	702735	win	54.574789

Say we want to understand the popularity of the names of each presidential candidate in 2020. To do this, we’ll need the combined data of names and elections.

Fig. 1.30 Merging two DataFrames

We’ll start by creating a new column containing the first name of each presidential candidate. This will help us join each name in elections to the corresponding name data in names.

# This `str` operation splits each candidate's full name at each 
# blank space, then takes just the candidiate's first name
elections["First Name"] = elections["Candidate"].str.split().str[0]
elections.head(5)

	Year	Candidate	Party	Popular vote	Result	%	First Name
0	1824	Andrew Jackson	Democratic-Republican	151271	loss	57.210122	Andrew
1	1824	John Quincy Adams	Democratic-Republican	113142	win	42.789878	John
2	1828	Andrew Jackson	Democratic	642806	win	56.203927	Andrew
3	1828	John Quincy Adams	National Republican	500897	loss	43.796073	John
4	1832	Andrew Jackson	Democratic	702735	win	54.574789	Andrew
# Here, we'll only consider `names` data from 2020
names_2020 = names[names["Year"]==2020]
names_2020.head()

	Name	Sex	Count	Year	First Letter
0	Olivia	F	17641	2020	O
1	Emma	F	15656	2020	E
2	Ava	F	13160	2020	A
3	Charlotte	F	13065	2020	C
4	Sophia	F	13036	2020	S

Now, we’re ready to merge the two tables. pd.merge is the pandas method used to merge DataFrames together.

merged = pd.merge(left = elections, right = names_2020, \
                  left_on = "First Name", right_on = "Name")
merged.head()
# Notice that pandas automatically specifies `Year_x` and `Year_y` 
# when both merged DataFrames have the same column name to avoid confusion

	Year_x	Candidate	Party	Popular vote	Result	%	First Name	Name	Sex	Count	Year_y	First Letter
0	1824	Andrew Jackson	Democratic-Republican	151271	loss	57.210122	Andrew	Andrew	F	12	2020	A
1	1824	Andrew Jackson	Democratic-Republican	151271	loss	57.210122	Andrew	Andrew	M	6036	2020	A
2	1828	Andrew Jackson	Democratic	642806	win	56.203927	Andrew	Andrew	F	12	2020	A
3	1828	Andrew Jackson	Democratic	642806	win	56.203927	Andrew	Andrew	M	6036	2020	A
4	1832	Andrew Jackson	Democratic	702735	win	54.574789	Andrew	Andrew	F	12	2020	A

Let’s take a closer look at the parameters:

left and right parameters are used to specify the DataFrames to be merge.

left_on and right_on parameters are assigned to the string names of the columns to be used when performing the merge. These two on parameters tell pandas what values should act as pairing keys to determine which rows to merge across the DataFrames. We’ll talk more about this idea of a pairing key next lecture.



URL: https://fahadsultan.com/csc272/1_programming/40_modalities.html


2. Encoding and Representation

Data exists in many forms, formats and modalities. Even missing data is a form of data.

Regardless of its form, format or modality, all data ultimately needs to be transformed into a matrix (or matrices) of numbers for it to be used for any machine learning or data mining algorithm.

The process of transforming raw data into numeric matrices is often referred to as Encoding and the resulting numeric matrices are referred to as Representations. These representations may focus on certain aspects (structure, content, semantics etc.) of the data more than others and may be more or less suitable for certain tasks.

Fig. 2.1 All data is ultimately transformed into a matrix of numbers.



URL: https://fahadsultan.com/csc272/1_programming/41_types.html


2.1. Feature Types

Tabular data (pd.DataFrame), as discussed previously, is made up of observations (rows) and features (columns). Data type (df.dtypes) of features fall into two primary categories: numeric and categorical.

There also exists a third special category of data type called missing. Missing data is a special data type because it is not a data type at all. It is a placeholder for a value that is not known or not applicable. Missing data is represented by NaN (not a number) in pandas. More on missing data in a bit.

Fig. 2.2 Classification of feature types

To study these feature types, we will use the dataset of food safety scores for restaurants in San Francisco. The scores and violation information have been made available by the San Francisco Department of Public Health.

import pandas as pd 

data = pd.read_csv('https://raw.githubusercontent.com/fahadsultan/csc272/main/data/restaurants_truncated.csv', index_col=0)
data.head()

	id	zip	phone	lat	lng	type	score	risk	violation
0	70064	94103.0	1.415565e+10	NaN	NaN	Routine - Unscheduled	75.0	High Risk	Improper reheating of food
1	90039	94103.0	NaN	NaN	NaN	Routine - Unscheduled	81.0	High Risk	High risk food holding temperature
2	89059	94115.0	1.415369e+10	NaN	NaN	Complaint	NaN	NaN	NaN
3	91044	94112.0	NaN	NaN	NaN	Routine - Unscheduled	84.0	Moderate Risk	Inadequate and inaccessible handwashing facili...
4	62768	94122.0	NaN	37.765421	-122.477256	Routine - Unscheduled	90.0	Low Risk	Food safety certificate or food handler card n...
2.1.1. Missing Data

Missing data occurs commonly in many data analysis applications. One of the goals of pandas is to make working with missing data as painless as possible. For example, all of the descriptive statistics on pandas objects exclude missing data by default.

The way that missing data is represented in pandas objects is somewhat imperfect, but it is sufficient for most real-world use. For data with float64 dtype, pandas uses the floating-point value NaN (Not a Number) to represent missing data.

We call this a sentinel value: when present, it indicates a missing (or null) value.

The isna method gives us a Boolean Series with True where values are null:

data.isna().sum()

id            0
zip           1
phone        27
lat          30
lng          30
type          0
score        15
risk         17
violation    17
dtype: int64


In pandas, missing data is also refered to as NA, which stands for Not Available. In statistics applications, NA data may either be data that does not exist or that exists but was not observed (through problems with data collection, for example). When cleaning up data for analysis, it is often important to do analysis on the missing data itself to identify data collection problems or potential biases in the data caused by missing data.

The built-in Python None value is also treated as NA.

Method

	

Description




dropna

	

Filter axis labels based on whether values for each label have missing data, with varying thresholds for how much missing data to tolerate.




fillna

	

Fill in missing data with some value




isna

	

Return Boolean values indicating which values are missing/NA.




notna

	

Negation of isna, returns True for non-NA values and False for NA values.

2.1.2. Numerical Features

Numeric data is data that can be represented as numbers. These variables generally describe some numeric quantity or amount and are also sometimes referred to as “quantitative” variables.

Since numerical features are already represented as numbers, they are already ready to be used in machine learning models and there is no need to encode them.

In the example above, numerical features include zip, phone, lat, lng, score.

Show code cell source
	zip	phone	lat	lng	score
0	94105	NaN	37.787925	-122.400953	82.0
1	94109	NaN	37.786108	-122.425764	NaN
2	94115	NaN	37.791607	-122.434563	82.0
3	94115	NaN	37.788932	-122.433895	78.0
4	94110	NaN	37.739161	-122.416967	94.0
2.1.2.1. Discrete Features

Discrete data is data that is counted. For example, the number of students in a class is discrete data. You can count the number of students in a class. You can not count the number of students in a class and get a fraction of a student. You can only count whole students.

In the restaurants inspection data set, zip, phone, score are discrete features.

Show code cell source
	zip	phone	score
0	94105	NaN	82.0
1	94109	NaN	NaN
2	94115	NaN	82.0
3	94115	NaN	78.0
4	94110	NaN	94.0
2.1.2.2. Continuous Features

Continuous data is data that is measured. For example, the height of a student is continuous data. You can measure the height of a student. You can measure the height of a student and get a fraction of a student. You can measure a student and get a height of 5 feet and 6.5 inches.

In the restaurants inspection data set, lat, lng are continuous features.

Show code cell source
	lat	lng
0	37.787925	-122.400953
1	37.786108	-122.425764
2	37.791607	-122.434563
3	37.788932	-122.433895
4	37.739161	-122.416967
2.1.3. Categorical Features

Categorical data is data that is not numeric. It is often represented as text or a set of text values. These variables generally describe some characteristic or quality of a data unit, and are also sometimes referred to as “qualitative” variables.

Show code cell source
	type	risk	violation
0	Routine - Unscheduled	High Risk	High risk food holding temperature
1	Complaint	NaN	NaN
2	Routine - Unscheduled	Low Risk	Inadequate warewashing facilities or equipment
3	Routine - Unscheduled	Low Risk	Improper food storage
4	Routine - Unscheduled	Low Risk	Unapproved or unmaintained equipment or utensils
2.1.3.1. Ordinal Features

Ordinal data is data that is ordered in some way. For example, the size of a t-shirt is ordinal data. The sizes are ordered from smallest to largest. The sizes are more or less than each other. They are different and ordered.

Show code cell source
	risk
0	High Risk
1	NaN
2	Low Risk
3	Low Risk
4	Low Risk
2.1.3.1.1. Encoding Ordinal Features

Ordinal features can be encoded using a technique called label encoding. Label encoding is simply converting each value in a column to a number. For example, the sizes of t-shirts could be represented as 0 (XS), 1 (S), 2 (M), 3 (L), 4 (XL), 5 (XXL).

data['risk_enc'] = data['risk'].replace({'Low Risk': 0, 'Moderate Risk': 1, 'High Risk': 2})
data.head()

	id	zip	phone	lat	lng	type	score	risk	violation	risk_enc
0	64454	94105	NaN	37.787925	-122.400953	Routine - Unscheduled	82.0	High Risk	High risk food holding temperature	2.0
1	33014	94109	NaN	37.786108	-122.425764	Complaint	NaN	NaN	NaN	NaN
2	1526	94115	NaN	37.791607	-122.434563	Routine - Unscheduled	82.0	Low Risk	Inadequate warewashing facilities or equipment	0.0
3	73	94115	NaN	37.788932	-122.433895	Routine - Unscheduled	78.0	Low Risk	Improper food storage	0.0
4	66402	94110	NaN	37.739161	-122.416967	Routine - Unscheduled	94.0	Low Risk	Unapproved or unmaintained equipment or utensils	0.0
2.1.3.2. Nominal Features

Nominal data is data that is not ordered in any way. For example, the color of a car is nominal data. There is no order to the colors. The colors are not more or less than each other. They are just different.

data[['type', 'violation']].head()

	type	violation
0	Routine - Unscheduled	High risk food holding temperature
1	Complaint	NaN
2	Routine - Unscheduled	Inadequate warewashing facilities or equipment
3	Routine - Unscheduled	Improper food storage
4	Routine - Unscheduled	Unapproved or unmaintained equipment or utensils
2.1.3.2.1. Encoding Nominal Features

Since nominal features don’t have any order, encoding them requires some creativity. The most common way to encode nominal features is to use a technique called one-hot encoding. One-hot encoding creates a new column for each unique value in the nominal feature. Each new column is a binary feature that indicates whether or not the original observation had that value.

The figure below illustrates how one-hot encoding for a “day” (of the week) column:

Fig. 2.3 One-hot encoding

pandas has a built-in .get_dummies function for doing this:

pd.get_dummies(data['type']).head()

	Complaint	New Construction	New Ownership	Reinspection/Followup	Routine - Unscheduled
0	0	0	0	0	1
1	0	0	0	0	1
2	1	0	0	0	0
3	0	0	0	0	1
4	0	0	0	0	1



URL: https://fahadsultan.com/csc272/1_programming/42_formats.html


2.2. Common Data Formats

Format is a term used to describe the way data is stored. For example, a single image is stored as a 2D array of pixels. A video is stored as a sequence of images. A sound is stored as a 1D array of samples. A text is stored as a sequence of characters.

2.2.1. Flat Formats

Flat data formats are native to pandas and are the simplest and the most ubiquitous file formats in general.

To avoid data redundancy, data is often factored into multiple tables. For example, in a database of a school, there may be a table for students, a table for teachers, a table for classes, a table for grades, etc. Depending on the question of interest, these tables are then joined together to form a single table.

2.2.1.1. Comma Separated Values (CSV)

CSV is an open format used to store tabular data. It is a text file where each line is a row of data. In other words, each line is separated by newline character \n. Within a row, each column is separated by a comma ,. The first row is optionally the header row containing the names of the columns.

Example of a CSV file is the elections.csv file.

In pandas you can read a CSV file as a pandas DataFrame using the pd.read_csv() function.

The first input to pd.read_csv() is the filename or filepath that you want to read. The other three most important parameters of the pd.read_csv() function are:

sep: (default: sep=',') specifies the separator used to separate columns. Default is , which means the columns are separated by a comma.

header: (default: header=0) specifies the row number to be used as the header. Default is 0 which means the first row is assumed to contain column names. If the file does not contain a header row, then header=None should be used.

names: (default: names=None) specifies the column names as a list of strings. Default is None which means the column names are read from the header row.

index_col: (default: index_col=None) specifies the column number or column name to be used as the index. Default is None which means the index is automatically generated as integers starting from 0. If the file does not contain a header row, then index_col=0 should be used.

import pandas as pd 

url  = 'https://raw.githubusercontent.com/fahadsultan/csc272/main/data/elections.csv'
data = pd.read_csv(url)
data.head()

	Year	Candidate	Party	Popular vote	Result	%
0	1824	Andrew Jackson	Democratic-Republican	151271	loss	57.210122
1	1824	John Quincy Adams	Democratic-Republican	113142	win	42.789878
2	1828	Andrew Jackson	Democratic	642806	win	56.203927
3	1828	John Quincy Adams	National Republican	500897	loss	43.796073
4	1832	Andrew Jackson	Democratic	702735	win	54.574789

To write a DataFrame to a CSV file, use the df.to_csv() function. The first input to df.to_csv() is the filename or filepath that you want to write to. Other important parameters of the df.to_csv() function are:

sep: (default: sep=',') specifies the separator used to separate columns. Default is , which means the columns are separated by a comma.

header: (default: header=True) specifies whether to write the header row. Default is True which means the header row is written. If you don’t want to write the header row, then header=False should be used.

index: (default: index=True) specifies whether to write the index column. Default is True which means the index column is written. If you don’t want to write the index column, then index=False should be used.

data.to_csv('elections.csv')


The CSV format has many variations, each different in regard of the separator used. For instance, a particularly popular variant called TSV (Tab Separated Values) uses tab character \t to separate columns instead of a comma. In other variants the column separator can also be a semicolon ; or a pipe |.

Example of a TSV file is restaurants.tsv file.

Note the use of sep='\t' parameter in pd.read_csv() in the code below:

import pandas as pd 
url  = "https://raw.githubusercontent.com/fahadsultan/csc272/main/data/restaurants.tsv"
data = pd.read_csv(url, sep='\t', index_col=0)
data.head()

	business_id	business_name	business_address	business_city	business_state	business_postal_code	business_latitude	business_longitude	business_location	business_phone_number	...	inspection_score	inspection_type	violation_id	violation_description	risk_category	Neighborhoods	SF Find Neighborhoods	Current Police Districts	Current Supervisor Districts	Analysis Neighborhoods
0	835	Kam Po Kitchen	801 Broadway St	San Francisco	CA	94133	37.797223	-122.410513	POINT (-122.410513 37.797223)	NaN	...	88.0	Routine - Unscheduled	835_20180917_103139	Improper food storage	Low Risk	107.0	107.0	6.0	3.0	6.0
1	905	Working Girls' Cafe'	0259 Kearny St	San Francisco	CA	94108	37.790477	-122.404033	POINT (-122.404033 37.790477)	NaN	...	87.0	Routine - Unscheduled	905_20190415_103114	High risk vermin infestation	High Risk	19.0	19.0	6.0	3.0	8.0
2	1203	TAWAN'S THAI FOOD	4403 GEARY Blvd	San Francisco	CA	94118	37.780834	-122.466590	POINT (-122.46659 37.780834)	1.415576e+10	...	77.0	Routine - Unscheduled	1203_20170803_103120	Moderate risk food holding temperature	Moderate Risk	5.0	5.0	8.0	4.0	11.0
3	1345	Cordon Bleu	1574 California St	San Francisco	CA	94109	37.790683	-122.420264	POINT (-122.420264 37.790683)	NaN	...	81.0	Routine - Unscheduled	1345_20170928_103105	Improper cooling methods	High Risk	105.0	105.0	4.0	3.0	21.0
4	1352	LA TORTILLA	495 Castro St B	San Francisco	CA	94114	37.760954	-122.434935	POINT (-122.434935 37.760954)	1.415586e+10	...	74.0	Routine - Unscheduled	1352_20180620_103177	Non service animal	Low Risk	38.0	38.0	3.0	5.0	5.0

5 rows × 22 columns

2.2.1.2. Excel Spreadsheet (XLSX)

XLSX is a proprietary format also used to store tabular data. Unlike a CSV, an XLSX is not a plain text file that you can simply read in any text editor. In contrast, an XLSX is a binary file which can be read only in specific software such as Microsft Excel or OpenOffice Calc. An excel spreedsheet does a lot more than just store(tabular) data such as storing formulas, charts and images, etc.

However, for our purposes here, the only distinction between a CSV file and an XLSX file is that a) an XLSX file can contain multiple sheets where each sheet is a table and b) you can read an XLSX file in pandas using the pd.read_excel() function.

pd.read_excel is very similar to pd.read_csv where the first input is the filename or filepath that you want to read. Other inputs such as header, names, index_col are the same as pd.read_csv. The only additional input is:

sheet_name: (default: sheet_name=0) specifies the sheet number or sheet name to be read. Default is 0 which means the first sheet is read. If the file contains multiple sheets, then sheet_name=None should be used.

You can download a sample excel spreadsheet from here

Note that the file contains two sheets: “All Data” and “Just US”

The line of code below reads in just the sheet labeled “Just US” from the spreadsheet, using the sheet_name parameter:

pd.read_excel?

import pandas as pd 
data = pd.read_excel('../data/Financial Sample.xlsx', sheet_name='US Only')
data.head()

	Segment	Country	Product	Discount Band	Units Sold	Manufacturing Price	Sale Price	Gross Sales	Discounts	Sales	COGS	Profit	Date	Month Number	Month Name	Year
0	Midmarket	United States of America	Montana	None	615.0	5	15	9225.0	0.0	9225.0	6150.0	3075.0	41974	12	December	2014
1	Government	United States of America	Paseo	None	1143.0	10	7	8001.0	0.0	8001.0	5715.0	2286.0	41913	10	October	2014
2	Channel Partners	United States of America	Paseo	None	912.0	10	12	10944.0	0.0	10944.0	2736.0	8208.0	41579	11	November	2013
3	Enterprise	United States of America	Velo	None	2821.0	120	125	352625.0	0.0	352625.0	338520.0	14105.0	41852	8	August	2014
4	Channel Partners	United States of America	Amarilla	None	1953.0	260	12	23436.0	0.0	23436.0	5859.0	17577.0	41730	4	April	2014

Just as you can write any pandas DataFrame to CSV file using df.to_csv(), you can write any DataFrame to XLSX file using the df.to_excel() function.

2.2.2. Hierarchical (Nested) Formats

Hierarchical data formats are used to store data that is inherently hierarchical. These formats are particularly popular on the internet for exchange of information with web services using APIs (Application Programming Interfaces).

2.2.2.1. Extensible Markup Language (XML)

XML is a format used to store hierarchical data. Data in XML is stored as a tree structure. This tree is constituent of nodes. Each node has a start tag and an end tag. The start tag is enclosed in angle brackets < and > e.g. <name>. The end tag is also enclosed in angle brackets but it also has a forward slash / after the opening angle bracket e.g. </name>.

The start tag and the end tag together are called an element.

The start tag can optionally contain attributes. Attributes are name-value pairs. The value is enclosed in double quotes ". The start tag can optionally contain child elements. Child elements are enclosed between the start tag and the end tag. The end tag can optionally contain text. Text is the value of the node. The text is enclosed between the start tag and the end tag.

The first node in an XML file is called the root node.

Example of an XML file is

<note>
<to>Tove</to>
<from>Jani</from>
<heading>Reminder</heading>
<body>Don't forget me this weekend!</body>
</note>

<breakfast_menu>
    <food>
        <name>Belgian Waffles</name>
        <price>$5.95</price>
        <description>Two of our famous Belgian Waffles with plenty of real maple syrup</description>
        <calories>650</calories>
    </food>
    <food>
        <name>Strawberry Belgian Waffles</name>
        <price>$7.95</price>
        <description>Light Belgian waffles covered with strawberries and whipped cream</description>
        <calories>900</calories>
    </food>
    <food>
        <name>Berry-Berry Belgian Waffles</name>
        <price>$8.95</price>
        <description>Light Belgian waffles covered with an assortment of fresh berries and whipped cream</description>
        <calories>900</calories>
    </food>
    <food>
        <name>French Toast</name>
        <price>$4.50</price>
        <description>Thick slices made from our homemade sourdough bread</description>
        <calories>600</calories>
    </food>
    <food>
        <name>Homestyle Breakfast</name>
        <price>$6.95</price>
        <description>Two eggs, bacon or sausage, toast, and our ever-popular hash browns</description>
        <calories>950</calories>
    </food>
</breakfast_menu>


Other XML files are: Breakfast Menu, Plant Catalog and CD catalog.

It is a text file where each line is a node of data. Each node has a name and a value. The name is separated from the value by a colon. The value is separated from the name by a colon. The first node is the root node. The root node contains the names of the nodes. The root node is separated from the data nodes by a blank line.

In pandas, you can read an XML file using the pd.read_xml() function. The first input to pd.read_xml() is the filename or filepath that you want to read. Other important parameters of the pd.read_xml() function are:

xpath: (default: xpath=None) specifies the path to the node(s) to be read. Default is None which means the entire XML file is read. If you want to read a specific node, then xpath should be used.

namespaces: (default: namespaces=None) specifies the namespaces used in the XML file. Default is None which means no namespaces are used. If the XML file uses namespaces, then namespaces should be used.

encoding: (default: encoding=None) specifies the encoding of the XML file. Default is None which means the encoding is automatically detected. If the XML file uses a specific encoding, then encoding should be used.

errors: (default: errors=None) specifies how to handle errors. Default is None which means the errors are ignored. If the XML file contains errors, then errors should be used.

data = pd.read_xml('https://www.w3schools.com/xml/simple.xml')
data.head()

	name	price	description	calories
0	Belgian Waffles	$5.95	Two of our famous Belgian Waffles with plenty ...	650
1	Strawberry Belgian Waffles	$7.95	Light Belgian waffles covered with strawberrie...	900
2	Berry-Berry Belgian Waffles	$8.95	Light Belgian waffles covered with an assortme...	900
3	French Toast	$4.50	Thick slices made from our homemade sourdough ...	600
4	Homestyle Breakfast	$6.95	Two eggs, bacon or sausage, toast, and our eve...	950
2.2.2.2. JavaScript Object Notation (JSON)

JSON is an open data format used to store hierarchical data. JSON data resembles a Python dictionary. It is a text file where each line is a key-value pair. The key is separated from the value by a colon. Similar to a Python dictionary, the value can be a string, a number, a dictionary, a boolean or a list.

JSON data can be oriented in two ways: records and columns.

In the records orientation, each line is a record.

[
  {
    "name": "Belgian Waffles",
    "price": "$5.95",
    "description": "Two of our famous Belgian Waffles with plenty of real maple syrup",
    "calories": 650
  },
  {
    "name": "Strawberry Belgian Waffles",
    "price": "$7.95",
    "description": "Light Belgian waffles covered with strawberries and whipped cream",
    "calories": 900
  },
  {
    "name": "Berry-Berry Belgian Waffles",
    "price": "$8.95",
    "description": "Light Belgian waffles covered with an assortment of fresh berries and whipped cream",
    "calories": 900
  },
  {
    "name": "French Toast",
    "price": "$4.50",
    "description": "Thick slices made from our homemade sourdough bread",
    "calories": 600
  },
  {
    "name": "Homestyle Breakfast",
    "price": "$6.95",
    "description": "Two eggs, bacon or sausage, toast, and our ever-popular hash browns",
    "calories": 950
  }
]


In the columns orientation, each line is a column. The same JSON data can be represented as a table as follows:

{
  "name": {
    "0": "Belgian Waffles",
    "1": "Strawberry Belgian Waffles",
    "2": "Berry-Berry Belgian Waffles",
    "3": "French Toast",
    "4": "Homestyle Breakfast"
  },
  "price": {
    "0": "$5.95",
    "1": "$7.95",
    "2": "$8.95",
    "3": "$4.50",
    "4": "$6.95"
  },
  "description": {
    "0": "Two of our famous Belgian Waffles with plenty of real maple syrup",
    "1": "Light Belgian waffles covered with strawberries and whipped cream",
    "2": "Light Belgian waffles covered with an assortment of fresh berries and whipped cream",
    "3": "Thick slices made from our homemade sourdough bread",
    "4": "Two eggs, bacon or sausage, toast, and our ever-popular hash browns"
  },
  "calories": {
    "0": 650,
    "1": 900,
    "2": 900,
    "3": 600,
    "4": 950
  }
}


Similar to pd.read_csv, pd.read_excel and pd.read_xml, you can read a JSON file using the pd.read_json() function.

The first input is filepath. There is no sep, header or index_col parameter because the JSON files don’t have flat structure.

import pandas as pd 
data = pd.read_json('../data/sample.json')
data.head()

	name	price	description	calories
0	Belgian Waffles	$5.95	Two of our famous Belgian Waffles with plenty ...	650
1	Strawberry Belgian Waffles	$7.95	Light Belgian waffles covered with strawberrie...	900
2	Berry-Berry Belgian Waffles	$8.95	Light Belgian waffles covered with an assortme...	900
3	French Toast	$4.50	Thick slices made from our homemade sourdough ...	600
4	Homestyle Breakfast	$6.95	Two eggs, bacon or sausage, toast, and our eve...	950

Similarly, pandas has a df.to_json() function to write a DataFrame to a JSON file. The parameter orient specifies the orientation of the JSON file. The default is orient='records' which means the JSON file is written in the records orientation. If you want to write the JSON file in the columns orientation, then orient='columns' should be used.

json_data = data.to_json(orient='records', lines=True)
print(json_data)

{"name":"Belgian Waffles","price":"$5.95","description":"Two of our famous Belgian Waffles with plenty of real maple syrup","calories":650}
{"name":"Strawberry Belgian Waffles","price":"$7.95","description":"Light Belgian waffles covered with strawberries and whipped cream","calories":900}
{"name":"Berry-Berry Belgian Waffles","price":"$8.95","description":"Light Belgian waffles covered with an assortment of fresh berries and whipped cream","calories":900}
{"name":"French Toast","price":"$4.50","description":"Thick slices made from our homemade sourdough bread","calories":600}
{"name":"Homestyle Breakfast","price":"$6.95","description":"Two eggs, bacon or sausage, toast, and our ever-popular hash browns","calories":950}

import pprint 

pprint.pprint(data.to_json(orient='columns'))

('{"name":{"0":"Belgian Waffles","1":"Strawberry Belgian '
 'Waffles","2":"Berry-Berry Belgian Waffles","3":"French Toast","4":"Homestyle '
 'Breakfast"},"price":{"0":"$5.95","1":"$7.95","2":"$8.95","3":"$4.50","4":"$6.95"},"description":{"0":"Two '
 'of our famous Belgian Waffles with plenty of real maple syrup","1":"Light '
 'Belgian waffles covered with strawberries and whipped cream","2":"Light '
 'Belgian waffles covered with an assortment of fresh berries and whipped '
 'cream","3":"Thick slices made from our homemade sourdough bread","4":"Two '
 'eggs, bacon or sausage, toast, and our ever-popular hash '
 'browns"},"calories":{"0":650,"1":900,"2":900,"3":600,"4":950}}')




URL: https://fahadsultan.com/csc272/1_programming/43_modalities.html


2.3. Modalities of Data

In this section, we will discuss the most common modalities of data and how to work with them in Python.

There are too many data modalities to eumerate an exhaustive list. Each data modality has its own unique characteristics and often require specialized methods to process and analyze. Study and analysis of many data modalities is an area of research within itself. For example, Computer Vision is the area of research that deals with images and videos. Speech processing is the area of research that deals with sounds. Natural Language Processing (NLP) is the area of research that deals with text.

Some common modalities and their associated areas of research are:

Text: Natural Language Processing, Computational Linguistics

Images: Computer Vision, Digital Image Processing

Sounds: Digital Signal Processing (DSP)

Graphs: Graph Theory, Network Theory

Time Series: Time Series Analysis

Geographic: Geographic Information Systems (GIS), Spatial Computing.

2.3.1. Text

Text is arguably the most ubiquitous non-numeric modality of data. Most of the data on the internet is text. Text data generally exists as a collection of documents called corpus, where each document is one or more sentences.

2.3.1.1. Bag of Words

Text data can be encoded into a number of different numeric representations. The most common is the Bag-of-Words (BoW) representation, which is closely related with One-Hot Encoding.

In the BoW representation, each row represents a document and each column represents a word in the vocabulary. The vocabulary is the list of all unique words in the corpus. The corpus is the collection of all the documents. A document is a sequence of words. The value in the cell at 
𝑖
th row and 
𝑗
th column represents the number of times the word 
𝑗
 appears in document 
𝑖
.

Creating a Bag of Words representation for a corpus generally entails the following steps:

Tokenization: Split each document into a sequence of tokens (words).

Create Vocabulary: Create a list of all unique tokens (words) for all documents in the corpus. Often words are normalized by converting all words to lowercase and removing punctuation.

Create Document Vectors: Create a vector for each document in the corpus. The vector is the same length as the vocabulary. The value in each cell of the vector is the number of times the word in the corresponding column appears in the document.

Create Document-Term Matrix: Create a 2D array where each row represents a document and each column represents a word in the vocabulary. The value in the cell at 
𝑖
th row and 
𝑗
th column represents the number of times the word 
𝑗
 appears in document 
𝑖
.

The image below shows a bag-of-words representation of a corpus of two documents. The vocabulary is the list of words on the left. The corpus is the 2D array of numbers on the right.




Fig. 2.4 Bag of Words Representation

2.3.2. Networks

Networks are a data structure that consists of a set of nodes (vertices) and a set of edges that relate the nodes to each other. The set of edges describes relationships among the vertices. Graphs are used to model many real-world systems, including computer networks, social networks, and transportation systems.

Fig. 2.5 Schematic of an example social network

There are two common ways to represent a graph as a matrix: 1. Adjacency Matrix and 2. Edge List.

The first way is to use an Adjacency matrix, which is a matrix where each row and column represents a vertex. If there is an edge from vertex 
𝑖
 to vertex 
𝑗
, then the entry in row 
𝑖
 and column 
𝑗
 is 1. Otherwise, the entry is 0. For example, the following matrix represents a graph with 4 vertices and 4 edges:

Fig. 2.6 Network above encoded in Adjacency Matrix representation

The second way to represent a graph as a matrix is to use an Edge list. An edge list is a list of pairs of vertices that are connected by an edge. For example, the following edge list represents the same graph as the adjacency matrix above:

Fig. 2.7 Network above encoded in Edge List representation

2.3.3. Images

An image is a 2D array of pixels. Each pixel is a 3D vector of red, green, and blue (RGB) values. The RGB values are usually represented as integers between 0 and 255. The RGB values are used to represent the color of the pixel. For example, a pixel with RGB values of (255, 0, 0) is red, (0, 255, 0) is green, and (0, 0, 255) is blue. A pixel with RGB values of (0, 0, 0) is black and (255, 255, 255) is white.

In this notebook, we will learn how to read and write images, and how to manipulate them.

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd 

# Read an image
img = plt.imread('https://raw.githubusercontent.com/fahadsultan/csc272/main/data/belltower.png');
plt.imshow(img);

/var/folders/l9/y8y3rmys2sl93tzzph3dl7jw0000gr/T/ipykernel_15810/3162821737.py:6: MatplotlibDeprecationWarning: Directly reading images from URLs is deprecated since 3.4 and will no longer be supported two minor releases later. Please open the URL for reading and pass the result to Pillow, e.g. with ``np.array(PIL.Image.open(urllib.request.urlopen(url)))``.
  img = plt.imread('https://raw.githubusercontent.com/fahadsultan/csc272/main/data/belltower.png');

img.shape # (height, width, channels)

(1600, 1267, 3)

img[0,0,:] # RGB values of the first pixel

array([240, 241, 243], dtype=uint8)

# Convert to grayscale
img_gray = img.mean(axis=2)
img_df   = pd.DataFrame(img_gray)

plt.imshow(img_df, cmap='gray');
plt.colorbar();

plt.imshow(256-img_df, cmap='gray');
plt.colorbar();

# Threshold the image
img_thresh = img_gray > 100 
plt.imshow(img_thresh, cmap='gray');

## Crop the image
img_crop = img_df.iloc[:700, :700]
plt.imshow(img_crop, cmap='gray');

# Rotate the image
img_rot = img_df.transpose()
plt.imshow(img_rot, cmap='gray');

2.3.3.1. Videos

Videos are a sequence of images. Within the context of videos, each image is called a frame. Most videos are a sequence of 24-30 frames per second.

Most modern videos are encoded using a variety of different codecs. A codec is a method of encoding and decoding a video. Some common codecs are H.264, MPEG-4, and VP9.

2.3.4. Audio

In this section, we will learn how to use representations of audio data in machine learning.

Audio files can be represented in a variety of ways. The most common is the waveform, which is a time series of the amplitude of the sound wave at each time point. The waveform is a one-dimensional array of numbers. The sampling rate is the number of samples per second.

To load an audio file, we can use the librosa library. The librosa.load function returns the waveform and the sampling rate.

Note

You may have to install the librosa library using !pip install librosa in a new code cell for the code below to work.

The audio file can be downloaded from this link.

import librosa
y, sr = librosa.load('../assets/StarWars3.wav')
plt.plot(y);
plt.xlabel('Time (samples)');
plt.ylabel('Amplitude');
plt.title('Star Wars Theme\nSampling rate: %s Hz\nLength: %s seconds' % (sr, len(y)/sr));

sr, len(y)

(22050, 66150)

S = librosa.stft(y)
S.shape

(1025, 130)


Power Spectral Density (PSD) is a measure of the power of a signal at different frequencies. The PSD is calculated using the Fourier Transform. The PSD is a useful representation of audio data because it is often easier to distinguish different sounds in the frequency domain than in the time domain.

fig, ax = plt.subplots()
img = librosa.display.specshow(librosa.amplitude_to_db(np.abs(S), ref=np.max),
    y_axis='log', x_axis='time', ax=ax);
ax.set_title('Power spectrogram');
fig.colorbar(img, ax=ax, format="%+2.0f dB");

2.3.5. Geographic

Geographic data is data that is associated with a location on the Earth. Geographic data is often represented as a latitude and longitude. The latitude is the distance north or south of the equator. The longitude is the distance east or west of the prime meridian .

url  = 'https://raw.githubusercontent.com/fahadsultan/csc343/main/data/uscities.csv'
data = pd.read_csv(, index_col='city')

plt.scatter(data['lng'], data['lat'], c=data['county_fips'], s=1);
plt.xlim(-130, -65);
plt.ylim(20, 55);
plt.xlabel('Longitude');
plt.ylabel('Latitude');

2.3.6. Time Series

Time series data is data that is collected over time. Time series data is often represented as a sequence of numbers. The numbers are usually collected at regular intervals. For example, the stock price of a company is collected every day at the close of the stock market.

url = "https://raw.githubusercontent.com/fahadsultan/csc272/main/data/elections.csv"
data = pd.read_csv(url)
wins = data[data['Result']=='win'].sort_values('Year')
losses = data[data['Result']=='loss'].sort_values('Year')
plt.plot(wins['Year'], wins['Popular vote']);
plt.plot(losses['Year'], losses['Popular vote']);
plt.legend(['Wins', 'Losses']);
plt.title('Popular vote in US presidential elections');




URL: https://fahadsultan.com/csc272/1_programming/30_collecting.html


3. Collecting Data



URL: https://fahadsultan.com/csc272/1_programming/31_apis.html


3.1. APIs

API stand for Application Programming Interface. APIs are a way for computers to talk to each other. They are a set of rules that allow one piece of software application to talk to another. APIs are used to share data between applications.

Many companies provide APIs to allow developers to access their services. For example, Twitter provides an API that allows programmers to access their service and get tweets and other information. Google provides APIs for many of their services, including Google Maps, Google Drive, and Google Calendar.

APIs are used to connect to other applications. For example, you can use the Twitter API to read and write tweets from your own application. You can use the Google Maps API to get directions from one place to another. You can use the Google Translate API to translate text from one language to another.

Most APIs require you to register for an API key. This is a unique identifier that allows the API to track your usage. You can get an API key by registering for an account with the service provider. For example, if you want to use the Twitter API, you need to register for a Twitter account and then request an API key.

Most APIs are free to use, but some require you to pay a fee. For example, the Google Maps API is free for up to 25,000 requests per day, but you have to pay if you want to make more than that. The Google Translate API is free for up to 500,000 characters per month, but you have to pay if you want to translate more than that.

Most APIs return data in the following formats: JSON, XML, or CSV. JSON is the most common format, but some APIs return data in XML or CSV format. You can use the requests library to make HTTP requests to APIs and get data in JSON format.

3.1.1. JSON

JSON stands for JavaScript Object Notation. It is a lightweight data-interchange format. It is easy for humans to read and write. It is easy for machines to parse and generate. It is based on a subset of the JavaScript Programming Language, Standard ECMA-262 3rd Edition - December 1999. JSON is a text format that is completely language independent but uses conventions that are familiar to programmers of the C-family of languages, including C, C++, C#, Java, JavaScript, Perl, Python, and many others. These properties make JSON an ideal data-interchange language.

JSON is built on two structures:

A collection of name/value pairs. In various languages, this is realized as an object, record, struct, dictionary, hash table, keyed list, or associative array.

An ordered list of values. In most languages, this is realized as an array, vector, list, or sequence.

These are universal data structures. Virtually all modern programming languages support them in one form or another. It makes sense that a data format that is interchangeable with programming languages also be based on these structures.

In JSON, they take on these forms:

An object is an unordered set of name/value pairs. An object begins with { (left brace) and ends with } (right brace). Each name is followed by : (colon) and the name/value pairs are separated by , (comma).

An array is an ordered collection of values. An array begins with [ (left bracket) and ends with ] (right bracket). Values are separated by , (comma).

A value can be a string in double quotes, or a number, or true or false or null, or an object or an array. These structures can be nested.

A string is a sequence of zero or more Unicode characters, wrapped in double quotes, using backslash escapes. A character is represented as a single character string. A string is very much like a C or Java string.

A number is very much like a C or Java number, except that the octal and hexadecimal formats are not used.

White space can be inserted between any pair of tokens. Excepting a few encoding details, that completely describes the language.

Here is a small example:

{
    "firstName": "John",
    "lastName": "Smith",
    "age": 25,
    "address": {
        "streetAddress": "21 2nd Street",
        "city": "New York",
        "state": "NY",
        "postalCode": "10021"
    },
    "phoneNumber": [
        {
            "type": "home",
            "number": "212 555-1234"
        },
        {
            "type": "fax",
            "number": "646 555-4567"
        }
    ]
}


An example of a public API that returns data in JSON format is the Open Notify API. This API returns the current location of the International Space Station in JSON format.

{
  "iss_position": {
    "latitude": "-50.7353",
    "longitude": "-179.9974"
  },
  "message": "success",
  "timestamp": 1619455654
}

'''Example code that makes an API call to Open Notify API'''

import requests

# Make an API call and store the response.

url = 'http://api.open-notify.org/astros.json'

response = requests.get(url)

# Print the status code of the response.

print(f"Status code: {response.status_code}")

# Store API response in a variable.

response_dict = response.json()

# Print the number of people currently in space.

print(f"People in space: {response_dict['number']}")

# Print the names of people currently in space.

for person in response_dict['people']:

    print(person['name'])

# Print the raw response.

print(response.text)

# Print the raw response as a list.

print(response.json())

# Print the raw response as a dictionary.

print(response.json()['people'])

# Print the raw response as a dictionary.

print(response.json()['people'][0])

# Print the raw response as a dictionary.

print(response.json()['people'][0]['name'])

Status code: 200
People in space: 10
Sergey Prokopyev
Dmitry Petelin
Frank Rubio
Stephen Bowen
Warren Hoburg
Sultan Alneyadi
Andrey Fedyaev
Jing Haiping
Gui Haichow
Zhu Yangzhu
{"number": 10, "people": [{"name": "Sergey Prokopyev", "craft": "ISS"}, {"name": "Dmitry Petelin", "craft": "ISS"}, {"name": "Frank Rubio", "craft": "ISS"}, {"name": "Stephen Bowen", "craft": "ISS"}, {"name": "Warren Hoburg", "craft": "ISS"}, {"name": "Sultan Alneyadi", "craft": "ISS"}, {"name": "Andrey Fedyaev", "craft": "ISS"}, {"name": "Jing Haiping", "craft": "Tiangong"}, {"name": "Gui Haichow", "craft": "Tiangong"}, {"name": "Zhu Yangzhu", "craft": "Tiangong"}], "message": "success"}
{'number': 10, 'people': [{'name': 'Sergey Prokopyev', 'craft': 'ISS'}, {'name': 'Dmitry Petelin', 'craft': 'ISS'}, {'name': 'Frank Rubio', 'craft': 'ISS'}, {'name': 'Stephen Bowen', 'craft': 'ISS'}, {'name': 'Warren Hoburg', 'craft': 'ISS'}, {'name': 'Sultan Alneyadi', 'craft': 'ISS'}, {'name': 'Andrey Fedyaev', 'craft': 'ISS'}, {'name': 'Jing Haiping', 'craft': 'Tiangong'}, {'name': 'Gui Haichow', 'craft': 'Tiangong'}, {'name': 'Zhu Yangzhu', 'craft': 'Tiangong'}], 'message': 'success'}
[{'name': 'Sergey Prokopyev', 'craft': 'ISS'}, {'name': 'Dmitry Petelin', 'craft': 'ISS'}, {'name': 'Frank Rubio', 'craft': 'ISS'}, {'name': 'Stephen Bowen', 'craft': 'ISS'}, {'name': 'Warren Hoburg', 'craft': 'ISS'}, {'name': 'Sultan Alneyadi', 'craft': 'ISS'}, {'name': 'Andrey Fedyaev', 'craft': 'ISS'}, {'name': 'Jing Haiping', 'craft': 'Tiangong'}, {'name': 'Gui Haichow', 'craft': 'Tiangong'}, {'name': 'Zhu Yangzhu', 'craft': 'Tiangong'}]
{'name': 'Sergey Prokopyev', 'craft': 'ISS'}
Sergey Prokopyev


Some popular APIs that return data in JSON format are:

Open Notify API

Open Weather Map API

Google Maps API

Twitter API

GitHub API

Reddit API

Spotify API

YouTube API

Facebook API

Instagram API

3.1.2. XML

XML stands for Extensible Markup Language. It is a markup language that defines a set of rules for encoding documents in a format that is both human-readable and machine-readable. The design goals of XML emphasize simplicity, generality, and usability across the Internet. It is a textual data format with strong support via Unicode for different human languages. Although the design of XML focuses on documents, the language is widely used for the representation of arbitrary data structures such as those used in web services.

Several schema systems exist to aid in the definition of XML-based languages, while programmers have developed many application programming interfaces (APIs) to aid the processing of XML data.

XML data is made up of storage units called entities, which contain either parsed or unparsed data. Parsed data is made up of characters, some of which form character data, and some of which form markup. Markup encodes a description of the document’s storage layout and logical structure. XML provides a mechanism to impose constraints on the storage layout and logical structure.

An example of XML data is shown below:

<?xml version="1.0" encoding="UTF-8"?>

<breakfast_menu>
    <food>
        <name>Belgian Waffles</name>
        <price>$5.95</price>
        <description>Two of our famous Belgian Waffles with plenty of real maple syrup</description>
        <calories>650</calories>
    </food>
    <food>
        <name>Strawberry Belgian Waffles</name>
        <price>$7.95</price>
        <description>Light Belgian waffles covered with strawberries and whipped cream</description>
        <calories>900</calories>
    </food>
    <food>
        <name>Berry-Berry Belgian Waffles</name>
        <price>$8.95</price>
        <description>Light Belgian waffles covered with an assortment of fresh berries and whipped cream</description>
        <calories>900</calories>
    </food>
    <food>
        <name>French Toast</name>
        <price>$4.50</price>
        <description>Thick slices made from our homemade sourdough bread</description>
        <calories>600</calories>
    </food>
    <food>
        <name>Homestyle Breakfast</name>
        <price>$6.95</price>
        <description>Two eggs, bacon or sausage, toast, and our ever-popular hash browns</description>
        <calories>950</calories>
    </food>

</breakfast_menu>

'''Example of an API that returns XML '''

import requests

url = 'http://api.open-notify.org/astros.xml'

response = requests.get(url)

print(response.text)






URL: https://fahadsultan.com/csc272/1_programming/32_scraping.html


3.2. Web Scraping

Web scraping is a technique to automatically access and extract large amounts of information from a website, which can save a huge amount of time and effort. In this article, we will go through an easy example of how to automate downloading hundreds of files from the New York MTA. This is a great exercise for web scraping beginners who are looking to understand how to web scrape. We’ll be extracting information from this web page: http://web.mta.info/developers/turnstile.html

# Import libraries
import urllib.request
from bs4 import BeautifulSoup

# Specify url
urlpage =  'http://web.mta.info/developers/turnstile.html'

# Query the website and return the html to the variable 'page'
page = urllib.request.urlopen(urlpage)

# Parse the html using beautiful soup and store in variable 'soup'
soup = BeautifulSoup(page, 'html.parser')

# Print out the type of the variable 'soup'
print(type(soup))

# Find all the links on the page
soup.findAll('a', href=True)

# Let's extract all the text from the page
print(soup.text)

# Find all the links on the page
soup.findAll('a', href=True)

# Let's extract all the text from the page
print(soup.text)

<class 'bs4.BeautifulSoup'>
























MTA Subway Hourly Ridership: Beginning February 2022 | State of New York










        Skip to Main Content
      





DATA.NY.GOV



Sign In




EnglishEnglishEspañolItalianoFrançais中文Русский












Search


          Search
      








 
 
 
 




OPEN NYCATALOGDEVELOPERSHELPVIDEO HELPSUPPORTED BROWSERSCATALOG NAVIGATIONABOUTPRESS RELEASESEXECUTIVE ORDEROPEN DATA PROGRAM OVERVIEWOPEN DATA HANDBOOKDATASET SUBMISSION GUIDEREPORTS




            Menu
          




Menu

Close


OPEN NYCATALOGDEVELOPERSHELPVIDEO HELPSUPPORTED BROWSERSCATALOG NAVIGATIONABOUTPRESS RELEASESEXECUTIVE ORDEROPEN DATA PROGRAM OVERVIEWOPEN DATA HANDBOOKDATASET SUBMISSION GUIDEREPORTS

Sign In






Search










 
 
 
 




    Language
    


English
Español
Italiano
Français
中文
Русский

























 




 
 


 
 


















































MTA Subway Hourly Ridership: Beginning February 2022 | State of New York










        Skip to Main Content
      





DATA.NY.GOV



Sign In




EnglishEnglishEspañolItalianoFrançais中文Русский












Search


          Search
      








 
 
 
 




OPEN NYCATALOGDEVELOPERSHELPVIDEO HELPSUPPORTED BROWSERSCATALOG NAVIGATIONABOUTPRESS RELEASESEXECUTIVE ORDEROPEN DATA PROGRAM OVERVIEWOPEN DATA HANDBOOKDATASET SUBMISSION GUIDEREPORTS




            Menu
          




Menu

Close


OPEN NYCATALOGDEVELOPERSHELPVIDEO HELPSUPPORTED BROWSERSCATALOG NAVIGATIONABOUTPRESS RELEASESEXECUTIVE ORDEROPEN DATA PROGRAM OVERVIEWOPEN DATA HANDBOOKDATASET SUBMISSION GUIDEREPORTS

Sign In






Search










 
 
 
 




    Language
    


English
Español
Italiano
Français
中文
Русский

























 




 
 


 
 




URL: https://fahadsultan.com/csc272/1_programming/20_visualization.html


4. Visualizing Data

Visualizing data is a key part of data science. It is not only a way to communicate your findings to others but, more importantly, it is a way to understand your data, models and algorithms better.

In this section, we will learn how to use the libraries matplotlib and seaborn to create visualizations.

matplotlib is the primary plotting library in Python. It is a very powerful and highly customizable library that can be used to create a wide variety of plots and graphs. However, despite its power, is can often be not very user-friendly, requiring a lot of code to create even simple plots.

seaborn is a data visualization library built on top of matplotlib that is easier to use and creates more visually appealing plots. I will try to use seaborn whenever possible, but may have to fall back occasionally to matplotlib for formatting details and customization.

Just as pandas is conventionally imported as pd, matplotlib.pyplot is conventionally imported as plt and seaborn is conventionally imported as sns.

Let’s start by importing the libraries we will need.

import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns


There has long been an impression amongst academics and practitioners that “numerical calculations are exact, but graphs are rough”. In 1973, Francis Anscombe set out to counter this common misconception by creating a set of four datasets that are today known as Anscombe’s quartet.

The code cell below downloads and loads it as pandas DataFrame this data set:

anscombe = sns.load_dataset("anscombe")
anscombe.head()

	dataset	x	y
0	I	10.0	8.04
1	I	8.0	6.95
2	I	13.0	7.58
3	I	9.0	8.81
4	I	11.0	8.33

Now let’s see what the summary statistics of x and y features look like, with respect to dataset feature:

anscombe.groupby("dataset").describe()

	x	y
	count	mean	std	min	25%	50%	75%	max	count	mean	std	min	25%	50%	75%	max
dataset																
I	11.0	9.0	3.316625	4.0	6.5	9.0	11.5	14.0	11.0	7.500909	2.031568	4.26	6.315	7.58	8.57	10.84
II	11.0	9.0	3.316625	4.0	6.5	9.0	11.5	14.0	11.0	7.500909	2.031657	3.10	6.695	8.14	8.95	9.26
III	11.0	9.0	3.316625	4.0	6.5	9.0	11.5	14.0	11.0	7.500000	2.030424	5.39	6.250	7.11	7.98	12.74
IV	11.0	9.0	3.316625	8.0	8.0	8.0	8.0	19.0	11.0	7.500909	2.030579	5.25	6.170	7.04	8.19	12.50

Note that for all four unique values of dataset, we have eleven (x, y) values, as seen in count.

For each value of dataset, x and y have nearly identical simple descriptive statistics.

Now let’s create a scatter plot of the data using seaborn:

g = sns.FacetGrid(anscombe, col="dataset");
g.map(sns.scatterplot, "x", "y", s=100, color="orange", linewidth=.5, edgecolor="black");


Anscombe’s quartet demonstrates both the importance of graphing data before analyzing it and the effect of outliers and other influential observations on statistical properties.

Data Visualization is arguably the most mistake-prone part of the data science process. It is very easy to create misleading visualizations that lead to incorrect conclusions. It is therefore important to be aware of the common pitfalls and to avoid them.

The following is a useful taxonomy for choosing the right visualization depending on your goals for your data:



URL: https://fahadsultan.com/csc272/1_programming/21_univariate.html


4.1. Univariate Visualizations

Uni-variate (single variable) visualizations are used to better understand one-dimensional data (think pandas Series) in the dataset.

In this section, we will look at techniques to understand the distribution of each feature. Distribution of a feature, simply put, is the frequency of each value of the variable.

We know from previous discussion that .value_counts() method on a pandas Series gives us the frequency distribution of each value in the Series.

We will use the World Bank dataset, which contains information about countries and social statistics.

import pandas as pd 
import seaborn as sns 
from matplotlib import pyplot as plt

sns.set_style('whitegrid')

data = pd.read_csv('https://raw.githubusercontent.com/fahadsultan/csc272ta/world_bank.csv', index_col=0)
data.head()

	Continent	Country	Primary completion rate: Male: % of relevant age group: 2015	Primary completion rate: Female: % of relevant age group: 2015	Lower secondary completion rate: Male: % of relevant age group: 2015	Lower secondary completion rate: Female: % of relevant age group: 2015	Youth literacy rate: Male: % of ages 15-24: 2005-14	Youth literacy rate: Female: % of ages 15-24: 2005-14	Adult literacy rate: Male: % ages 15 and older: 2005-14	Adult literacy rate: Female: % ages 15 and older: 2005-14	...	Access to improved sanitation facilities: % of population: 1990	Access to improved sanitation facilities: % of population: 2015	Child immunization rate: Measles: % of children ages 12-23 months: 2015	Child immunization rate: DTP3: % of children ages 12-23 months: 2015	Children with acute respiratory infection taken to health provider: % of children under age 5 with ARI: 2009-2016	Children with diarrhea who received oral rehydration and continuous feeding: % of children under age 5 with diarrhea: 2009-2016	Children sleeping under treated bed nets: % of children under age 5: 2009-2016	Children with fever receiving antimalarial drugs: % of children under age 5 with fever: 2009-2016	Tuberculosis: Treatment success rate: % of new cases: 2014	Tuberculosis: Cases detection rate: % of new estimated cases: 2015
0	Africa	Algeria	106.0	105.0	68.0	85.0	96.0	92.0	83.0	68.0	...	80.0	88.0	95.0	95.0	66.0	42.0	NaN	NaN	88.0	80.0
1	Africa	Angola	NaN	NaN	NaN	NaN	79.0	67.0	82.0	60.0	...	22.0	52.0	55.0	64.0	NaN	NaN	25.9	28.3	34.0	64.0
2	Africa	Benin	83.0	73.0	50.0	37.0	55.0	31.0	41.0	18.0	...	7.0	20.0	75.0	79.0	23.0	33.0	72.7	25.9	89.0	61.0
3	Africa	Botswana	98.0	101.0	86.0	87.0	96.0	99.0	87.0	89.0	...	39.0	63.0	97.0	95.0	NaN	NaN	NaN	NaN	77.0	62.0
5	Africa	Burundi	58.0	66.0	35.0	30.0	90.0	88.0	89.0	85.0	...	42.0	48.0	93.0	94.0	55.0	43.0	53.8	25.4	91.0	51.0

5 rows × 47 columns

4.1.1. Bar Plots

Bar plots are used to visualize the frequency distribution of features that take on a small set of unique values. Such features are more often categorical features as opposed to numerical features.

Visualizing categorical features is easy. We can use .value_counts() method to get the frequency distribution of each value in the feature. This frequency distribution can be plotted using a bar chart.

val_counts = data['Continent'].value_counts()
print(val_counts)

Africa        47
Europe        43
Asia          34
N. America    18
Oceania       13
S. America    11
Name: Continent, dtype: int64


In seaborn the method to make a bar chart is barplot(). The method takes in two arguments: x and y.

sns.barplot(x=val_counts.index, y=val_counts.values);


Note that the plots above are incomplete. If you show this plot to someone, they will not be able to understand what the plot is about.

To complete the plot, so it is understandable, we need to add atleast a title, axis labels and, depending on context, a legend.

4.1.2. Axis labels and Title

To add axis labels and title, we need to assign the plot object returned by sns.barplot to a variable and then use .set() method on the plot object.

ax = sns.barplot(x=val_counts.index, y=val_counts.values);

ax.set(xlabel='Continent', ylabel='Count of countries (rows)', title='Count of countries in each continent');


Note that we have used .set() method on the plot object, returned by sns.barplot() method, and passed the arguments xlabel, ylabel and title to the method.

Now, the plot is “complete” and can be understood by someone who has not seen the data.

Bar plots don’t scale. If you try to plot a feature (categorical or numerical) with a lot of unique values, the plot will be unreadable. Yes, even with a title and axis labels!

Here is an example:

val_counts = data['Country'].value_counts();

ax = sns.barplot(x=val_counts.index, y=val_counts.values);

ax.set(xlabel='Country', ylabel='Count', title='Frequency of countries in the World Bank data');


The plot above is unreadable. We can’t make out the labels on the x-axis. This is because there are too many unique values in the country feature.

There is no easy way to fix this for categorical features. For numerical features, however, there is a way.

4.1.3. Histograms

Histograms look very similar to bar plots. However, they are different in a subtle but critical way.

Histograms solve the problem of plotting a feature with a lot of unique values by binning the values.

Binning is the process of dividing the range of values of a feature into bins. Each bin represents a range of values. The height of the bar in a histogram represents the number of values in the bin. In most cases, the bins are of equal width.

For example, if we have a feature with values ranging from 0 to 100, we can divide the range into 10 bins of width 10 each. The first bin will contain values from 0 to 10, the second bin will contain values from 10 to 20 and so on.

col = "Gross domestic product: % growth : 2016"

ax = sns.histplot(data=data, x=col);

ax.set(title="Distribution of GDP growth in 2016");


By default, sns.histplot will try to infer the bin edges from the data. However, it is possible to set the bin edges explicitly. This can be useful when comparing multiple distributions.

4.1.4. Box Plots

Box plots display distributions using information about quartiles.

A quartile represents a 25% portion of the data. We say that:

The first quartile (Q1) repesents the 25th percentile – 25% of the data lies below the first quartile

The second quartile (Q2) represents the 50th percentile, also known as the median – 50% of the data lies below the second quartile

The third quartile (Q3) represents the 75th percentile – 75% of the data lies below the third quartile.

In a box plot, the lower extent of the box lies at Q1, while the upper extent of the box lies at Q3. The horizontal line in the middle of the box corresponds to Q2 (equivalently, the median).

The Inter-Quartile Range (IQR) measures the spread of the middle % of the distribution, calculated as the (
3
𝑟
𝑑
 Quartile 
−
 
1
𝑠
𝑡
 Quartile).

The whiskers of a box-plot are the two points that lie at the [
1
𝑠
𝑡
 Quartile 
−
(
1.5
×
 IQR)], and the [
3
𝑟
𝑑
 Quartile 
+
 (
1.5
×
 IQR)]. They are the lower and upper ranges of “normal” data (the points excluding outliers). Subsequently, the outliers are the data points that fall beyond the whiskers, or further than ( 
1.5
×
 IQR) from the extreme quartiles.

col = "Gross domestic product: % growth : 2016"

print(data[col].describe())
ax = sns.boxplot(data=data, y=col, width=0.5);

count    159.000000
mean       2.780503
std        3.106862
min      -10.400000
25%        1.450000
50%        2.900000
75%        4.500000
max       11.000000
Name: Gross domestic product: % growth : 2016, dtype: float64


If it helps, you can think of the box plot as a birds-eye-view of histogram. Histogram as seeing a hill from the side, while box plot is seeing the hill from above through a drone or through a bird’s eye.

Also note that boxplot is to .describe() as barplot is to .value_counts().



URL: https://fahadsultan.com/csc272/1_programming/22_multivariate.html


4.2. Multivariate Visualizations

Up until now, we’ve discussed how to visualize single-feature distributions. Now, let’s understand how to visualize the relationship between more than one features.

We will continue to use the World Bank dataset, which contains information from 2015/16 about countries around the world. We will use the same features as before: GDP per capita, life expectancy, and population.

import pandas as pd 
import seaborn as sns 
from matplotlib import pyplot as plt

sns.set_style('whitegrid')

data = pd.read_csv('https://raw.githubusercontent.com/fahadsultan/csc272/main/data/world_bank.csv', index_col=0)
data.head()

	Continent	Country	Primary completion rate: Male: % of relevant age group: 2015	Primary completion rate: Female: % of relevant age group: 2015	Lower secondary completion rate: Male: % of relevant age group: 2015	Lower secondary completion rate: Female: % of relevant age group: 2015	Youth literacy rate: Male: % of ages 15-24: 2005-14	Youth literacy rate: Female: % of ages 15-24: 2005-14	Adult literacy rate: Male: % ages 15 and older: 2005-14	Adult literacy rate: Female: % ages 15 and older: 2005-14	...	Access to improved sanitation facilities: % of population: 1990	Access to improved sanitation facilities: % of population: 2015	Child immunization rate: Measles: % of children ages 12-23 months: 2015	Child immunization rate: DTP3: % of children ages 12-23 months: 2015	Children with acute respiratory infection taken to health provider: % of children under age 5 with ARI: 2009-2016	Children with diarrhea who received oral rehydration and continuous feeding: % of children under age 5 with diarrhea: 2009-2016	Children sleeping under treated bed nets: % of children under age 5: 2009-2016	Children with fever receiving antimalarial drugs: % of children under age 5 with fever: 2009-2016	Tuberculosis: Treatment success rate: % of new cases: 2014	Tuberculosis: Cases detection rate: % of new estimated cases: 2015
0	Africa	Algeria	106.0	105.0	68.0	85.0	96.0	92.0	83.0	68.0	...	80.0	88.0	95.0	95.0	66.0	42.0	NaN	NaN	88.0	80.0
1	Africa	Angola	NaN	NaN	NaN	NaN	79.0	67.0	82.0	60.0	...	22.0	52.0	55.0	64.0	NaN	NaN	25.9	28.3	34.0	64.0
2	Africa	Benin	83.0	73.0	50.0	37.0	55.0	31.0	41.0	18.0	...	7.0	20.0	75.0	79.0	23.0	33.0	72.7	25.9	89.0	61.0
3	Africa	Botswana	98.0	101.0	86.0	87.0	96.0	99.0	87.0	89.0	...	39.0	63.0	97.0	95.0	NaN	NaN	NaN	NaN	77.0	62.0
5	Africa	Burundi	58.0	66.0	35.0	30.0	90.0	88.0	89.0	85.0	...	42.0	48.0	93.0	94.0	55.0	43.0	53.8	25.4	91.0	51.0

5 rows × 47 columns

4.2.1. Distribution of a numeric feature w.r.t a categorical features

Let’s start by visualizing the distribution of a numeric feature, across the categories defined by a categorical feature. In other words, we want to visualize the distribution of a numeric feature, separately for each category of another categorical feature.

4.2.1.1. Overlaid Histograms (1 numeric, 1 categorical)

We can use a histogram to visualize the distribution of a numeric variable. To visualize how this distribution differs between the groups created by another categorical variable, we can create a histogram for each group separately.

In order to create overlaid histograms, we will continue to use sns.histplot. The only addition we need to make is to use the hue argument to specify the categorical feature that defines the groups.

americas = data[data['Continent'].apply(lambda x: "America" in x)]

col = "Gross domestic product: % growth : 2016"

ax = sns.histplot(data = americas, x = col, hue="Continent",  multiple="stack");

ax.set(title="GDP of North American vs. South American countries");


Note the use of the hue argument to histplot. It adds a new dimension to the plot, by coloring the bars according to the value of the categorical feature.

multiple="stack" is an optional argument and is used to improve the visibility when bars are stacked on top of each other.

These visualizations are arguably the most ubiquitous in science. The canonical version of overlaid histograms are where a new drug is tested against a placebo, and the distribution of some outcome (e.g. blood pressure) is plotted for the placebo group and the drug group.

Most common statistical tests are designed to answer the question: “Do these two groups differ?” This question is answered by comparing the distributions of the two groups.

4.2.1.2. Side-by-side box plots
col = "Gross domestic product: % growth : 2016"

ax = sns.boxplot(data = data, y = col, x="Continent", width=0.9);

ax.set(title="GDP distribution of countries by continent 2016");

4.2.2. Visualizing Relationships

In addition to visualizing the distribution of features, we often want to understand how two features are related.

4.2.2.1. Scatter Plots (2 or more numeric features)

Scatter plots are one of the most useful tools in representing the relationship between two numerical features. They are particularly important in gauging the strength (correlation) of the relationship between features. Knowledge of these relationships can then motivate decisions in our modeling process.

In Matplotlib, we use the function plt.scatter to generate a scatter plot. Notice that unlike our examples of plotting single-variable distributions, now we specify sequences of values to be plotted along the x axis and the y axis.

wb = pd.read_csv('https://raw.githubusercontent.com/fahadsultan/csc272/main/data/world_bank.csv', index_col=0)

ax = sns.scatterplot(data = wb, \
                     x ='Adult literacy rate: Female: % ages 15 and older: 2005-14', \
                     y = "per capita: % growth: 2016")

ax.set(title="Female adult literacy against % growth");


In Seaborn, we call the function sns.scatterplot. We use the x and y parameters to indicate the values to be plotted along the x and y axes, respectively. By using the hue parameter, we can specify a third variable to be used for coloring each scatter point.

sns.scatterplot(data = wb, \
                y = "per capita: % growth: 2016", \
                x = "Adult literacy rate: Female: % ages 15 and older: 2005-14", 
                hue = "Continent")

plt.title("Female adult literacy against % growth");

ax = sns.scatterplot(data = wb, \
                    y = "per capita: % growth: 2016", \
                    x = "Adult literacy rate: Female: % ages 15 and older: 2005-14", 
                    hue = "Continent", \
                    size="Population: millions: 2016")

ax.figure.set_size_inches(8, 6);

ax.set(title="Female adult literacy against % growth");

4.2.2.2. Joint Plots (2 or more numeric features)

sns.jointplot creates a visualization with three components: a scatter plot, a histogram of the distribution of x values, and a histogram of the distribution of y values.

A joint plot visualizes both: relationship and distributions.

sns.jointplot(data = wb, 
              x = "per capita: % growth: 2016", \
              y = "Adult literacy rate: Female: % ages 15 and older: 2005-14")

# plt.suptitle allows us to shift the title up so it does not overlap with the histogram
plt.suptitle("Female adult literacy against % growth")
plt.subplots_adjust(top=0.9);

4.2.2.3. Hex plots

Hex plots can be thought of as a two dimensional histograms that shows the joint distribution between two variables. This is particularly useful working with very dense data. In a hex plot, the x-y plane is binned into hexagons. Hexagons that are darker in color indicate a greater density of data – that is, there are more datapoints that lie in the region enclosed by the hexagon.

We can generate a hex plot using sns.jointplot modified with the kind parameter.

sns.jointplot(data = wb, \
              x = "per capita: % growth: 2016", \
              y = "Adult literacy rate: Female: % ages 15 and older: 2005-14", \
              kind = "hex")

# plt.suptitle allows us to shift the title up so it does not overlap with the histogram
plt.suptitle("Female adult literacy against % growth")
plt.subplots_adjust(top=0.9);

4.2.3. Temporal Data: Line Plot

If you are trying to visualize the relationship between two numeric variables, and one of those variables is time, then you should use a line plot.

Line plots are useful for visualizing the relationship between two numeric variables when one of them is time.

In seaborn, we can create a line plot using the function sns.lineplot. We use the x and y parameters to specify the variable to be plotted along the x and y axes, respectively.

data = pd.read_csv('https://raw.githubusercontent.com/fahadsultan/csc272/main/data/elections.csv')

sns.lineplot(data = data, x = "Year", y = "Popular vote", hue='Result', marker='o');


Note that seaborn automatically aggregates the data by taking the mean of each numeric variable at each time point. The shaded region around the line represents the 95% confidence interval for the mean. We’ll talk more about confidence intervals in a later lecture.

4.2.4. Multi-panel Visualizations

To create a multi-panel visualization, we can use the sns.FacetGrid.

This class takes in a dataframe, the names of the variables that will form the row, column, or hue dimensions of the grid, and the plot type to be produced for each subset of the data. The plot type is provided as a method of the FacetGrid object.

import pandas as pd 
import seaborn as sns 

tips = sns.load_dataset("tips")

g = sns.FacetGrid(tips, col="time",  row="sex");
g.map(sns.scatterplot, "total_bill", "tip");


The variable specification in FacetGrid.map() requires a positional argument mapping, but if the function has a data parameter and accepts named variable assignments, you can also use FacetGrid.map_dataframe():

g = sns.FacetGrid(tips, col="time",  row="sex");
g.map_dataframe(sns.histplot, x="total_bill");


The FacetGrid constructor accepts a hue parameter. Setting this will condition the data on another variable and make multiple plots in different colors. Where possible, label information is tracked so that a single legend can be drawn:

g = sns.FacetGrid(tips, col="time", hue="sex");
g.map_dataframe(sns.scatterplot, x="total_bill", y="tip");
g.add_legend();


The FacetGrid object has some other useful parameters and methods for tweaking the plot:

g = sns.FacetGrid(tips, col="sex", row="time", margin_titles=True)
g.map_dataframe(sns.scatterplot, x="total_bill", y="tip")
g.set_axis_labels("Total bill ($)", "Tip ($)")
g.set_titles(col_template="{col_name} patrons", row_template="{row_name}")
g.set(xlim=(0, 60), ylim=(0, 12), xticks=[10, 30, 50], yticks=[2, 6, 10])
g.tight_layout()
g.savefig("facet_plot.png")




URL: https://fahadsultan.com/csc272/1_programming/23_dosdonts.html


4.3. Dos and Donts of Visualization

Distinguishing good vs. bad visualizations requires a design aesthetic, and a vocabulary to talk about data representations.

Here, we will discuss some recommendations for creating effective visualizations, and some common pitfalls to avoid.

4.3.1. Dos ✅

The following recommendations are Edward Tufte’s Visualization Aesthetic

4.3.1.1. Maximize data ink-ratio

The data ink ratio is the proportion of the plotting area dedicated to displaying the data. It is defined as:

Data-Ink Ratio
=
Data Ink
Total ink used in graph

The goal is to maximize the data ink ratio. In other words, we want to maximize the proportion of the plotting area dedicated to displaying the data and minimize the proportion dedicated to non-data ink.

In the example above, note that both panels show the same data. However, the panel on the right has a higher data ink ratio because it removes the background, the grid lines, and the border around the plotting area.


Similar to the first example, the data shown in both panels of the figure above is the same. However, the panel on the left has a low data ink ratio because it includes needless icons, grid lines and fancy fonts. The panel on the right cleans up the visualization by minimizing all the non-data ink.
4.3.1.2. Use 1:1 Aspect Ratio

The aspect ratio of the visualization should be 1:1. In other words, the width of the visualization should be equal to the height of the visualization. This ensures that the data is not distorted.

This is particularly true for line graphs. If the aspect ratio is not 1:1, then the slope of the line will be distorted. The steepness of apparent trends in a line plot is a function of aspect ratio. Aim for 45° lines or Golden ratio (
≈
1.618
) as most interpretable.

4.3.1.3. Bar graphs start at 0

Bar graphs should always start at zero. If the bar graph does not start at zero, then the differences between the bars will be distorted.

4.3.1.4. Use continuous scales

The scale of the axes should be continuous. In other words, the scale should be linear or logarithmic. Do not use a scale that is discontinuous or non-linear.

4.3.1.5. Use the right colors

If the data is categorical, then use qualitative colormaps. If your data ranges from negative to positive values use divergent colormaps. If your data ranges from low to high values, then use sequential colormaps.

4.3.1.6. Order your numerical axes

The numerical axes should be ordered. For example, if the x-axis represents time, then the x-axis should be ordered from earliest to latest. If the x-axis represents age, then the x-axis should be ordered from youngest to oldest.

Othwerwise, the data will be difficult to interpret as the viewer will not know which data point corresponds to which value on the x-axis. Example of an unordered x-axis:

4.3.2. Donts ❌
4.3.2.1. Forget to label

All visualizations should have at minimum contain the following:

A clear and descriptive Title.

All axes should be labeled using name of the variable and the units of measurement.

The axes should also be labeled with the range of values shown.

Legend, if applicable.

Don’t be like this guy:




Note that the figure above is missing axis labels. No, “wave1”, “wave2”, “wave3” and “wave4” are not proper labels for the x-axis. “Are we stuck?” is also not a very informative title.

4.3.2.2. Scale Distortion

The scale of the effect in the graphic should match the scale of the effect in the data. In other words, the difference in size of the graphics should be proportional to the difference in values in the data.

Scale Distortion
→
size of effect in graphic
≠
size of effect in data

4.3.2.3. Use uneven ranges

If ranges are not equal, then you can essentially tell any story with the data you want. Catching this is difficult, but it is a common trick deliberately used to mislead the viewer.

4.3.2.4. Use line graphs where the x-axis is not time

Plotting a line graph where the x-axis is not time is confusing. Use a bar graph instead.

4.3.2.5. Use the wrong colors

If the data is categorical, then use qualitative colormaps. Do not use sequential colormaps.

4.3.2.6. Use 3D

Please. Just. Don’t.



URL: https://fahadsultan.com/csc272/2_maths/10_prob_stats_log.html


5. Prob, Stats and Logs

Probability and statistics are related areas of mathematics which concern themselves with analyzing the relative frequency of events. Both subjects are important, relevant, and useful. But they are different, and understanding the distinction is crucial in properly interpreting the relevance of mathematical evidence.

Still, there are fundamental differences in the way they see the world:

	

Probability

	

Statistics




Goal:

	

Predicting the Future

	

Summarizing the Past




Branch of Mathematics:

	

Theoretical (pure math)

	

Application-driven (applied math)




Reasoning / Inference:

	

Deduction (Rules -> Data)

	

Induction (Data -> Rules)




Worldview:

	

Ideal

	

Real / Messy




Level of Confidence:

	

Certainty

	

Estimation




Researchers / Practitioners

	

Happy go-lucky

	

Tormented




In summary, probability theory enables us to find the consequences of a given ideal world, while statistical theory enables us to measure the extent to which our world is ideal. This constant tension between theory and practice is why statisticians prove to be a tortured group of individuals compared with the happy-go-lucky probabilists.

This distinction will perhaps become clearer if we trace the thought process of a mathematician encountering their first gambling game:

Fig. 5.1 Modern probability theory first emerged from gambling tables of France in mid 1600s. Blaise Pascal and Pierre de Fermat wondered whether the player or the house had the advantage in a particular betting game.

If a gambler were a probabilist, they would see the dice and think “Six-sided dice? Each side of the dice is presumably equally likely to land face up. Now assuming that each face comes up with probability 1/6, I can figure out what my chances are of winning.”

If instead a gambler were statistician, they would see the dice and think “How do I know that they are not loaded?” They’ll watch the game a while, and keep track of how often each number comes up. Then they can decide if my observations are consistent with the assumption of equal-probability faces. Once they have the probabilities figured out, they can call a probabilist to tell them how to bet.

In this course, we will use Statistics to go from Data to Probability-based models, as shown in the figure below:

Having learned or estimated the model, we can then use Probability to make predictions about the future.



URL: https://fahadsultan.com/csc272/2_maths/11_probability.html


5.1. Probability

Probability allows us to talk about uncertainty, in certain terms. Once, we are able to quantify uncertainties, we can deterministically make deductions about the future. The language of statistics also allows us to talk about uncertainty in uncertain but tractable terms that we can reason about.

5.1.1. Random Variable

A random variable is a mathematical formalization of an abstract quantity that has some degree of uncertainty associated with the values it may take on. The set of all possible values that a random variable can take on is called its range.

A random variable is very much similar to a variable in computer programming. In the context of pandas, a random variable is a column or feature in a DataFrame.

Just as numerical features in a DataFrame can be either discrete or continuous, random variables can also be either discrete or continuous. The two types require different mathematical formalizations as we will see later.

Random variables are usually denoted by capital letters, such as 
𝑋
 or 
𝑌
. The values that a random variable can take on are denoted by lower case letters, such as 
𝑥
 or 
𝑦
.

It is important to note that in the real world, it is often impossible to obtain the range of a random variable. Since most real-world datasets are samples, df['X'].unique() does not necessarily give us the range of 
𝑋
.

It is also important to remember that 
𝑥
 is a single value but 
𝑋
 is a collection of values (i.e. pd.Series).

In the example below, 
𝐶
 (coin) and 
𝐷
 (dice) are two random variables.

import pandas as pd 

data = pd.read_csv('../data/experiment.csv')
data.head()

	C	D
0	T	1
1	T	3
2	T	3
3	H	2
4	T	1

The ranges of 
𝐶
 and 
𝐷
 are 
{
𝐻
,
𝑇
}
 and 
{
1
,
2
,
3
,
4
,
5
,
6
}
 respectively. It is worth repeating for emphasis that the ranges of the two variables is independent of observed data, since the observed data is a limited sample.

5.1.2. Experiment, Outcome 
𝜔
 and Sample Space 
Ω

An outcome, denoted by 
𝜔
, is the set of values that one or more random variables take on as a result of an experiment.

An experiment is a process that yields outcomes out of set of all possible outcomes.

The sample space, denoted by 
Ω
, is the set of all possible outcomes.

The important operative word here is “possible”. The sample space is not the set of all observed outcomes, the set of all possible outcomes.

If an experiment involves two random variables say 
𝑋
 and 
𝑌
 which can take on 
𝑛
 possible values (i.e. 
 
range
𝑋
=
{
𝑥
1
,
𝑥
2
,
…
,
𝑥
𝑛
}
)
 and 
𝑚
 possible values (i.e. 
 
range
𝑌
=
{
𝑦
1
,
𝑦
2
,
…
,
𝑦
𝑚
}
) respectively, then the sample space 
Ω
 is the set of all possible combinations of 
𝑥
𝑖
 and 
𝑦
𝑗
 and is of size 
𝑛
×
𝑚
.




𝜔
𝑖

	

𝑋

	

𝑌




𝜔
1

	

𝑥
1

	

𝑦
1




𝜔
2

	

𝑥
1

	

𝑦
2




:

	

:

	

:




𝜔
𝑚

	

𝑥
1

	

𝑦
𝑚




𝜔
𝑚
+
1

	

𝑥
2

	

𝑦
1




𝜔
𝑚
+
2

	

𝑥
2

	

𝑦
2




:

	

:

	

:




𝜔
𝑛
×
𝑚

	

𝑥
𝑛

	

𝑦
𝑚




In other words, the sample space is the cross product of the ranges of all random variables involved in the experiment.

In our example, the experiment is the act of tossing a coin and rolling a dice.

Each row in the data is an outcome 
𝑤
𝑖
 from the set of all possible outcomes 
Ω
.

𝐶
 variable can take on two (
𝑛
=
2
) values: 
{
𝐻
,
𝑇
}
 and 
𝐷
 variable can take on six 
𝑚
=
6
 value: 
{
1
,
2
,
3
,
4
,
5
,
6
}
. This means that the sample space 
Ω
 is of size 
𝑛
×
𝑚
=
2
×
6
=
12
.

However, the observed outcomes are only 11, as shown below.

data.groupby(['C', 'D']).count().reset_index()

	C	D
0	H	1
1	H	2
2	H	3
3	H	4
4	H	5
5	H	6
6	T	1
7	T	2
8	T	3
9	T	4
10	T	5

This means that the sample space 
Ω
 is not the set of all observed outcomes. This is despite the fact that many observed outcomes are observed more than once. The missing outcome, that is never observed, is 
𝑤
12
=
(
𝑇
,
6
)
.

5.1.3. Probability Model 
𝑃
(
𝑋
)

Probability model is a function that assigns a probability score 
𝑃
(
𝜔
𝑖
)
 to each possible outcome 
𝜔
𝑖
 for every 
𝜔
𝑖
∈
Ω
 such that



0
<
𝑃
(
𝜔
𝑖
)
<
1
 
 
 
and
 
 
 
∑
𝜔
∈
Ω
𝑃
(
𝜔
𝑖
)
=
1

For example, if we have a random variable 
𝐷
 for rolling a die, the probability model assigns a probability to each number that we can roll. The probability model is usually denoted by 
𝑃
(
𝜔
𝑖
)
 or 
𝑃
(
𝐷
=
𝑑
)

𝜔

	

𝐷

	

𝑃
(
𝐷
=
𝑑
)




𝜔
1

	

1

	

𝑃
(
𝐷
=
1
)




𝜔
2

	

2

	

𝑃
(
𝐷
=
2
)




𝜔
3

	

3

	

𝑃
(
𝐷
=
3
)




𝜔
4

	

4

	

𝑃
(
𝐷
=
4
)




𝜔
5

	

5

	

𝑃
(
𝐷
=
5
)




𝜔
6

	

6

	

𝑃
(
𝐷
=
6
)

such that 
0
≤
𝑃
(
𝐷
=
𝑑
)
≤
1
 and and 
∑
𝑑
∈
𝐷
𝑃
(
𝑑
=
𝐷
)
=
1
.

fair_die = pd.read_csv('../data/fair_die.csv')
fair_die

	D	P(D)
0	1	0.166667
1	2	0.166667
2	3	0.166667
3	4	0.166667
4	5	0.166667
5	6	0.166667

The code cell above shows the probability model for the random variable 
𝐷
 for a fair die in our examples, where each number has a probability of 
1
6
.

import seaborn as sns 
from matplotlib import pyplot as plt

axs = sns.catplot(data=fair_die, kind='bar', x="D", y="P(D)", color="lightblue");
axs.set(title="Probability distribution of rolling a fair die");


A word of caution on mathematical notation and dimensionality:

Uppercase letters (
𝑋
,
𝑌
.
.
.
) often refer to a random variable. Lowercase letters (
𝑥
,
𝑦
.
.
.
) often refer to a particular outcome of a random variable.

The following refer to a probability value (int, float etc.):

𝑃
(
𝑋
=
𝑥
)

also written in shorthand as 
𝑃
(
𝑥
)

𝑃
(
𝑋
=
𝑥
∧
𝑌
=
𝑦
)

also written in shorthand as 
𝑃
(
𝑥
,
𝑦
)

The following refer to a collection of values (pd.Series, pd.DataFrame etc.):

𝑃
(
𝑋
)

𝑃
(
𝑋
∧
𝑌
)

also written as P(X, Y)

𝑃
(
𝑋
=
𝑥
,
𝑌
)

5.1.4. Probability of an Event 
𝑃
(
𝜙
)

An event 
𝜙
 is a set of possible worlds 
{
𝜔
𝑖
,
𝜔
𝑗
,
.
.
.
𝜔
𝑛
}
. In other words, an event 
𝜙
 is a subset of 
Ω
 i.e. 
𝜙
⊂
Ω

If we continue with the example of rolling a die, we can define an event 
𝜙
 as the set of all possible worlds where the die rolls an even number. From the table above, we can see that there are three possible worlds where the die rolls an even number.

Therefore, the event 
𝜙
 is the set 
{
𝜔
2
,
𝜔
4
,
𝜔
6
}
 or 
{
𝐷
=
2
,
𝐷
=
4
,
𝐷
=
6
}
.

𝑃
(
𝜙
)
=
∑
𝜔
∈
𝜙
𝑃
(
𝜔
)
 is the sum of probabilities of the set of possible worlds defining 
𝜙

𝑃
(
𝜙
1
)
=
𝑃
(
Die rolls an even number
)
=
𝑃
(
𝜔
2
)
+
𝑃
(
𝜔
4
)
+
𝑃
(
𝜔
6
)
=
0.167
+
0.167
+
0.167
≈
0.5

event_condition = fair_die['D'].apply(lambda x: x % 2 == 0)

event = fair_die[event_condition]

P_event = event['P(D)'].sum()

round(P_event, 2)

0.5

5.1.5. Joint Probability 
𝑃
(
𝐴
,
𝐵
)

Joint probability is the probability of two events occurring together.The joint probability is usually denoted by 
𝑃
(
𝐴
,
𝐵
)
, which is shorthand for 
𝑃
(
𝐴
∧
𝐵
)
 read as Probability of 
𝐴
 AND 
𝐵
.

Note that 
𝑃
(
𝐴
,
𝐵
)
=
𝑃
(
𝐵
,
𝐴
)
 since 
𝐴
∧
𝐵
=
𝐵
∧
𝐴
.

For example, if we are rolling two dice, the joint probability is the probability of rolling a 1 on the first die and a 2 on the second die.

In Data Science, we rarely know the true joint probability. Instead, we estimate the joint probability from data. We will talk more about this when we talk about Statistics.

joint_probs = pd.read_csv('../data/experiment_probs.csv')
joint_probs

	C	D	P(C, D)
0	H	1	0.24
1	H	2	0.13
2	H	3	0.09
3	H	4	0.01
4	H	5	0.03
5	H	6	0.01
6	T	1	0.19
7	T	2	0.09
8	T	3	0.13
9	T	4	0.04
10	T	5	0.00
11	T	6	0.04
joint_probs['P(C, D)'].sum()

1.0


Note that sum of joint probabilities is 1 i.e. 
∑
𝑃
(
𝐶
,
𝐷
)
=
1
 at the end of the day, since the sum of all probabilities is 1.

The following three are all true at the same time:

∑
𝐶
,
𝐷
𝑃
(
𝐶
,
𝐷
)
=
1
 where 
𝑃
(
𝐶
,
𝐷
)
 is a probability table with 12 rows and 3 columns: 
𝐶
,
𝐷
,
𝑃
(
𝐶
,
𝐷
)
.

∑
𝐶
𝑃
(
𝐶
)
=
1
 where 
𝑃
(
𝐶
)
 is a probability table with 2 rows (
𝐻
,
𝑇
) and 2 columns: 
𝐶
,
𝑃
(
𝐶
)
.

∑
𝐷
𝑃
(
𝐷
)
=
1
 where 
𝑃
(
𝐷
)
 is a probability table with 6 rows (
1
,
2
,
3
,
4
,
5
,
6
) and 2 columns: 
𝐷
,
𝑃
(
𝐷
)
.

joint_probs

	C	D	P(C, D)
0	H	1	0.24
1	H	2	0.13
2	H	3	0.09
3	H	4	0.01
4	H	5	0.03
5	H	6	0.01
6	T	1	0.19
7	T	2	0.09
8	T	3	0.13
9	T	4	0.04
10	T	5	0.00
11	T	6	0.04
joint_probs["CD_vals"] = joint_probs.apply(lambda x: "C=%s and D=%s" % (x['C'], x['D']), axis=1)

joint_probs

	C	D	P(C, D)	CD_vals
0	H	1	0.24	C=H and D=1
1	H	2	0.13	C=H and D=2
2	H	3	0.09	C=H and D=3
3	H	4	0.01	C=H and D=4
4	H	5	0.03	C=H and D=5
5	H	6	0.01	C=H and D=6
6	T	1	0.19	C=T and D=1
7	T	2	0.09	C=T and D=2
8	T	3	0.13	C=T and D=3
9	T	4	0.04	C=T and D=4
10	T	5	0.00	C=T and D=5
11	T	6	0.04	C=T and D=6
axs = sns.barplot(data=joint_probs, x="CD_vals", y="P(C, D)", color="lightblue");
axs.set(title="Joint probability distribution of C and D\n Note that the joint probabilities sum to 1", \
        xlabel="C and D values", \
        ylabel="P(C, D)");
plt.xticks(rotation=90);

5.1.6. Marginal Probability 
𝑃
(
𝐴
)

Because most data sets are multi-dimensional i.e. involving multiple random variables, we can sometimes find ourselves in a situation where we want to know the joint probability 
𝑃
(
𝐴
,
𝐵
)
 of two random variables 
𝐴
 and 
𝐵
 but we don’t know 
𝑃
(
𝐴
)
 or 
𝑃
(
𝐵
)
. In such cases, we compute the marginal probability of one variable from joint probability over multiple random variables.

Marginalizing is the process of summing over one or more variables (say B) to get the probability of another variable (say A). This summing takes place over the joint probability table.



𝑃
(
𝐴
)
=
∑
𝑏
∈
Ω
𝐵
𝑃
(
𝐴
,
𝐵
=
𝑏
)
P_C = joint_probs.groupby('C').sum()['P(C, D)']
P_C.name = 'P(C)'
P_D = joint_probs.groupby('D').sum()['P(C, D)']
P_D.name = 'P(D)'
P_C

C
H    0.51
T    0.49
Name: P(C), dtype: float64

P_D, P_D.sum()

(D
 1    0.43
 2    0.22
 3    0.22
 4    0.05
 5    0.03
 6    0.05
 Name: P(D), dtype: float64,
 1.0)

joint_probs

	C	D	P(C, D)
0	H	1	0.24
1	H	2	0.13
2	H	3	0.09
3	H	4	0.01
4	H	5	0.03
5	H	6	0.01
6	T	1	0.19
7	T	2	0.09
8	T	3	0.13
9	T	4	0.04
10	T	5	0.00
11	T	6	0.04
joint_probs

	C	D	P(C, D)
0	H	1	0.24
1	H	2	0.13
2	H	3	0.09
3	H	4	0.01
4	H	5	0.03
5	H	6	0.01
6	T	1	0.19
7	T	2	0.09
8	T	3	0.13
9	T	4	0.04
10	T	5	0.00
11	T	6	0.04
fig, axs = plt.subplots(1, 3, figsize=(15, 5), sharey=True)

axs[0].bar(joint_probs["CD_vals"], joint_probs["P(C, D)"], color="lightblue");
axs[1].bar(P_C.index, P_C);
axs[2].bar(P_D.index, P_D, color="navy");

axs[0].tick_params('x', labelrotation=90)

axs[0].set_title("P(C, D)");
axs[1].set_title("P(C)");
axs[2].set_title("P(D)");

fig.suptitle("P(C) and P(D) (center and right) marginalized from joint probability P(C, D) left");


As we look at new concepts in probability, it is important to stay mindful of i) what the probability sums to ii) what are the dimensions of the table that represents the probability.

You can see from the cell below that the dimensions of marginal probability table is the length of the range of the variable.

You can see from the code below that both the computed marginal probabilities in add up to 1.

P_C.sum().round(3), P_D.sum().round(3)

(1.0, 1.0)

5.1.7. Independent Random Variables

Random variables can be either independent or dependent. If two random variables are independent, then the value of one random variable does not affect the value of the other random variable.

For example, if we are rolling two dice, we can use two random variables to represent the numbers that we roll. The two random variables are independent because the value of one die does not affect the value of the other die. If two random variables are dependent, then the value of one random variable does affect the value of the other random variable. For example, if we are measuring the temperature and the humidity, we can use two random variables to represent the temperature and the humidity. The two random variables are dependent because the temperature affects the humidity and the humidity affects the temperature.

More formally, two random variables 
𝑋
 and 
𝑌
 are independent if and only if 
𝑃
(
𝑋
,
𝑌
)
=
𝑃
(
𝑋
)
⋅
𝑃
(
𝑌
)
.

P_C.name = "P(C)"
P_D.name = "P(D)"
merged = pd.merge(joint_probs, P_C, on='C')
merged = pd.merge(merged,      P_D, on='D')
merged

	C	D	P(C, D)	P(C)	P(D)
0	H	1	0.24	0.51	0.43
1	T	1	0.19	0.49	0.43
2	H	2	0.13	0.51	0.22
3	T	2	0.09	0.49	0.22
4	H	3	0.09	0.51	0.22
5	T	3	0.13	0.49	0.22
6	H	4	0.01	0.51	0.05
7	T	4	0.04	0.49	0.05
8	H	5	0.03	0.51	0.03
9	T	5	0.00	0.49	0.03
10	H	6	0.01	0.51	0.05
11	T	6	0.04	0.49	0.05
merged['P(C) P(D)'] = merged['P(C)'] * merged['P(D)']
merged

	C	D	P(C, D)	P(C)	P(D)	P(C) P(D)
0	H	1	0.24	0.51	0.43	0.2193
1	T	1	0.19	0.49	0.43	0.2107
2	H	2	0.13	0.51	0.22	0.1122
3	T	2	0.09	0.49	0.22	0.1078
4	H	3	0.09	0.51	0.22	0.1122
5	T	3	0.13	0.49	0.22	0.1078
6	H	4	0.01	0.51	0.05	0.0255
7	T	4	0.04	0.49	0.05	0.0245
8	H	5	0.03	0.51	0.03	0.0153
9	T	5	0.00	0.49	0.03	0.0147
10	H	6	0.01	0.51	0.05	0.0255
11	T	6	0.04	0.49	0.05	0.0245
merged[['P(C, D)', 'P(C) P(D)']].round(2)

	P(C, D)	P(C) P(D)
0	0.24	0.22
1	0.19	0.21
2	0.13	0.11
3	0.09	0.11
4	0.09	0.11
5	0.13	0.11
6	0.01	0.03
7	0.04	0.02
8	0.03	0.02
9	0.00	0.01
10	0.01	0.03
11	0.04	0.02

The two random variables 
𝐶
 and 
𝐷
 therefore are NOT independent because 
𝑃
(
𝐶
,
𝐷
)
≠
𝑃
(
𝐶
)
⋅
𝑃
(
𝐷
)
.

sns.barplot(data=merged, x="CD_vals", y="P(C, D)", color="navy", alpha=0.5, label="$P(C, D)$");
sns.barplot(data=merged, x="CD_vals", y="P(C) P(D)", color="orange", alpha=0.5, label="$P(C)\cdot P(D)$");
plt.xticks(rotation=90);
plt.ylabel("Probability");
plt.legend();
plt.title("$C$ and $D$ are NOT independent since $P(C, D) \\neq P(C) \cdot P(D)$");

5.1.8. Conditional Probability 
𝑃
(
𝐴
|
𝐵
)

Conditional probability is the probability of one event occurring given that another event has occurred.

The conditional probability is usually denoted by 
𝑃
(
𝐴
|
𝐵
)
 and is defined as:

𝑃
(
𝐴
|
𝐵
)
=
𝑃
(
𝐴
,
𝐵
)
𝑃
(
𝐵
)

The denominator is the marginal probability of 
𝐵
.




For example, if we are flipping two coins, the conditional probability of flipping heads in the second toss, knowing the first toss was tails is:

Possible world

	

Coin
1

	

Coin
2

	

𝑃
(
𝜔
)




𝜔
1

	

H

	

H

	

0.25




𝜔
2

	

H

	

T

	

0.25




𝜔
3

	

T

	

H

	

0.25




𝜔
4

	

T

	

T

	

0.25

𝑃
(
Coin
2
=
𝐻
|
Coin
1
=
𝑇
)
=
𝑃
(
Coin
2
=
𝐻
,
Coin
1
=
𝑇
)
𝑃
(
Coin
1
=
𝑇
)
=
0.25
0.5
=
0.5
merged

	C	D	P(C, D)	P(C)	P(D)	P(C) P(D)
0	H	1	0.24	0.51	0.43	0.2193
1	T	1	0.19	0.49	0.43	0.2107
2	H	2	0.13	0.51	0.22	0.1122
3	T	2	0.09	0.49	0.22	0.1078
4	H	3	0.09	0.51	0.22	0.1122
5	T	3	0.13	0.49	0.22	0.1078
6	H	4	0.01	0.51	0.05	0.0255
7	T	4	0.04	0.49	0.05	0.0245
8	H	5	0.03	0.51	0.03	0.0153
9	T	5	0.00	0.49	0.03	0.0147
10	H	6	0.01	0.51	0.05	0.0255
11	T	6	0.04	0.49	0.05	0.0245
merged['P(D | C)'] = merged['P(C, D)'] / merged['P(C)']
merged['P(D | C)']

0     0.470588
1     0.387755
2     0.254902
3     0.183673
4     0.176471
5     0.265306
6     0.019608
7     0.081633
8     0.058824
9     0.000000
10    0.019608
11    0.081633
Name: P(D | C), dtype: float64

merged[['C', 'D', 'P(D | C)']]

axs = sns.catplot(data=merged, x="D", y="P(D | C)", hue="C", kind="bar");
axs.set(title="Conditional probability distribution of D given C\nNote that the blue bars add up to 1");


Note that the sum of conditional probabilites, unlike joint probability, is not 1.

merged["P(D | C)"].sum()

2.0


This is because






∑
𝐶
∑
𝐷
𝑃
(
𝐷
|
𝐶
)
=
∑
𝐷
𝑃
(
𝐷
|
𝐶
=
Heads
)
+
∑
𝐷
𝑃
(
𝐷
|
𝐶
=
Tails
)

And 
∑
𝐷
𝑃
(
𝐷
|
𝐶
=
Heads
)
 and 
∑
𝐷
𝑃
(
𝐷
|
𝐶
=
Tails
)
 are individually probability distributions that each sum to 1, over different values of 
𝐷
.

In other words, in the plot above, the blue bars add up to 1 and the orange bars add up to 1.

heads = merged[merged["C"] == "H"]
tails = merged[merged["C"] == "T"]

heads["P(D | C)"].sum(), tails["P(D | C)"].sum()

(1.0, 1.0)

5.1.9. Product Rule 
𝑃
(
𝐴
,
𝐵
)

Rearranging the definition of conditional probability, we get the product rule:

𝑃
(
𝐴
,
𝐵
)
=
𝑃
(
𝐴
|
𝐵
)
⋅
𝑃
(
𝐵
)

Similarly, we can also write:

𝑃
(
𝐴
,
𝐵
)
=
𝑃
(
𝐵
|
𝐴
)
⋅
𝑃
(
𝐴
)

In summary,

𝑃
(
𝐴
,
𝐵
)
=
𝑃
(
𝐴
|
𝐵
)
⋅
𝑃
(
𝐵
)
=
𝑃
(
𝐵
|
𝐴
)
⋅
𝑃
(
𝐴
)
5.1.10. Chain Rule 
𝑃
(
𝐴
,
𝐵
,
𝐶
)

The chain rule is a generalization of the product rule to more than two events.

𝑃
(
𝐴
,
𝐵
,
𝐶
)
=
𝑃
(
𝐴
|
𝐵
,
𝐶
)
⋅
𝑃
(
𝐵
,
𝐶
)

𝑃
(
𝐴
,
𝐵
,
𝐶
)
=
𝑃
(
𝐴
|
𝐵
,
𝐶
)
⋅
𝑃
(
𝐵
|
𝐶
)
⋅
𝑃
(
𝐶
)

since 
𝑃
(
𝐵
,
𝐶
)
=
𝑃
(
𝐵
|
𝐶
)
⋅
𝑃
(
𝐶
)
 as per the product rule.

Chain rule essentially allows expressing the joint probability of multiple random variables as a product of conditional probabilities. This is useful because conditional probabilities are often easier to estimate from data than joint probabilities.

5.1.11. Inclusion-Exclusion Principle 
𝑃
(
𝐴
∨
𝐵
)

Inclusion-Exclusion Principle is a way of calculating the probability of two events occurring i.e. 
𝑃
(
𝐴
=
𝑎
 
OR
 
𝐵
=
𝑏
)
 denoted generally as 
𝑃
(
𝐴
=
𝑎
∨
𝐵
=
𝑏
)
.

It is defined as:

𝑃
(
𝐴
=
𝑎
∨
𝐵
=
𝑏
)
=
𝑃
(
𝐴
=
𝑎
)
+
𝑃
(
𝐵
=
𝑏
)
−
𝑃
(
𝐴
=
𝑎
∧
𝐵
=
𝑏
)

For example, if we are rolling two dice, the Inclusion-Exclusion Principle can be used to calculate the probability of rolling a 1 on the first die or a 2 on the second die.

𝑃
(
Coin
1
=
𝐻
∨
Coin
2
=
𝑇
)

=
𝑃
(
Coin
2
=
𝐻
)
+
𝑃
(
Coin
1
=
𝑇
)
−
𝑃
(
Coin
2
=
𝐻
∧
Coin
1
=
𝑇
)

=
0.5
+
0.5
−
0.25

=
0.75

5.1.12. Bayes Theorem 
𝑃
(
𝐴
|
𝐵
)

Bayes theorem is a way of calculating conditional probability. For example, if we are rolling two dice, Bayes theorem can be used to calculate the probability of rolling a 1 on the first die given that we rolled a 2 on the second die.

𝑃
(
𝐴
|
𝐵
)
=
𝑃
(
𝐵
|
𝐴
)
⋅
𝑃
(
𝐴
)
𝑃
(
𝐵
)

𝑃
(
𝐴
|
𝐵
)
 in the context of Bayes theorem is called the Posterior probability.

𝑃
(
𝐵
|
𝐴
)
 is called the Likelihood.

𝑃
(
𝐴
)
 is called the Prior probability.

𝑃
(
𝐵
)
 is called the Evidence, also known as Marginal Likelihood.

𝑃
(
Posterior
)
=
𝑃
(
Likelihood
)
⋅
𝑃
(
Prior
)
𝑃
(
Evidence
)

Bayes Theorem allows a formal method of updating prior beliefs with new evidence and is the foundation of Bayesian Statistics. We will talk more about this when we talk about Statistics.

In machine learning, the task is often to find 
𝑃
(
𝑌
|
𝑋
1
=
𝑥
1
,
𝑋
2
=
𝑥
2
,
…
𝑋
𝐷
=
𝑥
𝐷
)
 i.e. the probability of an unknown Y, given some values for 
𝐷
 features (
𝑋
1
,
𝑋
2
…
𝑋
𝐷
). Bayes theorem allows us to calculate this probability from the data.

Let’s assume we are interested in predicting if a person is a football player (
𝑌
𝐹
=
1
) or not (
𝑌
𝐹
=
0
), given their height (
𝑋
𝐻
) and weight (
𝑋
𝑊
).

Say, we observe a person who is 7 feet tall and weighs 200 pounds. We can use Bayes theorem to calculate the probability of this person being a football player using the following equation:

𝑃
(
𝑌
|
𝑋
𝐻
=
7
,
𝑋
𝑊
=
200
)
=
𝑃
(
𝑋
𝐻
=
7
,
𝑋
𝑊
=
200
|
𝑌
𝐹
)
⋅
𝑃
(
𝑌
𝐹
)
𝑃
(
𝑋
𝐻
=
7
,
𝑋
𝑊
=
200
)

Note that here 
𝑃
(
𝑋
𝐻
=
7
,
𝑋
𝑊
=
200
|
𝑌
𝐹
)
 is the Likelihood probability of observing someone who is 7 feet tall and weighs 200 pounds, knowing if they are a football player.

𝑃
(
𝑌
𝐹
)
 is the Prior probability of a person being a football player out of the entire population.

𝑃
(
𝑋
𝐻
=
7
,
𝑋
𝑊
=
200
)
 is the probability of the Evidence i.e. probability of observing anyone who is 7 feet tall and weighs 200 pounds in the entire population.



URL: https://fahadsultan.com/csc272/2_maths/12_stats.html


5.2. Statistics

Every variable we observe in the data has a particular frequency distribution and in turn a probability distribution. These exact distribtions are unique to the variable under consideration. However, the shapes of these distributions are not unique. There are a few common shapes that we see over and over again. In other words, the world’s rich variety of data appears only in a small number of classical shapes. Once abstracted from specific data observations, they become probability distributions, worthy of independent study.

These classical distributions have two nice properties:

They describe the frequency distributions that often arise in practice.

More importantly, they can be described as a mathematical function P(X) with very few parameters (unknowns).

As indicated in the previous section, probability provides a way to express and reason about uncertainty. In other words, once you have probability distributions, you can use them to reason about the world. However, in the real world, we don’t know the probability distributions. We have to estimate them from data.

This is where statistics comes in. Statistics allows us to look at data and intelligently guess what the underlying probability distributions might be in the following two steps:

Pick or assume an underlying probability distribution that we think might have generated the data.

Estimate the parameters of the assumed probability distribution from the data.

Below we will look at some of the most common distributions for categorical variables, focusing specifically on the parameters that define them. Once we have estimated parameters, we have a complete description of the probability distribution that can be used to reason about the world.

5.2.1. Categorical Statistical Distributions
5.2.1.1. Bernoulli 🪙

Number of possible outcomes 
𝑘
=
2

Number of trials 
𝑛
=
1

Example: Coin toss (Heads/Tails), yes/no, true/false, success/failure

Number of parameters: 
1

Parameter: Probability of success 
𝑝

Bernoulli Distribution is a discrete probability distribution used to model a single trial 
𝑛
=
1
 of a binary random that can have two possible outcomes 
𝑘
=
2
.

For instance, if we were interested in probability of observing a head in a single coin toss, we would use the Bernoulli distribution, where “1” is defined to mean “heads” and “0” is defined to mean “tails”.

The Bernoulli distribution has a single parameter, the probability of success, which we will call 
𝑝
. The probability of observing a head is 
𝑝
 and the probability of observing a tail is 
𝑞
=
1
−
𝑝
. The probability function of the Bernoulli distribution is:

	
		
	
𝑃
(
𝑥
)
	
=
𝑝
𝑥
(
1
−
𝑝
)
1
−
𝑥

	
=
{
𝑝
	
if 
𝑥
=
1


1
−
𝑝
	
if 
𝑥
=
0

where 
𝑥
 is the outcome of the coin toss.

If we observe a head, then 
𝑥
=
1
 and the probability function is 
𝑃
(
1
)
=
𝑝
. If we observe a tail, then 
𝑥
=
0
 and the likelihood function is 
𝑃
(
0
)
=
1
−
𝑝
.

If a variable 
𝑋
 follows Bernoulli distribtion with 
𝑝
=
0.5
, it is denoted as 
𝑋
∼
Bernoulli
(
𝑝
)
.

import seaborn as sns
import pandas as pd

data = pd.DataFrame()
data["X"] = ["Heads", "Tails"]
data["P(X)"] = [0.5, 0.5]
axs = sns.barplot(data=data, x="X", y="P(X)", color="lightblue");
axs.set(title="Bernoulli Distribution of a Fair Coin Flip \n $X \\sim Bernoulli(0.5)$");

5.2.1.2. Categorical 🎲

Number of possible outcomes 
𝑘
>
2

Number of trials 
𝑛
=
1

Example: Rolling a die, choosing a color, choosing a letter

Number of parameters: 
𝑘

Parameter: Probability of each outcome 
𝑝
1
,
𝑝
2
,
…
,
𝑝
𝑘

The categorical distribution is a generalization of the Bernoulli distribution. It models the probability of observing a particular outcome from a set of 
𝑘
>
2
 outcomes in a single trials.

For example, it models the probability of observing a particular face when rolling a k-sided die once.

The probability mass function of the categorical distribution is:

	
	
	
	
𝑓
(
𝑥
)
=
{
𝑝
1
	
if 
𝑥
=
1


𝑝
2
	
if 
𝑥
=
2


⋮


𝑝
𝑘
	
if 
𝑥
=
𝑘

where 
𝑝
1
,
𝑝
2
,
…
,
𝑝
𝑘
 are the probabilities of observing each of the 
𝑘
 outcomes. The probabilities must sum to 1.

cat_data = pd.DataFrame()
cat_data['X'] = range(1, 7)
cat_data['Y'] = 1/6
axs = sns.barplot(data=cat_data, x="X", y="Y", color="lightblue");
axs.set(title="Categorical Distribution of a Fair Die Roll");

from scipy.stats import categorical

n = 50
p = 0.5

binom_data = pd.DataFrame()
binom_data['X'] = range(51)
binom_data['Y'] = binom.pmf(binom_data['X'], n, p)

axs = sns.lineplot(data=binom_data, x=binom_data['X'], y=binom_data['Y']);
axs.set(title="Binomial Distribution of 50 Flips of a Fair Coin \n $X \\sim Binomial(n=50, p=0.5)$");

5.2.1.3. Binomial 🪙 🪙 🪙

Number of possible outcomes 
𝑘
=
2

Number of trials 
𝑛
>
1

Example: Count of Heads in 
𝑛
 coin tosses

Number of parameters: 
2

Parameters: Probability of success 
𝑝
 and number of trials 
𝑛

The binomial distribution is the discrete probability distribution of the number of successes in a sequence of 
𝑛
 independent trials, each asking a yes–no question, and each with its own boolean-valued outcome: success/yes/true/one (with probability 
𝑝
) or failure/no/false/zero (with probability 
𝑞
=
1
−
𝑝
).

For example, if we were interested in probability of observing a head in 
𝑛
 coin tosses, we would use the binomial distribution, where “1” is defined to mean “heads” and “0” is defined to mean “tails”.

The probability function of the binomial distribution is:

𝑃
(
𝑋
=
𝑥
)
=
(
𝑛
𝑥
)
𝑝
𝑥
(
1
−
𝑝
)
𝑛
−
𝑥

where 
𝑛
 is the number of trials and 
𝑝
 is the probability of success. The binomial coefficient 
(
𝑛
𝑥
)
 is the number of ways to choose 
𝑥
 items from a set of 
𝑛
 items. The binomial coefficient is defined as:

(
𝑛
𝑥
)
=
𝑛
!
𝑥
!
(
𝑛
−
𝑥
)
!

where 
𝑛
!
 is the factorial of 
𝑛
.

from scipy.stats import binom

n = 50
p = 0.5

binom_data = pd.DataFrame()
binom_data['X'] = range(51)
binom_data['Y'] = binom.pmf(binom_data['X'], n, p)

axs = sns.lineplot(data=binom_data, x=binom_data['X'], y=binom_data['Y']);
axs.set(title="Binomial Distribution of 50 Flips of a Fair Coin \n $X \\sim Binomial(n=50, p=0.5)$");

5.2.1.4. Multinomial 🎲 🎲 🎲

Number of possible outcomes 
𝑘
>
2

Number of trials 
𝑛
>
1

Example: As a result of 
𝑛
=
9
 rolls of a die,
Count of 1-face ⚀ = 2 
∧

Count of 2-face ⚁ = 1 
∧

Count of 3-face ⚂ = 2 
∧

Count of 4-face ⚃ = 1 
∧

Count of 5-face ⚄ = 2 
∧

Count of 6-face ⚅ = 1

Number of parameters: 
𝑘
+
2

Parameters: 
𝑛
, 
𝑘
 and probability of each outcome 
𝑝
1
,
𝑝
2
,
…
,
𝑝
𝑘
 such that 
∑
𝑖
=
1
𝑘
𝑝
𝑖
=
1

Multinomial distribution is a multivariate generalization of the binomial distribution. It models the probability of observing a particular count for each of 
𝑘
>
2
 outcomes in 
𝑛
>
1
 trials.

For example, it models the probability of counts for rolling a k-sided die n times.

The probability function of the multinomial distribution is:

𝑃
(
𝑋
=
{
𝑥
1
,
𝑥
2
,
…
𝑥
𝑘
}
)
=
𝑛
!
𝑥
1
!
𝑥
2
!
…
𝑥
𝑘
!
𝑝
1
𝑥
1
𝑝
2
𝑥
2
…
𝑝
𝑘
𝑥
𝑘

where 
𝑛
 is the number of trials, 
𝑘
 is the number of outcomes, 
𝑛
1
,
𝑛
2
,
…
,
𝑛
𝑘
 are the counts for each outcome, and 
𝑝
1
,
𝑝
2
,
…
,
𝑝
𝑘
 are the probabilities of each outcome. The probabilities must sum to 1 and the counts must sum to 
𝑛
 i.e. 
∑
𝑖
=
1
𝑘
𝑝
𝑖
=
1
 and 
∑
𝑖
=
1
𝑘
𝑥
𝑖
=
𝑛
.

For instance, using Multinomial distribution, as a result of rolling a die 9 times, we can calculate the probability of observing each even-sided face 2 times and each odd-sided face 1 times as follows :

⚀⚁⚂⚃⚄⚅
𝑃
(
#
⚀
=
1
,
#
⚁
=
2
,
#
⚂
=
1
,
#
⚃
=
2
,
#
⚄
=
1
,
#
⚅
=
2
)
=
9
!
1
!
2
!
1
!
2
!
1
!
2
!
(
1
6
)
9
=
362880
8
×
(
1
6
)
9
=
0.0045

Visualizing the probability distribution of the Multinomial distribution is hard. The following figure shows the probability distribution of the Multinomial distribution for 
𝑛
=
5
, 
𝑘
=
3
, 
𝑝
1
=
0.5
, 
𝑝
2
=
0.3
 and 
𝑝
3
=
0.2
.

5.2.1.5. Summary

The following table presents a summary and comparison of the categorical statistical distributions:

Distribution

	

Number of Trials 
𝑛

	

Number of Outomes 
𝑘

	

Number of parameters

	

Parameters

	

Example




Bernoulli

	

1

	

2

	

1

	

𝑝

	

A single coin toss




Categorical

	

1

	

>
2

	

𝑘

	

𝑝
1
,
𝑝
2
,
…
,
𝑝
𝑘

	

A single roll of a die




Binomial

	

>
1

	

2

	

2

	

𝑛
,
𝑝

	

Count of heads in 
𝑛
 coin tosses




Multinomial

	

>
1

	

>
2

	

𝑘
+
2

	

𝑛
,
𝑘
,
𝑝
1
,
𝑝
2
,
…
,
𝑝
𝑘

	

Count of each face in 
𝑛
 rolls of a die

5.2.2. Maximum Likelihood Estimation

Maximum likelihood estimation (MLE) is a method of estimating the parameters of a probability distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable. The point in the parameter space that maximizes the likelihood function is called the maximum likelihood estimate.

Likelihood function is the probability of observing the data given the parameters.

Maximum likelihood estimate of Bernoulli distribution’s only parameter 
𝑝
, given data, is:

𝑝
^
=
Number of successes
Number of observations

Similarly, Maximum likelihood estimate of 
𝑘
 parameters of Categorical distribution is:

	

	

	
𝑝
^
𝑖
	
=
Number of observations of outcome i
Number of observations



𝑛
^
	
=
Number of observations

Note that the maximum likelihood estimate of the parameters of the Binomial and Multinomial distributions are not as simple as the Bernoulli and Categorical distributions and require multiple samples. However, since there is a Bernoulli distribution for each trial of the Binomial distribution and a Categorical distribution for each trial of the Multinomial distribution, we can use the maximum likelihood estimates of the Bernoulli and Categorical distributions to estimate the parameters of the Binomial and Multinomial distributions.

5.2.3. Law of Large Numbers

The law of large numbers states that as the number of trials of a random experiment increases, the average of the observed outcomes approaches the expected value.

For example, if we toss a fair coin 10 times, we might expect to observe 5 heads but may or may not get 5. However, if we toss a fair coin 1,000,000 times, we can confidently expect to observe nearabouts 500,000 heads, if the coin is indeed fair.

The law of large numbers is the basis for the frequentist approach to statistics. The frequentist approach to statistics is based on the idea that the probability of an event is the long-run relative frequency of the event. In other words, the probability of an event is the proportion of times the event occurs in a large number of trials.

In the context of MLE, the law of large numbers implies that as the number of observations increases, the maximum likelihood estimate of the parameters approaches the true parameters of the distribution.

from numpy.random import choice 

n = 500

data = pd.DataFrame()
data['X'] = choice([0, 1], size=n)
data['trials'] = range(1, n+1)
data['p_hat'] = data['X'].cumsum() / data['trials']
sns.lineplot(data=data, x='trials', y='p_hat', label='estimated p');
axs = sns.lineplot(data=data, x='trials', y=0.5, color='red', label='true p');

axs.set(title="Estimating the Probability of Heads $\\hat{p} = \\frac{Count(x=1)}{n}$ \n " + \
              "$X \\sim Bernoulli(p)$ \n Note ", xlabel='Number of Trials', ylabel='Estimated Probability of Heads');

5.2.4. Central Dogma of Statistics

The central dogma of data science, simply put, is that general claims about a population can be made from a sample of data.

This raises concerns about the sampling process such as the representativeness of the sample, the size of the sample, the sampling bias, etc. Which in turn raises concerns about potential negative effects of the claims made based on questionable data.

Issues with sampling and underrepresentative data are not new. In 1936, the Literary Digest magazine conducted a poll to predict the outcome of the presidential election. The poll was based on a sample of 2.4 million people. The poll predicted that Alf Landon would win the election with 57% of the vote. However, the actual election results were the opposite. Franklin D. Roosevelt won the election with 62% of the vote.

The reason for the failure of the poll was that the sample was not representative of the population. The sample was biased towards the wealthy and the educated. The poll was also conducted by sending out postcards to people who subscribed to the Literary Digest magazine. In 1936, only the wealthy and the educated subscribed to magazines.

The Literary Digest magazine went bankrupt soon after the election. George Gallup, who correctly predicted the outcome of the election, went on to found the American Institute of Public Opinion, which later became the Gallup Poll.

The big takeaway from this story is that size alone is not enough. In other words, large sample sizes is a necessary but not sufficient condition for making general claims about a population.



URL: https://fahadsultan.com/csc272/2_maths/13_log.html


5.3. Logarithms

The logarithm is the inverse exponential function of base b. That is,

if 
𝑦
=
𝑒
𝑥
then 
𝑙
𝑜
𝑔
𝑒
𝑦
=
𝑥

From the two equations above, it follows that

𝑙
𝑜
𝑔
𝑒
𝑒
𝑥
=
𝑥

Here 
𝑒
 is an irrational constant called Euler’s number that begins with 2.718281828459.

import pandas as pd 
import seaborn as sns
from math import log, exp
from matplotlib import pyplot as plt

data = pd.DataFrame()
data['X'] = range(1, 101)
data['Y'] = data['X'].apply(lambda x: math.e ** x)

axs = sns.lineplot(data=data, x='X', y='Y');
axs.set(title="$Y = f(x) = e^{~x}$" + " where " + "$x \in [0, 100)$");


Note

Note that to use the log function in python, we have to import it from the math library:

from math import log

Also note that the default base of the built-in log function is 
𝑒
.

Now let’s try to apply the logarithm to the equation 
𝑦
=
𝑒
𝑥
 and call it 
𝑙
𝑜
𝑔
(
𝑦
)
. For additional comparison, let’s also compute 
𝑙
𝑜
𝑔
(
𝑥
)
.

data['log(Y)'] = data['Y'].apply(lambda x: log(x))
data['log(X)'] = data['X'].apply(lambda x: log(x))
axs = sns.lineplot(data=data, x='X', y='Y');
axs = sns.lineplot(data=data, x='X', y='log(Y)');
axs = sns.lineplot(data=data, x='X', y='log(X)');
plt.legend(labels=['$Y = e^{x}$', '$log_e(Y)$', '$log_e(X)$']);
plt.ylim(0, 50);
plt.xlim(0, 50);


Exponential functions grow at a very fast rate. In contrast, logarithms grow a very slow rate.

data.tail()

	X	Y	log(Y)	log(X)
95	96	4.923458e+41	96.0	4.564348
96	97	1.338335e+42	97.0	4.574711
97	98	3.637971e+42	98.0	4.584967
98	99	9.889030e+42	99.0	4.595120
99	100	2.688117e+43	100.0	4.605170

For comparison, there are 1e+42 stars in the universe. Meaning 
2.1718
100
>
 stars in the universe.

Logarithm to the base 
𝑒
 is called the natural logarithm. The natural logarithm of a number 
𝑥
 is generally written as 
ln
 
𝑥
 or 
𝑙
𝑜
𝑔
𝑒
𝑥
.

The relationship between the exponential function and the logarithm is preserved when considering logarithms to any other base. Therefore, more generally,

if 
𝑦
=
𝑏
𝑥
then 
𝑙
𝑜
𝑔
𝑏
𝑦
=
𝑥

The logarithm of a number y to base b is the exponent to which we must raise b to get y.

For example, 
2
3
=
8
, so 
𝑙
𝑜
𝑔
2
8
=
3
.

Logarithms are very useful things, and arise often in data analysis. Here I detail three important roles logarithms play in data science.

5.3.1. Monotonically Increasing Function

A function 
𝑓
(
𝑥
)
 is monotonically increasing if 
𝑥
1
>
𝑥
2
 implies that 
𝑓
(
𝑥
1
)
>
𝑓
(
𝑥
2
)
.

The logarithm is a monotonically increasing function. This means that if 
𝑥
1
>
𝑥
2
 then 
𝑙
𝑜
𝑔
(
𝑥
1
)
>
𝑙
𝑜
𝑔
(
𝑥
2
)
.

This is a very useful property, because it means that we can apply a logarithmic transformation without changing the order of the data. Preserving the relative order of the data is important for many statistical techniques.

axs = sns.scatterplot(data=data, x='X', y='Y',      hue='X', palette='bright', s=80);
axs = sns.scatterplot(data=data, x='X', y='log(Y)', hue='X', palette='bright', s=80);
axs = sns.scatterplot(data=data, x='X', y='log(X)', hue='X', palette='bright', s=80);
axs = sns.lineplot(data=data, x='X', y='Y', label='Y', alpha=0.5);
axs = sns.lineplot(data=data, x='X', y='log(Y)', label='log(Y)', alpha=0.5);
axs = sns.lineplot(data=data, x='X', y='log(X)', label='log(X)', alpha=0.5);
axs.legend([],[], frameon=False)
axs.set(title="Note that the order of the points is preserved between $e^X$, $X$ and $log(X)$");
plt.ylim(0, 25);
plt.xlim(0, 25);


In the context of data science, this is useful because in many cases we are interested in comparing the relative size of things rather than the absolute size of things. For instance, often we are interested in answering if 
𝑃
(
𝑋
)
>
𝑃
(
𝑋
)
.

With logarithms, we can answer this question by comparing 
𝑙
𝑜
𝑔
(
𝑃
(
𝑋
)
)
 and 
𝑙
𝑜
𝑔
(
𝑃
(
𝑋
)
)
 instead.

df = pd.DataFrame()
df['X'] = range(1, 11)
df['Y'] = range(11, 21)

df['X'].apply(log) < df['Y'].apply(log)

0    True
1    True
2    True
3    True
4    True
5    True
6    True
7    True
8    True
9    True
dtype: bool

5.3.2. Laws of Logarithms

The three laws of logarithms are:

Product rule:

𝑙
𝑜
𝑔
𝑏
(
𝑥
⋅
𝑦
)
=
𝑙
𝑜
𝑔
𝑏
𝑥
+
𝑙
𝑜
𝑔
𝑏
𝑦
log(4 * 2) == log(4) + log(2)

True


Quotient rule:

𝑙
𝑜
𝑔
𝑏
(
𝑥
𝑦
)
=
𝑙
𝑜
𝑔
𝑏
𝑥
−
𝑙
𝑜
𝑔
𝑏
𝑦
log(4 / 2) == log(4) - log(2)

True


Power rule:

𝑙
𝑜
𝑔
𝑏
(
𝑥
𝑟
)
=
𝑟
⋅
𝑙
𝑜
𝑔
𝑏
𝑥
log(4**2) == 2 * log(4)

True

5.3.3. Multiplying Probabilities

Logarithms were first invented as an aide to computation, by reducing the problem of multiplication to that of addition.

In particular, to compute the product 
𝑝
=
𝑥
⋅
𝑦
, we could compute the sum of the logarithms 
𝑠
=
𝑙
𝑜
𝑔
𝑏
𝑥
+
𝑙
𝑜
𝑔
𝑏
𝑦
 and then take the inverse of the logarithm (i.e. raising 
𝑏
 to the 
𝑠
𝑡
ℎ
 power) to get 
𝑝
, because:

𝑝
=
𝑥
⋅
𝑦
=
𝑏
(
𝑙
𝑜
𝑔
𝑏
𝑥
+
𝑙
𝑜
𝑔
𝑏
𝑦
)

This is the trick that powered the mechanical slide rules that geeks used in the days before pocket calculators.

However, this idea remains important today, particularly when multiplying long chains of probabilities. Probabilities are small numbers. Thus multiplying long chains of probability yield very small numbers that govern the chances of very rare events. There are serious numerical stability problems with floating point multiplication on real computers. Numerical errors will creep in, and will eventually overwhelm the true value of small-enough numbers.

Summing the logarithms of probabilities is much more numerically stable than multiplying them, but yields an equivalent result because:





∏
𝑖
=
1
𝑛
𝑝
𝑖
=
𝑏
𝑃
,
where
 
 
𝑃
=
∑
𝑖
=
1
𝑛
𝑙
𝑜
𝑔
𝑏
(
𝑝
𝑖
)

We can raise our sum to an exponential if we need the real probability, but usually this is not necessary.

When we just need to compare two probabilities to decide which one is larger we can safely stay in log world, because bigger logarithms correspond to bigger probabilities.

Note

Note that the 
𝑙
𝑜
𝑔
(
𝑃
(
𝑋
=
𝑥
)
)
 are all negative numbers for the range 
𝑥
∈
(
0
,
1
)
, 0 and 1 exclusive, since log(1) = 0 and log(0) is undefined.

Equations with logs of probabilities involve negative numbers which might look odd. Don’t let that throw you off.

5.3.4. Ratios of Probabilities

Ratios are quantities of the form 
𝑎
𝑏
. They occur often in data sets either as elementary features or values derived from feature pairs. Ratios naturally occur in normalizing data for conditions (i.e. weight after some treatment over the initial weight) or time (i.e. today’s price over yesterday’s price).

But ratios behave differently when reflecting increases than decreases. The ratio 
200
/
100
 is 
200
 above baseline, but 
100
/
200
 is only 50% below despite being a similar magnitude change. Thus doing things like averaging ratios is committing a statistical sin. Do you really want a doubling followed by a halving to average out as an increase, as opposed to a neutral change?

One solution here is taking the logarithm of these ratios, so that they yield equal displacement, since 
𝑙
𝑜
𝑔
2
(
2
)
=
1
 and 
𝑙
𝑜
𝑔
2
(
1
/
2
)
=
−
1
. We get the extra bonus that a unit ratio maps to zero, so positive and negative numbers correspond to improper and proper ratios, respectively.

Fig. 5.2 Plotting ratios on a scale cramps the space allocated to small ratios relative to large ratios (left). Plotting the logarithms of ratios better represents the underlying data (right).

A rookie mistake my students often make involves plotting the value of ratios instead of their logarithms. Figure above (left) is a graph from a student paper, showing the ratio of new score over old score on data over 24 hours (each red dot is the measurement for one hour) on four different data sets (each given a row). The solid black line shows the ratio of one, where both scores give the same result. Now try to read this graph: it isn’t easy because the points on the left side of the line are cramped together in a narrow strip. What jumps out at you are the outliers. Certainly the new algorithm does terrible on 7UM917 in the top row: that point all the way to the right is a real outlier.

Except that it isn’t. Now look at the panel on the right, where we plot the logarithms of the ratios. The space devoted to left and right of the black line can now be equal. And it shows that this point wasn’t really such an outlier at all. The magnitude of improvement of the leftmost points is much greater than that of the rightmost points. This plot reveals that new algorithm generally makes things better, only because we are showing logs of ratios instead of the ratios themselves.

Similar to the case of multiplying probabilities, ratio of probabilities can be computed by subtracting the logarithms of the probabilities:

𝑙
𝑜
𝑔
 
𝑏
(
𝑎
𝑏
)
=
𝑙
𝑜
𝑔
 
𝑏
(
𝑎
−
𝑏
)
5.3.5. Normalizing Skewed Distributions

Variables which follow symmetric, bell-shaped distributions tend to be nice as features in models. They show substantial variation, so they can be used to discriminate between things, but not over such a wide range that outliers are overwhelming.

But not every distribution is symmetric. Consider the one in Figure 2.13 (left). The tail on the right goes much further than the tail on the left. And we are destined to see far more lopsided distributions when we discuss power laws, in Section 5.1.5. Wealth is representative of such a distribution, where the poorest human has zero or perhaps negative wealth, the average person (optimistically) is in the thousands of dollars, and Bill Gates is pushing $100 billion as of this writing.

Fig. 5.3 Hitting a skewed data distribution (left) with a log often yields a more bell-shaped distribution (right).

We need a normalization to convert such distributions into something easier to deal with. To ring the bell of a power law distribution we need something non-linear, that reduces large values to a disproportionate degree compared to more modest values.

The logarithm is the transformation of choice for power law variables. Hit your long-tailed distribution with a log and often good things happen. The distribution in Figure 2.13 happened to be the log normal distribution, so taking the logarithm yielded a perfect bell-curve on right. Taking the logarithm of variables with a power law distribution brings them more in line with traditional distributions. For example, as an upper-middle class professional, my wealth is roughly the same number of logs from my starving students as I am from Bill Gates!

Sometimes taking the logarithm proves too drastic a hit, and a less dramatic non-linear transformation like the square root works better to normalize a dis- tribution. The acid test is to plot a frequency distribution of the transformed values and see if it looks bell-shaped: grossly-symmetric, with a bulge in the middle. That is when you know you have the right function.

In matplotlib you can specify the use of log scale for an axis with plt.yscale('log') or plt.xscale('log').

Example code cell below. Note how the values on the y-axis are still evenly spaced but increase exponentially.

df = pd.DataFrame()
df['X'] = range(1, 101)
df['Y'] = df['X'].apply(lambda x: math.e ** x)

sns.lineplot(data=df, x='X', y='Y');
plt.yscale('log');


Similarly, you can specify the use of log scale for both axes with log_scale = [True, False] in most plotting functions in seaborn. The first value in the list is for the x-axis, the second for the y-axis. True means log scale, False means the default linear scale.

import math 
import pandas as pd
df = pd.DataFrame()
df['X'] = range(1, 20)
df['Y'] = df['X'].apply(lambda x: 2 ** x)

sns.histplot(data=df, x='Y');

ax = sns.histplot(data=df, x='Y', log_scale=[False, True]);
ax.set(title="Y-axis is log-scaled");
plt.figure();
ax = sns.histplot(data=df, x='Y', log_scale=[True, False]);
ax.set(title="X-axis is log-scaled");

 



URL: https://fahadsultan.com/csc272/2_maths/40_linearalgebra.html


6. Linear Algebra and Geometry

In Encoding and Representation, we discussed how all data, (regardless of its form, format or modality) needs to be converted into a matrix of numbers.

Linear Algebra is the mathematics of such matrices of numbers. It is a fundamental tool in many fields, including data science and machine learning. In this section, we will review some of the basic concepts of linear algebra.

Before we talk about matrices however, we need to talk about vectors. In order to talk about vectors, in turn, we need to talk about scalars.



URL: https://fahadsultan.com/csc272/2_maths/41_vectors.html


6.1. Vectors
6.1.1. Scalars

Most everyday mathematics consists of manipulating numbers one at a time. Formally, we call these values scalars.

For example, the temperature in Greenville is a balmy 
72
 degrees Fahrenheit. If you wanted to convert the temperature to Celsius you would evaluate the expression 
𝑐
=
5
9
(
𝑓
−
32
)
, setting 
𝑓
=
72
. In this equation, the values 
5
, 
9
, and 
32
 are constant scalars. The variables 
𝑐
 and 
𝑓
 in general represent unknown scalars.

We denote scalars by ordinary lower-cased letters (e.g. 
𝑥
, 
𝑦
, and 
𝑧
) and the space of all (continuous) real-valued scalars by 
𝑅
. The expression 
𝑥
∈
𝑅
 is a formal way to say that 
𝑥
 is a real-valued scalar. The symbol 
∈
 (pronounced “in”) denotes membership in a set. For example, 
𝑥
,
𝑦
∈
0
,
1
 indicates that 
𝑥
 and 
𝑦
 are variables that can only take on values of 
0
 or 
1
.

Scalars in Python are represented by numeric types such as int and float.

x = 3
y = 2

print("x+y:", x+y, "x-y:", x-y, "x*y:", x*y, "x/y:", x/y, "x**y:", x**y)

6.1.2. Vectors

For current purposes, you can think of a vector as a fixed-length array of scalars. As with their code counterparts, we call these scalars the elements of the vector (synonyms include entries and components).

We denote vectors by bold lowercase letters, (e.g., 
𝑥
, 
𝑦
, and 
𝑧
). The vector of all ones is denoted 
1
. The vector of all zeros is denoted 
0
.

The unit vector 
𝑒
𝑖
 is a vector of all 0’s, except entry 
𝑖
, which has value 1:

𝑒
𝑖
=
(
0
,
…
0
,
1
,
0
,
…
0
)

This is also called a one-hot vector.

We can refer to an element of a vector by using a subscript. For example, 
𝑥
2
 denotes the second element of 
𝑥
. Since 
𝑥
2
 is a scalar, we do not bold it. By default, we visualize vectors by stacking their elements vertically.

0-based indexing vs. 1-based indexing

In Python, as in most programming languages, vector indices start at 
0
. This is known as zero-based indexing.

In linear algebra, however, subscripts begin at 
1
 (one-based indexing).

A vector 
𝑥
∈
𝑅
𝑛
 is a list of 
𝑛
 numbers, usually written as a column vector





𝑥
=
[
𝑥
1


𝑥
2


⋮


𝑥
𝑛
]

Here 
𝑥
1
…
𝑥
𝑛
 are elements of the vector. Later on, we will distinguish between such column vectors and row vectors whose elements are stacked horizontally.

Vectors are implemented in Python as list or tuples. In pandas, we have for vectors pd.Series which additionally has labels for each value. In general, such pd.Series can have arbitrary lengths, subject to memory limitations.

import pandas as pd 
x = pd.Series(range(10, 100, 10))

x

0    10
1    20
2    30
3    40
4    50
5    60
6    70
7    80
8    90
dtype: int64


Fundamentally, a vector is a list of numbers such as the Python list below.

v = [1, 7, 0, 1]


Mathematicians most often write this as either a column or row vector, which is to say either as





𝑥
=
[
1


7


0


1
]

or

			
𝑥
⊤
=
[
1
	
7
	
0
	
1
]
.
6.1.2.1. Geometry of Vectors

First, we need to discuss the two common geometric interpretations of vectors, as either points or directions in space.

Given a vector, the first interpretation that we should give it is as a point in space. In two or three dimensions, we can visualize these points by using the components of the vectors to define the location of the points in space compared to a fixed reference called the origin. This can be seen in the figure below.

Fig. 6.1 An illustration of visualizing vectors as points in the plane. The first component of the vector gives the 
𝑥
-coordinate, the second component gives the 
𝑦
-coordinate. Higher dimensions are analogous, although much harder to visualize.

This geometric point of view allows us to consider the problem on a more abstract level. No longer faced with some insurmountable seeming problem like classifying pictures as either cats or dogs, we can start considering tasks abstractly as collections of points in space and picturing the task as discovering how to separate two distinct clusters of points.

import pandas as pd 
from matplotlib import pyplot as plt

plt.xlim(-3, 3)
plt.ylim(-3, 3)

vector1 = [1, 2]
vector2 = [2, -1]

displacement = 0.1

# Plotting vector 1
plt.scatter(x=vector1[0], y=vector1[1], color='navy');
plt.text(x=vector1[0]+displacement, y=vector1[1], \
             s=f"(%s, %s)" % (vector1[0], vector1[1]), size=15);

# Plotting vector 2
plt.scatter(x=vector2[0], y=vector2[1], color='magenta');
plt.text(x=vector2[0]+displacement, y=vector2[1], \
             s=f"(%s, %s)" % (vector2[0], vector2[1]), size=15);

# Plotting the x and y axes
plt.axhline(0, color='black');
plt.axvline(0, color='black');

# Plotting the legend
plt.legend(['vector1', 'vector2'], loc='upper left');


In parallel, there is a second point of view that people often take of vectors: as directions in space. Not only can we think of the vector 
v
=
[
3
,
2
]
𝑇
 as the location 
3
 units to the right and 
2
 units up from the origin, we can also think of it as the direction itself to take 
3
 steps to the right and 
2
 steps up. In this way, we consider all the vectors in figure below the same.

Fig. 6.2 Any vector can be visualized as an arrow in the plane. In this case, every vector drawn is a representation of the vector 
(
3
,
2
)
⊤

plt.xlim(-3, 3)
plt.ylim(-3, 3)

# Plotting vector 1
plt.quiver(0, 0, vector1[0], vector1[1], scale=1, scale_units='xy', angles='xy', color='navy')
plt.text(x=vector1[0]+displacement, y=vector1[1], \
             s=f"(%s, %s)" % (vector1[0], vector1[1]), size=20);

# Plotting vector 2
plt.quiver(0, 0, vector2[0], vector2[1], scale=1, scale_units='xy', angles='xy', color='violet')
plt.text(x=vector2[0]+displacement, y=vector2[1], \
             s=f"(%s, %s)" % (vector2[0], vector2[1]), size=20);

plt.legend(['vector1', 'vector2'], loc='upper left');

# Plotting the x and y axes
plt.axhline(0, color='black');
plt.axvline(0, color='black');


One of the benefits of this shift is that we can make visual sense of the act of vector addition. In particular, we follow the directions given by one vector, and then follow the directions given by the other, as seen below:

Fig. 6.3 We can visualize vector addition by first following one vector, and then another.

Vector subtraction has a similar interpretation. By considering the identity that 
𝑢
=
𝑣
+
(
𝑢
−
𝑣
)
, we see that the vector 
𝑢
−
𝑣
 is the direction that takes us from the point 
𝑣
 to the point 
𝑢
.

vector1 = df[['x', 'y']].iloc[0]
vector2 = df[['x', 'y']].iloc[1]

sum = vector1 + vector2

sum = pd.Series(vector1) + pd.Series(vector2)

sum

0    4
1    6
dtype: int64

vector1 = pd.Series([1, 2])
vector2 = pd.Series([2, -1])
sum = vector1 + vector2

plt.xlim(-3, 3)
plt.ylim(-3, 3)

# Plotting vector 1
plt.quiver(0, 0, vector1[0], vector1[1], scale=1, scale_units='xy', angles='xy', color='navy')
plt.text(x=vector1[0]+displacement, y=vector1[1], \
             s=f"(%s, %s)" % (vector1[0], vector1[1]), size=20);

# Plotting vector 2
plt.quiver(vector1[0], vector1[1], vector2[0], vector2[1], scale=1, scale_units='xy', angles='xy', color='magenta')
plt.text(x=vector2[0]+displacement, y=vector2[1], \
             s=f"(%s, %s)" % (vector2[0], vector2[1]), size=20);


plt.quiver(0, 0, sum[0], sum[1], scale=1, scale_units='xy', angles='xy', color='lime')
plt.text(x=sum[0]+displacement, y=sum[1], \
             s=f"(%s, %s)" % (sum[0], sum[1]), size=20);

plt.legend(['vector1', 'vector2', 'sum'], loc='upper left');

# Plotting the x and y axes
plt.axhline(0, color='black');
plt.axvline(0, color='black');

6.1.2.2. Norms

Some of the most useful operators in linear algebra are norms. A norm is a function 
‖
⋅
‖
 that maps a vector to a scalar.

Informally, the norm of a vector tells us magnitude or length of the vector.

For instance, the 
𝑙
2
 norm measures the euclidean length of a vector. That is, 
𝑙
2
 norm measures the euclidean distance of a vector from the origin 
(
0
,
0
)
.



‖
𝑥
‖
2
=
∑
𝑖
=
1
𝑛
𝑥
𝑖
2
x = pd.Series(vector1)
l2_norm = (x**2).sum()**(1/2)
l2_norm

2.23606797749979


The 
𝑙
1
 norm is also common and the associated measure is called the Manhattan distance. By definition, the 
𝑙
1
 norm sums the absolute values of a vector’s elements:



‖
𝑥
‖
1
=
∑
𝑖
=
1
𝑛
|
𝑥
𝑖
|

Compared to the 
𝑙
2
 norm, it is less sensitive to outliers. To compute the 
𝑙
1
 norm, we compose the absolute value with the sum operation.

l1_norm = x.abs().sum()
l1_norm

6


Both the 
𝑙
1
 and 
𝑙
2
 norms are special cases of the more general norms:



‖
𝑥
‖
𝑝
=
(
∑
𝑖
=
1
𝑛
|
𝑥
𝑖
|
𝑝
)
1
/
𝑝
.
vec = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9])

p = 3

lp_norm = ((abs(vec))**p).sum()**(1/p)

lp_norm

12.651489979526238

6.1.2.3. Dot Product

One of the most fundamental operations in linear algebra (and all of data science and machine learning) is the dot product.

Given two vectors 
x
,
y
∈
𝑅
𝑑
, their dot product 
x
⊤
y
 (also known as inner product 
⟨
x
,
y
⟩
) is a sum over the products of the elements at the same position:



x
⊤
y
=
∑
𝑖
=
1
𝑑
𝑥
𝑖
𝑦
𝑖
import pandas as pd

x = pd.Series([1, 2, 3])
y = pd.Series([4, 5, 6])

x.dot(y) # 1*4 + 2*5 + 3*6 

32


Equivalently, we can calculate the dot product of two vectors by performing an elementwise multiplication followed by a sum:

sum(x * y)

32


Dot products are useful in a wide range of contexts. For example, given some set of values, denoted by a vector 
𝑥
∈
𝑅
𝑛
 , and a set of weights, denoted by 
𝑥
∈
𝑅
𝑛
, the weighted sum of the values in 
𝑥
 according to the weights 
𝑤
 could be expressed as the dot product 
𝑥
⊤
𝑤
. When the weights are nonnegative and sum to 
1
, i.e., 
(
∑
𝑖
=
1
𝑛
𝑤
𝑖
=
1
)
, the dot product expresses a weighted average. After normalizing two vectors to have unit length, the dot products express the cosine of the angle between them. Later in this section, we will formally introduce this notion of length.

6.1.2.4. Dot Products and Angles

If we take two column vectors 
𝑢
 and 
𝑣
, we can form their dot product by computing:



𝑢
⊤
𝑣
=
∑
𝑖
𝑢
𝑖
⋅
𝑣
𝑖

Because the equation above is symmetric, we will mirror the notation of classical multiplication and write

𝑢
⋅
𝑣
=
𝑢
⊤
𝑣
=
𝑣
⊤
𝑢
,

to highlight the fact that exchanging the order of the vectors will yield the same answer.

The dot product also admits a geometric interpretation: dot product it is closely related to the angle between two vectors.

Fig. 6.4 Between any two vectors in the plane there is a well defined angle 
𝜃
. We will see this angle is intimately tied to the dot product.

To start, let’s consider two specific vectors:

𝑣
=
(
𝑟
,
0
)
and
𝑤
=
(
𝑠
cos
⁡
(
𝜃
)
,
𝑠
sin
⁡
(
𝜃
)
)

The vector 
𝑣
 is length 
𝑟
 and runs parallel to the 
𝑥
-axis, and the vector 
𝑤
 is of length 
𝑠
 and at angle 
𝜃
 with the 
𝑥
-axis.

If we compute the dot product of these two vectors, we see that

𝑣
⋅
𝑤
=
𝑟
𝑠
cos
⁡
(
𝜃
)
=
‖
𝑣
‖
‖
𝑤
‖
cos
⁡
(
𝜃
)

With some simple algebraic manipulation, we can rearrange terms to obtain the equation for any two vectors 
𝑣
 and 
𝑤
:

𝜃
=
arccos
⁡
(
𝑣
⋅
𝑤
‖
𝑣
‖
‖
𝑤
‖
)

We will not use it right now, but it is useful to know that we will refer to vectors for which the angle is 
𝜋
/
2
(or equivalently 
90
∘
) as being orthogonal.

By examining the equation above, we see that this happens when 
𝜃
=
𝜋
/
2
, which is the same thing as 
𝑐
𝑜
𝑠
(
𝜃
)
=
0
.

The only way this can happen is if the dot product itself is zero, and two vectors are orthogonal if and only if 
𝑣
⋅
𝑤
=
0
.

This will prove to be a helpful formula when understanding objects geometrically.

It is reasonable to ask: why is computing the angle useful? Consider the problem of classifying text data. We might want the topic or sentiment in the text to not change if we write twice as long of document that says the same thing.

For some encoding (such as counting the number of occurrences of words in some vocabulary), this corresponds to a doubling of the vector encoding the document, so again we can use the angle.

v = pd.Series([0, 2])
w = pd.Series([2, 0])

v.dot(w)

0

from math import acos

def l2_norm(vec):
    return (vec**2).sum()**(1/2)

v = pd.Series([0, 2])
w = pd.Series([2, 0])

v.dot(w) / (l2_norm(v) * l2_norm(w))

0.0

from math import acos, pi

theta = acos(v.dot(w) / (l2_norm(v) * l2_norm(w)))

theta == pi / 2

True

6.1.2.5. Cosine Similarity/Distance

In ML contexts where the angle is employed to measure the closeness of two vectors, practitioners adopt the term cosine similarity to refer to the portion

cos
⁡
(
𝜃
)
=
𝑣
⋅
𝑤
‖
𝑣
‖
‖
𝑤
‖
.

The cosine takes a maximum value of 
1
 when the two vectors point in the same direction, a minimum value of 
−
1
 when they point in opposite directions, and a value of 
0
 when the two vectors are orthogonal.

Note that cosine similarity can be converted to cosine distance by subtracting it from 
1
 and dividing by 2.

Cosine Distance
=
1
−
Cosine Similarity
2

where 
Cosine Similarity
=
𝑣
⋅
𝑤
‖
𝑣
‖
‖
𝑤
‖

Cosine distance is a very useful alternative to Euclidean distance for data where the absolute magnitude of the features is not particularly meaningful, which is a very common scenario in practice.

from random import uniform
import pandas as pd
import seaborn as sns

df = pd.DataFrame()
df['cosine similarity'] = pd.Series([uniform(-1, 1) for i in range(100)])
df['cosine distance']   = (1 - df['cosine similarity'])/2
ax = sns.scatterplot(data=df, x='cosine similarity', y='cosine distance');
ax.set(title='Cosine Similarity vs. Cosine Distance')
plt.grid()

def l2_norm(vec):
    return (vec**2).sum()**(1/2)

plt.axhline(0, color='black');
plt.axvline(0, color='black');

v = pd.Series([1.2, 1.2])
w = pd.Series([2, 2.5])

plt.quiver(0, 0, v[0], v[1], scale=1, scale_units='xy', angles='xy', color='navy')
plt.quiver(0, 0, w[0], w[1], scale=1, scale_units='xy', angles='xy', color='magenta')

plt.xlim(-3, 3)
plt.ylim(-3, 3)

cosine_similarity = v.dot(w) / (l2_norm(v) * l2_norm(w))
cosine_similarity = round(cosine_similarity, 2)

cosine_distance = (1 - cosine_similarity) / 2
cosine_distance = round(cosine_distance, 2)

plt.title("θ ≈ 0° (or 0 radians)\n"+\
          "Cosine Similarity: %s \nCosine Distance: %s" % \
          (cosine_similarity, cosine_distance), size=15);

plt.axhline(0, color='black');
plt.axvline(0, color='black');

v = pd.Series([2, 2])
w = pd.Series([1, -1])

plt.quiver(0, 0, v[0], v[1], scale=1, scale_units='xy', angles='xy', color='navy')
plt.quiver(0, 0, w[0], w[1], scale=1, scale_units='xy', angles='xy', color='magenta')

plt.xlim(-3, 3)
plt.ylim(-3, 3)

cosine_similarity = v.dot(w) / (l2_norm(v) * l2_norm(w))
cosine_similarity = round(cosine_similarity, 2)


cosine_distance = (1 - cosine_similarity) / 2
cosine_distance = round(cosine_distance, 2)

plt.title("θ = 90° (or π / 2 radians)"+\
          "\nCosine Similarity: %s "+\
          "\nCosine Distance: %s" % \
          (cosine_similarity, cosine_distance), size=15);


Note that cosine similarity can be negative, which means that the angle is greater than 
90
∘
, i.e., the vectors point in opposite directions.

v = pd.Series([2, 2])
w = pd.Series([-1, -1])

plt.quiver(0, 0, v[0], v[1], scale=1, scale_units='xy', angles='xy', color='navy')
plt.quiver(0, 0, w[0], w[1], scale=1, scale_units='xy', angles='xy', color='magenta')

plt.xlim(-3, 3)
plt.ylim(-3, 3)

plt.axhline(0, color='black');
plt.axvline(0, color='black');

cosine_similarity = v.dot(w) / (l2_norm(v) * l2_norm(w))

cosine_similarity = round(cosine_similarity, 2)

cosine_distance = (1 - cosine_similarity) / 2
cosine_distance = round(cosine_distance, 2)

plt.title("θ = 180° (or π radians)\n"+\
          "Cosine Similarity: %s \nCosine Distance: %s" % \
          (cosine_similarity, cosine_distance), size=15);

6.1.3. Hyperplanes

In addition to working with vectors, another key object that you must understand to go far in linear algebra is the hyperplane, a generalization to higher dimensions of a line (two dimensions) or of a plane (three dimensions). In an -dimensional vector space, a hyperplane has dimensions and divides the space into two half-spaces.

Let’s start with an example. Suppose that we have a column vector 
𝑤
=
[
2
,
1
]
⊤
. We want to know, “what are the points 
𝑣
 with 
𝑤
⋅
𝑣
=
1
?” We can define 
𝑣
=
[
𝑥
,
𝑦
]

𝑤
⋅
𝑣
=
1
[
2
,
1
]
⊤
⋅
[
𝑥
,
𝑦
]
=
1
2
𝑥
+
𝑦
=
1
𝑦
=
−
2
𝑥
+
1

Recall that the equation for a line is 
𝑦
=
𝑚
𝑥
+
𝑏
. Therefore, in the equations above, we have defined a line where the slope (
𝑚
) is 
−
2
 and the intercept (
𝑏
) is 
1
.

In this way, we have found a way to cut our space into two halves, where all the points on one side have dot product below a threshold, and the other side above as we see below:

Fig. 6.5 If we now consider the inequality version of the expression, we see that our hyperplane (in this case: just a line) separates the space into two halves.

The story in higher dimension is much the same. If we now take 
𝑤
=
[
1
,
2
,
3
]
⊤
 and ask about the points in three dimensions with 
𝑤
⋅
𝑣
=
1
, we obtain a plane at right angles to the given vector 
𝑤
. The two inequalities again define the two sides of the plane as is shown below:

Fig. 6.6 Hyperplanes in any dimension separate the space into two halves.

While our ability to visualize runs out at this point, nothing stops us from doing this in tens, hundreds, or billions of dimensions. This occurs often when thinking about machine learned models.

For instance, we can understand linear classification models, as methods to find hyperplanes that separate the different target classes. In this context, such hyperplanes are often referred to as decision planes. The majority of deep learned classification models end with a linear layer fed into a softmax, so one can interpret the role of the deep neural network to be to find a non-linear embedding such that the target classes can be separated cleanly by hyperplanes.



URL: https://fahadsultan.com/csc272/2_maths/42_matrices.html


6.2. Matrices

We denote matrices by bold capital letters (e.g., 
𝑋
, 
𝑌
, and 
𝑍
), and represent them in code by pd.DataFrame. The expression 
𝐴
∈
𝑅
𝑚
×
𝑛
 indicates that a matrix 
A
 contains 
𝑚
×
𝑛
 real-valued scalars, arranged as 
𝑚
 rows and 
𝑛
 columns. When 
𝑚
=
𝑛
, we say that a matrix is square. Visually, we can illustrate any matrix as a table. To refer to an individual element, we subscript both the row and column indices, e.g., 
𝑎
𝑖
𝑗
 is the value that belongs to 
𝐴
’s 
𝑖
𝑡
ℎ
 row and 
𝑗
𝑡
ℎ
 column:

			
			
			
			
𝐴
=
[
𝑎
11
	
𝑎
12
	
⋯
	
𝑎
1
𝑛


𝑎
21
	
𝑎
22
	
⋯
	
𝑎
2
𝑛


⋮
	
⋮
	
⋱
	
⋮


𝑎
𝑚
1
	
𝑎
𝑚
2
	
⋯
	
𝑎
𝑚
𝑛
]
.
import pandas as pd 

df = pd.DataFrame({'a': [1, 20, 3, 40], 'b': [50, 6, 70, 8]})

df.index = ['v1', 'v2', 'v3', 'v4']

df

	a	b
v1	1	50
v2	20	6
v3	3	70
v4	40	8
import seaborn as sns
from matplotlib import pyplot as plt

ax = sns.scatterplot(x='a', y='b', data=df, s=100);
ax.set(title='Scatterplot of a vs b', xlabel='a', ylabel='b');

def annotate(row):
    plt.text(x=row['a']+0.05, y=row['b'], s=row.name, size=20);

df.apply(annotate, axis=1);

6.2.1. Transpose

Sometimes we want to flip the axes. When we exchange a matrix’s rows and columns, the result is called its transpose. Formally, we signify a matrix’s 
A
 transpose by 
𝐴
⊤
 and if 
𝐵
=
𝐴
⊤
, then 
𝑏
𝑖
𝑗
=
𝑎
𝑖
𝑗
 for all 
𝑖
 and 
𝑗
. Thus, the transpose of an 
𝑚
×
𝑛
 matrix is an 
𝑛
×
𝑚
 matrix:

			
			
			
			
𝐴
⊤
=
[
𝑎
11
	
𝑎
21
	
…
	
𝑎
𝑚
1


𝑎
12
	
𝑎
22
	
…
	
𝑎
𝑚
2


⋮
	
⋮
	
⋱
	
⋮


𝑎
1
𝑛
	
𝑎
2
𝑛
	
…
	
𝑎
𝑚
𝑛
]
.

In pandas, you can transpose a DataFrame with the .T attribute:

df.T

	0	1	2
a	1	2	3
b	4	5	6
c	7	8	9

Note that columns of original dataframe df are the same as index of df.T

df.columns == df.T.index

array([ True,  True,  True])

df.T

	0	1	2	3
a	1	20	3	40
b	50	6	70	8
6.2.2. Matrix-Vector Products

Now that we know how to calculate dot products, we can begin to understand the product between an 
𝑚
×
𝑛
 matrix and an 
𝑛
-dimensional vector 
𝑥
.

To start off, we visualize our matrix in terms of its row vectors






𝐴
=
[
𝑎
1
⊤


𝑎
2
⊤


⋮


𝑎
𝑚
⊤
]
,

where each 
𝑎
𝑖
⊤
∈
𝑅
𝑛
 is a row vector representing the 
𝑖
th
 row of the matrix 
𝐴
.

The matrix–vector product 
𝐴
𝑥
 is simply a column vector of length 
𝑚
 , whose 
𝑖
𝑡
ℎ
 element is the dot product 
𝑎
𝑖
⊤
𝑥











𝐴
𝑥
=
[
𝑎
1
⊤


𝑎
2
⊤


⋮


𝑎
𝑚
⊤
]
𝑥
=
[
𝑎
1
⊤
𝑥


𝑎
2
⊤
𝑥


⋮


𝑎
𝑚
⊤
𝑥
]
.

We can think of multiplication with a matrix 
𝐴
∈
𝑅
𝑚
×
𝑛
 as a transformation that projects vectors from 
𝑅
𝑛
 to 
𝑅
𝑚
.

These transformations are remarkably useful. For example, we can represent rotations as multiplications by certain square matrices. Matrix–vector products also describe the key calculation involved in computing the outputs of each layer in a neural network given the outputs from the previous layer.

6.2.2.1. Finding similar vectors

Note that, given a vector 
𝑣
, vector-matrix products can also be used to compute the similarity of 
𝑣
 and each row 
𝑎
𝑖
⊤
 of matrix 
𝐴
. This is because the matrix-vector product 
𝐴
𝑣
 will contain the dot products of 
𝑣
 and each row in 
𝐴
.

There is one thing to be careful about: recall that the formula for cosine similarity is:

cos
(
𝜃
)
=
𝑎
⋅
𝑏
‖
𝑎
‖
‖
𝑏
‖

The dot product (numerator, on the right hand side) is equal to the cosine of the angle between the two vectors when the vectors are normalized (i.e. each divided by their norms).

The example below shows how to compute the cosine similarity between a vector 
𝑣
 and each row of matrix 
𝐴
.

import pandas as pd
data = pd.read_csv('https://raw.githubusercontent.com/fahadsultan/csc272/main/data/chat_dataset.csv')

# creating bow representation
vocab = (' '.join(data['message'].values)).lower().split()
bow = pd.DataFrame(columns=vocab)
for word in vocab: 
    bow[word] = data['message'].apply(lambda msg: msg.count(word))

# l2 norm of a vector
def l2_norm(vec):
    return (sum(vec**2))**(1/2)

# bow where each row is a unit vectors i.e. ||row|| = 1
bow_unit = bow.apply(lambda row: row/l2_norm(row), axis=1)

# random message : I don't have an opinion on this
msg = bow_unit.iloc[20] 

# cosine similarity of first message with all other messages
msg_sim = bow_unit.dot(msg.T)
msg_sim.index = data['message']

msg_sim.sort_values(ascending=False)

message
I don't have an opinion on this           1.000000
I don't really have an opinion on this    0.984003
I have no strong opinion about this       0.971575
I have no strong opinions about this      0.971226
I have no strong opinion on this          0.969768
                                            ...   
I'm not sure what to do 😕                 0.204587
I'm not sure what to do next 🤷‍♂️         0.201635
I'm not sure what to do next 🤔            0.200593
The food was not good                     0.200295
The food was not very good                0.197220
Length: 584, dtype: float64

6.2.3. Matrix-Matrix Multiplication

Once you have gotten the hang of dot products and matrix–vector products, then matrix–matrix multiplication should be straightforward.

Say that we have two matrices 
𝐴
∈
𝑅
𝑛
×
𝑘
 and 
𝐵
∈
𝑅
𝑘
×
𝑚
:

			
			
			
			
			
			
			
			
𝐴
=
[
𝑎
11
	
𝑎
12
	
⋯
	
𝑎
1
𝑘


𝑎
21
	
𝑎
22
	
⋯
	
𝑎
2
𝑘


⋮
	
⋮
	
⋱
	
⋮


𝑎
𝑛
1
	
𝑎
𝑛
2
	
⋯
	
𝑎
𝑛
𝑘
]
,
𝐵
=
[
𝑏
11
	
𝑏
12
	
⋯
	
𝑏
1
𝑚


𝑏
21
	
𝑏
22
	
⋯
	
𝑏
2
𝑚


⋮
	
⋮
	
⋱
	
⋮


𝑏
𝑘
1
	
𝑏
𝑘
2
	
⋯
	
𝑏
𝑘
𝑚
]
.

Let 
𝑎
𝑖
⊤
∈
𝑅
𝑘
 denote the row vector representing the 
𝑖
th
 row of the matrix 
𝐴
 and let 
𝑏
𝑗
∈
𝑅
𝑘
 denote the column vector from the 
𝑗
th
 column of the matrix 
𝐵
 :






			
𝐴
=
[
𝑎
1
⊤


𝑎
2
⊤


⋮


𝑎
𝑛
⊤
]
,
𝐵
=
[
𝑏
1
	
𝑏
2
	
⋯
	
𝑏
𝑚
]
.

To form the matrix product 
𝐶
∈
𝑅
𝑛
×
𝑚
 , we simply compute each element 
𝑐
𝑖
𝑗
 as the dot product between the 
𝑖
th
 row of 
𝐴
 and the 
𝑖
th
 column of 
𝐵
 , i.e., 
𝑎
𝑖
⊤
𝑏
𝑗
 :






			
	
		


	
		

			

	
		
𝐶
=
AB
=
[
𝑎
1
⊤


𝑎
2
⊤


⋮


𝑎
𝑛
⊤
]
[
𝑏
1
	
𝑏
2
	
⋯
	
𝑏
𝑚
]
=
[
𝑎
1
⊤
𝑏
1
	
𝑎
1
⊤
𝑏
2
	
⋯
	
𝑎
1
⊤
𝑏
𝑚


𝑎
2
⊤
𝑏
1
	
𝑎
2
⊤
𝑏
2
	
⋯
	
𝑎
2
⊤
𝑏
𝑚


⋮
	
⋮
	
⋱
	
⋮


𝑎
𝑛
⊤
𝑏
1
	
𝑎
𝑛
⊤
𝑏
2
	
⋯
	
𝑎
𝑛
⊤
𝑏
𝑚
]
.

We can think of the matrix–matrix multiplication 
AB
 as performing 
𝑚
 matrix–vector products or 
𝑚
×
𝑛
 dot products and stitching the results together to form an 
𝑚
×
𝑛
 matrix. In the following snippet, we perform matrix multiplication on A and B. Here, A is a matrix with two rows and three columns, and B is a matrix with three rows and four columns. After multiplication, we obtain a matrix with two rows and four columns.

6.2.3.1. Computing similarity / distance matrix

Note that, given two matrices 
𝐴
 and 
𝐵
, matrix-matrix products can also be used to compute the similarity between each row 
𝑎
𝑖
⊤
 of matrix 
𝐴
 and each row 
𝑏
𝑗
⊤
 of matrix 
𝐵
. This is because the matrix-matrix product 
𝐴
𝐵
 will contain the dot products of each pair of rows in 
𝐴
 and 
𝐵
.

similarity_matrix = bow_unit.dot(bow_unit.T)

similarity_matrix.shape

similarity_matrix.head()

	0	1	2	3	4	5	6	7	8	9	...	574	575	576	577	578	579	580	581	582	583
0	1.000000	0.568191	0.453216	0.565809	0.612787	0.608156	0.377769	0.631309	0.793969	0.642410	...	0.516106	0.612624	0.660911	0.490580	0.845518	0.573549	0.648564	0.629598	0.594468	0.570452
1	0.568191	1.000000	0.508206	0.928580	0.687137	0.681944	0.423604	0.707906	0.551139	0.720354	...	0.573755	0.686954	0.741100	0.546166	0.801864	0.643139	0.721001	0.705988	0.652357	0.639666
2	0.453216	0.508206	1.000000	0.506075	0.548093	0.707857	0.778316	0.621438	0.538883	0.648861	...	0.457088	0.547948	0.650512	0.443105	0.593732	0.512998	0.574392	0.605515	0.520351	0.609064
3	0.565809	0.928580	0.506075	1.000000	0.685614	0.679085	0.421828	0.704938	0.548828	0.717334	...	0.571349	0.684074	0.737993	0.543876	0.798502	0.640442	0.717978	0.703027	0.649622	0.638039
4	0.612787	0.687137	0.548093	0.685614	1.000000	0.735468	0.273748	0.559585	0.714100	0.599091	...	0.926494	0.825530	0.806139	0.933257	0.802775	0.945544	0.931874	0.913601	0.928473	0.690556

5 rows × 584 columns

from matplotlib import pyplot as plt
plt.imshow(similarity_matrix, cmap='Greens')
plt.colorbar();
plt.title("Similarity Matrix: Each cell contains cosine \nsimilarity between two messages");
plt.xlabel("Message Index");
plt.ylabel("Message Index");


Note how 1. the similarity matrix is symmetric, i.e. 
𝑠
𝑖
𝑚
𝑖
𝑗
=
𝑠
𝑖
𝑚
𝑗
𝑖
 and 2. the diagonal elements are all 1, i.e. 
𝑠
𝑖
𝑚
𝑖
𝑖
=
1
.

distance_matrix = 1 - bow_unit.dot(bow_unit.T)

from matplotlib import pyplot as plt
plt.imshow(distance_matrix, cmap='Reds')
plt.colorbar();
plt.title("Distance Matrix: Each cell contains cosine \ndistance between two messages");
plt.xlabel("Message Index");
plt.ylabel("Message Index");




URL: https://fahadsultan.com/csc272/2_maths/43_applications.html


6.3. Applications
6.3.1. Dimensionality Reduction

Dimensionality reduction is a technique that is used to reduce the number of features in a dataset.

Reducing the number of features of a dataset is desirable for the following reasons:

It reduces the time and storage space required and subsequently reduces the computation time.

It removes redundant features and the overcome the curse of dimensionality.

Curse of dimensionality ☠️

The curse of dimensionality refers to the fact that for each additional feature, the number of training examples required to train the machine learning algorithm grows exponentially. This is because the volume of the space increases so fast that the available data become sparse.

It allows us to visualize high-dimensional data in a 2-dimensional or 3-dimensional space.

import pandas as pd
data = pd.read_csv('https://raw.githubusercontent.com/fahadsultan/csc272/main/data/chat_dataset.csv')
data.head()

	message	sentiment
0	I really enjoyed the movie	positive
1	The food was terrible	negative
2	I'm not sure how I feel about this	neutral
3	The service was excellent	positive
4	I had a bad experience	negative
# creating bow representation
vocab = (' '.join(data['message'].values)).lower().split()
bow = pd.DataFrame(columns=vocab)
for word in vocab: 
    bow[word] = data['message'].apply(lambda msg: msg.count(word))

def l2_norm(x):
    return (sum(x**2))**(1/2)

bow_unit = bow.apply(lambda x: x/l2_norm(x), axis=1)

from sklearn.decomposition import PCA

# n_components indicates how many dimensions
# you want your data to be reduced to
pca = PCA(n_components = 2)

bow_reduced = pca.fit_transform(bow)

bow_reduced = pd.DataFrame(bow_reduced)

bow_reduced.head()

	0	1
0	-13.183063	-13.360581
1	-14.616593	-13.413976
2	-11.650563	-15.537625
3	-14.605181	-13.347637
4	-16.310469	5.483213
from matplotlib import pyplot as plt 

labels = data['sentiment'].replace({'neutral':0, 'positive':1, 'negative':-1})

pos = bow_reduced[labels==1]
neg = bow_reduced[labels==-1]
neu = bow_reduced[labels==0]

plt.scatter(neu[0], neu[1], c='y', label='neutral');
plt.scatter(pos[0], pos[1], c='b', label='positive');
plt.scatter(neg[0], neg[1], c='r', label='negative');

plt.legend();

plt.title('PCA on BOW: Each point is a message');
plt.xlabel('PC1');
plt.ylabel('PC2');


It is important to point out that dimensionality reduction is not the same as feature selection. The main difference is that in dimensionality reduction, we transform the data in a lower dimensional space while in feature selection we select a subset of the original features. In other words, PC1 and PC2 are linear combinations of the original features, while the features selected in feature selection are the original features.

6.3.2. K-Nearest Neighbors (KNN)

K-Nearest Neighbors (KNN) is a supervised machine learning algorithm that can be used for both classification and regression problems. KNN is a non-parametric, lazy learning algorithm that classifies a data point based on the 
𝑘
 data points that are nearest to it. KNN does not make any assumptions on the underlying data distribution.

Note that nearest points (observations) can be found by multiplying the matrix representation of observations by its transpose. The resulting matrix contains the distances between all pairs of observations.

Once you have the similarity matrix, you can find the 
𝑘
 nearest neighbors of a particular observation by sorting the row of the similarity matrix corresponding to that observation.

Your prediction can then be mean or median of the target values of the 
𝑘
 nearest neighbors.

6.3.3. Recommender Systems

Recommender systems are a type of information filtering system that are used to predict the rating or preference that a user would give to an item. They are widely used in e-commerce, entertainment, and social media platforms. Recommender systems are of two types:

Nearest Neighbors (KNN) are often used to build recommender systems. Recommender systems are used to recommend items to users based on their past preferences.

6.3.3.1. Collaborative Filtering

Collaborative filtering is a technique that is used to filter out items that a user might like on the basis of reactions by similar users. It works by searching a large group of people and finding a smaller set of users with tastes similar to a particular user. It looks at the items they like and combines them to create a ranked list of suggestions.

Nearest Neighbors (KNN) are used to find the users that are similar to a particular user. The items that are liked by the similar users are then recommended to the particular user.

import pandas as pd

data = pd.read_csv('../data/bratings.csv', index_col=0)

data['title'] = data['Title'].apply(lambda x: x[:10]+"...")


For instance, in the data above, if we wanted to recommend a book to user JohnPal, we would just find the most similar user using Nearest Neighbor and recommend what the most similar user liked that JohnPal hasn’t read.

This would require re-formatting the data to a form where each row represents a user and each column is a book.

unique_titles = list(data['title'].unique())

def agg_user(grobj):
    user_titles = list(grobj['title'].unique())
    vec = pd.Series(0, index=unique_titles)
    vec.loc[user_titles] = 1
    return vec

data.groupby('profileName').apply(agg_user)

	Gods and K...	The Mayor ...	Blessings...	Stitch 'N ...	Why Men Lo...	Red Storm ...	Great Expe...	Sex, Drugs...	A Crown Of...	The Bread ...	...	Push: A No...	Tarzan of ...	Ultra Blac...	Stone of T...	The Truth ...	Left to Te...	Good to Gr...	Blue Like ...	Love & Res...	1491: New ...
profileName																					
! Metamorpho ;) "Reflective and Wiser Seer"	0	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0
"	0	0	0	0	0	0	0	0	0	0	...	0	0	1	0	0	0	0	0	0	0
"-thewarlock-"	0	0	0	0	0	0	0	0	0	0	...	0	0	0	1	0	0	0	0	0	0
"24heineck"	0	0	0	0	0	0	0	0	0	0	...	0	0	0	1	0	0	0	0	0	0
"350am"	0	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
~LEON~	0	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0
~Storm~	0	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0
~S~	0	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0
~Terry~	0	0	0	0	1	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0
~auntysue~	0	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0

42214 rows × 151 columns

6.3.3.2. Content Based Filtering

Content based filtering is a technique that is used to filter out items that a user might like on the basis of the description of the item itself. It works by creating a profile of the user’s interests based on the items that the user has liked in the past. It then recommends items that match the user’s profile.

Nearest Neighbors (KNN) are used to find the items that are similar to the items that a user has liked in the past. The similar items are then recommended to the user.

data = pd.read_csv('../data/imdb_top_1000.csv')
data.head()

	Poster_Link	Series_Title	Released_Year	Certificate	Runtime	Genre	IMDB_Rating	Overview	Meta_score	Director	Star1	Star2	Star3	Star4	No_of_Votes	Gross
0	https://m.media-amazon.com/images/M/MV5BMDFkYT...	The Shawshank Redemption	1994	A	142 min	Drama	9.3	Two imprisoned men bond over a number of years...	80.0	Frank Darabont	Tim Robbins	Morgan Freeman	Bob Gunton	William Sadler	2343110	28,341,469
1	https://m.media-amazon.com/images/M/MV5BM2MyNj...	The Godfather	1972	A	175 min	Crime, Drama	9.2	An organized crime dynasty's aging patriarch t...	100.0	Francis Ford Coppola	Marlon Brando	Al Pacino	James Caan	Diane Keaton	1620367	134,966,411
2	https://m.media-amazon.com/images/M/MV5BMTMxNT...	The Dark Knight	2008	UA	152 min	Action, Crime, Drama	9.0	When the menace known as the Joker wreaks havo...	84.0	Christopher Nolan	Christian Bale	Heath Ledger	Aaron Eckhart	Michael Caine	2303232	534,858,444
3	https://m.media-amazon.com/images/M/MV5BMWMwMG...	The Godfather: Part II	1974	A	202 min	Crime, Drama	9.0	The early life and career of Vito Corleone in ...	90.0	Francis Ford Coppola	Al Pacino	Robert De Niro	Robert Duvall	Diane Keaton	1129952	57,300,000
4	https://m.media-amazon.com/images/M/MV5BMWU4N2...	12 Angry Men	1957	U	96 min	Crime, Drama	9.0	A jury holdout attempts to prevent a miscarria...	96.0	Sidney Lumet	Henry Fonda	Lee J. Cobb	Martin Balsam	John Fiedler	689845	4,360,000

In the data above, for instance, if a user liked The Shawshank Redemption then you need to find the most-similar movie (not the user) using Nearest Neighbor and recommend that to the user.

6.3.4. Social Media Feeds

Social media platforms such as Facebook, Twitter, and Instagram use recommender systems to recommend posts to users. The posts that are recommended to a user are based on the posts that the user has liked in the past.

These platforms also use algorithms similar to KNN to sort the posts in a user’s feed. The posts that are similar to the posts that a user has liked in the past are shown at the top of the user’s feed.



URL: https://fahadsultan.com/csc272/2_maths/50_calculus.html


7. Calculus



URL: https://fahadsultan.com/csc272/2_maths/51_integral.html


7.1. Integral Calculus



URL: https://fahadsultan.com/csc272/2_maths/52_differential.html


7.2. Differential Calculus
7.2.1. Cost and Objective Functions

Mean squared error is also known as the L2 loss function. It is defined as follows:



MSE
=
1
𝑛
∑
𝑖
=
1
𝑛
(
𝑦
𝑖
−
𝑦
^
𝑖
)
2

where 
𝑦
𝑖
 is the true value of the 
𝑖
th example, 
𝑦
^
𝑖
 is the predicted value of the 
𝑖
th example, and 
𝑛
 is the number of examples.

Other common loss functions include the L1 loss function, which is defined as follows:



L1
=
1
𝑛
∑
𝑖
=
1
𝑛
|
𝑦
𝑖
−
𝑦
^
𝑖
|

The L1 loss function is also known as the mean absolute error (MAE)

The L1 loss function is less sensitive to outliers than the L2 loss function. For example, if we have the following two vectors:

𝑦
=
[
1
,
2
,
3
,
4
,
5
]
𝑦
^
=
[
1
,
2
,
3
,
4
,
100
]

The MSE between these two vectors is 1, while the MAE is 18.4. The MSE is more sensitive to outliers because it squares the difference between the true and predicted values. The L1 loss function is less sensitive to outliers because it takes the absolute value of the difference between the true and predicted values.

7.2.2. Optimization

Optimization is the process of finding the minimum (or maximum) of a function that depends on some inputs, called design variables. In machine learning, we often want to find the minimum of a loss function, which is a function that measures how bad our model is. For example, in linear regression, we want to find the parameters that minimize the mean squared error (MSE) between the predictions of our model and the true values.



URL: https://fahadsultan.com/csc272/3_problems_in/40_supervised.html


8. Supervised Learning

Supervised learning is similar to how we learn in school.

First, there is a learning or training phase where we learn from lots of examples. Examples are essentially a set of questions and their answers.

The training phase is then followed by a testing phase where we apply our learning to a relatively small set of previously unseen questions.

Finally, there is an evaluation on how well we applied our knowledge to the test questions by comparing our answers to the correct answers.

Most machine learning problems involve predicting a single random variable 
𝑦
 from one or more random variables 
𝑋
.

The underlying assumption, when we set out to accomplish this, is that the value of 
𝑦
 is dependent on the value of 
𝑋
 and the relationship between the two is governed by some unknown function 
𝑓
 i.e.

𝑦
=
𝑓
(
𝑋
)

𝑋
 is here simply some data that we have as a pandas Dataframe pd.DataFrame whereas 
𝑦
 here is the target variable, one value for each observation, that we want to predict, as a pandas Series pd.Series.




The figure above just depicts the core assumption underlying most machine learning problems.

Assumptions, loosely speaking, are what we formally call models.

Therefore, the basic mathematical model underlying most machine learning problems is that the target variable 
𝑦
 is a function of the input variables 
𝑋
 i.e. 
𝑦
=
𝑓
(
𝑋
)
.

If this assumption does NOT hold, then there is nothing to learn and we cannot predict 
𝑦
 from 
𝑋
. In other words, 
𝑦
 is independent of 
𝑋
. For example, if we try to predict the outcome of a coin toss using the time of day, we will fail miserably because the two are independent of each other.

The core problem, distinct from any models or assumptions, here is that the function 
𝑓
 is unknown to us and we need to ”learn” it from the data.

Such problems fall under the broad category of Supervised Learning.

There are two primary types of supervised learning problems:

Classification - when the target variable 
𝑦
 is categorical

Regression - when the target variable 
𝑦
 is continuous

Example Dataset

The code below loads the iris dataset from the sklearn library. This dataset is a classic example of a classification problem.

X is a pandas DataFrame with 4 columns and 150 rows. Each row represents a flower and each column represents a feature of the flower. The features are:

sepal length in cm

sepal width in cm

petal length in cm

petal width in cm

y is a pandas Series with 150 rows. Each row represents the species of the flower. The species are:

Iris Setosa

Iris Versicolour

Iris Virginica

The goal is to predict the species of a flower given its features.

from sklearn.datasets import load_iris

data = load_iris(as_frame=True)
X    = data['data']
y    = data['target']

print("\n================= X - Features ==================\n")
print(X.sample(5, random_state=42))
print("\n============== y (Classification) ==============\n")
print(y.sample(5, random_state=42))

================= X - Features ==================

     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)
73                 6.1               2.8                4.7               1.2
18                 5.7               3.8                1.7               0.3
118                7.7               2.6                6.9               2.3
78                 6.0               2.9                4.5               1.5
76                 6.8               2.8                4.8               1.4

============== y (Classification) ==============

73     1
18     0
118    2
78     1
76     1
Name: target, dtype: int64

8.1. Train-Test Split

The first step in supervised learning is to split the data into two sets: a training set and a test set.

The training set is used to train or learn the parameters of the model. The test set is used to evaluate the performance of the learning.

The convention is to use majority of the data for training and the rest for testing. The ratio of training to test data is typically 80:20 or 70:30.






It is extremely important to ensure the following:

The training set and test set are mutually exclusive i.e. no observation in the training set should be in the test set and vice versa.

The train and test sets are representative of the overall data. For example, if the data is sorted by date, then the train set should have observations from all dates and not just the most recent dates.

To ensure this, the train set must be randomly drawn from the data.

This can be implemented by randomly shuffling the data before splitting it into train and test sets or using the built-in .sample method.

8.1.1. Code Example

The code below uses the train_test_split function from the sklearn library to split the data into train and test sets.

The test_size parameter is set to 0.2 which means that 20% of the data will be used for testing and the remaining 80% will be used for training.

The random_state parameter is set to 42 which means that the random number generator will be initialized to a known state. This ensures that the results are reproducible.

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

print("Train set:\n X_train.shape =", X_train.shape, ", y_train.shape =", y_train.shape)
print("Test set: \n X_test.shape =",  X_test.shape,  ", y_test.shape =",  y_test.shape)

Train set:
 X_train.shape = (112, 4) , y_train.shape = (112,)
Test set: 
 X_test.shape = (38, 4) , y_test.shape = (38,)

8.2. Training Phase

In supervised learning problems, we have a set of observations, each observation consisting of a set of input variables 
𝑋
 and a target variable 
𝑦
, and we want to learn a function 
𝑓
 that maps the input 
𝑋
 to the output 
𝑦
.




In the figure above, 
𝐹
 is the family of functions that we are considering. These are the second level of assumptions that we make and are what are commonly referred to as models.

The function 
𝑓
 is the output function that we want have learned from the data.

For example, a common family of functions 
𝐹
 is linear i.e.

𝐹
(
𝑥
)
=
𝑚
⋅
𝑥
+
𝑏

If we are trying to predict the temperature in Fahrenheit given the temperature in Celsius, then

𝑓
(
𝑥
)
=
𝑥
⋅
9
5
+
32

where x is the input temperature in Celsius and f(x) is the output temperature in Fahrenheit.

The above is just an example and there exist many other families of functions that we can consider. These will be covered in Chapters 10-13.

8.2.1. Code Example

The code below uses K-Nearest Neighbors (KNN) to learn the relationship between the features and the target variable.

The fit method is used to train the model. The fit method takes two arguments:

X_train - the features of the training data

y_train - the target variable of the training data

The fit method learns the relationship between the features and the target variable and stores it in the model object.

The fit method is common to all models in sklearn and is used to train the model.

from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB

classifier = MultinomialNB()

classifier.fit(X_train, y_train)

MultinomialNB()

8.3. Testing Phase

Once we have learned the function 
𝑓
 from the training data, we can apply it to the test data to predict the target variable 
𝑦
.

8.3.1. Code Example

The code below uses the predict method to predict the target variable for the test data.

The predict method is used to test the model. The predict method takes one argument:

X_test - the features of the test data

The predict method applies the learned function 
𝑓
 to the test data and returns the predicted target variable.

The predict method is common to all models in sklearn and is used to test the model.

preds = classifier.predict(X_test)

preds

array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,
       0, 2, 1, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0])

8.4. Evaluation

If supervised learning is like school, then the evaluation phase is like the grading phase and criteria.

The evaluation phase is where we evaluate how well we have learned the function 
𝑓
 from the training data and how well we can predict the target variable 
𝑦
 from the test data.

The exact criteria for evaluation depends on the type of supervised learning problem. That is, evaluation metrics used for classification problems are different from those used for regression problems. These metrics are discussed in detail in the following chapters.

8.4.1. Code Example

The code below uses the accuracy_score method to evaluate the performance of the model.

The accuracy_score method is used to evaluate the model. The accuracy_score method takes two arguments:

y_test - the actual target variable of the test data

y_pred - the predicted target variable of the test data

The accuracy_score method compares the predicted target variable to the actual target variable and returns the accuracy of the model.

from sklearn.metrics import accuracy_score

accuracy_score(y_test, preds)

0.9736842105263158

8.5. Cross Validation

Just as most courses in school don’t have just the final test, most machine learning problems don’t have just one test set either. Often times, it is better to create multiple test sets, evaluate the performance of the model on each of them and then report the average performance of the model. This practice is called cross validation in machine learning.

There exist many different ways to create multiple test sets. The most common way is to randomly split the data into multiple randomly sampled train and test sets. This is called random cross validation.

8.5.1. Code Example

The code below uses the cross_val_score method to perform random cross validation.

The cross_val_score method is used to cross validate the model. The cross_val_score method takes four arguments:

estimator - the model object

X - the features of the data

y - the target variable of the data

cv - the number of cross validation splits

The cross_val_score method performs random cross validation and returns an evaluation metric of the model for each split.

from sklearn.model_selection import cross_val_score

scores = cross_val_score(classifier, X, y, cv=5, scoring='accuracy')

scores

array([1.        , 0.96666667, 0.9       , 0.9       , 1.        ])




URL: https://fahadsultan.com/csc272/3_problems_in/41_classification.html


8.6. Classification

Classification lies at the heart of both human and machine intelligence. Deciding what letter, word, or image has been presented to our senses, recognizing faces or voices, sorting mail, assigning grades to homeworks; these are all examples of assigning a category to an input.

One method for classification is to use handwritten rules. There are many areas of data mining where handwritten rule-based classifiers constitute a state-of-the-art system, or at least part of it. Rules can be fragile, however, as situations or data change over time, and for some tasks humans aren’t necessarily good at coming up with the rules. Most cases of classification therefore are instead done via supervised machine learning.

Classification is the type of supervised learning where 
𝑦
 is a discrete categorical variable.

The discrete output variable 
𝑦
 is often also called the label or target or class.

For example, we might want to predict whether a patient has a disease or not, based on their symptoms. In this case, 
𝑦
 is a binary variable, taking the value 1 if the patient has the disease, and 0 otherwise. Other examples of classification problems include predicting the sentiment of a movie review: positive, negative, or neutral.

For example,




In other words, the classification problem is to learn a function 
𝑓
 that maps the input 
𝑋
 to the discrete output 
𝑌
.

8.6.1. Evaluation Metrics

The most common metric for evaluating a classifier is accuracy. Accuracy is the proportion of correct predictions. It is the number of correct predictions divided by the total number of predictions.

𝐴
𝑐
𝑐
𝑢
𝑟
𝑎
𝑐
𝑦
=
Number of correct predictions
Total number of predictions

For example, if we have a test set of 100 documents, and our classifier correctly predicts the class of 80 of them, then the accuracy is 80%.

Accuracy is a good metric when the classes are balanced 
𝑁
𝑐
𝑙
𝑎
𝑠
𝑠
1
≈
𝑁
𝑐
𝑙
𝑎
𝑠
𝑠
2
. However, when the classes are imbalanced, accuracy can be misleading. For example, if we have a test set of 100 documents, and 95 of them are positive and 5 of them are negative, then a classifier that always predicts positive will have an accuracy of 95%. However, this classifier is not useful, because it never predicts negative.

8.6.1.1. Multi-class classification as multiple Binary classifications

Every multi-class classification problem can be decomposed into multiple binary classification problems. For example, if we have a multi-class classification problem with 3 classes, we can decompose it into 3 binary classification problems.






Assuming the categorical variable that we are trying to predict is binary, we can define the accuracy in terms of the four possible outcomes of a binary classifier:

True Positive (TP): The classifier correctly predicted the positive class.

False Positive (FP): The classifier incorrectly predicted the negative class as positive.

True Negative (TN): The classifier correctly predicted the negative class.

False Negative (FN): The classifier incorrectly predicted the positive class as negative.

True positive means that the classifier correctly predicted the positive class. False positive means that the classifier incorrectly predicted the positive class. True negative means that the classifier correctly predicted the negative class. False negative means that the classifier incorrectly predicted the negative class.

These definitions are summarized in the table below:

	

Prediction 
𝑦
^
=
𝑓
′
(
𝑥
)

	

Truth 
𝑦
=
𝑓
(
𝑥
)




True Negative (TN)

	

0

	

0




False Negative (FN)

	

0

	

1




False Positive (FP)

	

1

	

0




True Positive (TP)

	

1

	

1

In terms of the four outcomes above, the accuracy is:

Accuracy
=
𝑇
𝑃
+
𝑇
𝑁
𝑇
𝑃
+
𝑇
𝑁
+
𝐹
𝑃
+
𝐹
𝑁

Accuracy is a useful metric, but it can be misleading.

Other metrics that are often used to evaluate classifiers are:

Precision: The proportion of positive predictions that are correct. Mathematically, it is defined as:

Precision
=
𝑇
𝑃
𝑇
𝑃
+
𝐹
𝑃

Recall: The proportion of positive instances that are correctly predicted. Mathematically, it is defined as:

Recall
=
𝑇
𝑃
𝑇
𝑃
+
𝐹
𝑁

The precision and recall are often combined into a single metric called the F1 score. The F1 score is the harmonic mean of precision and recall. The harmonic mean of two numbers is given by:

F1 Score: The harmonic mean of precision and recall.

F1-Score
=
2
×
Precision
⋅
Recall
Precision
+
Recall

Many kinds of machine learning algorithms are used to build classifiers. Two common classifiers are Naive Bayes and Logistic Regression.

These exemplify two primary category of models for doing classification:

Generative models like naive Bayes build a model of how a class could generate some input data. Given an observation, they return the class most likely to have generated the observation.

Discriminative models like logistic regression instead learn what features from the input are most useful to discriminate between the different possible classes.

While discriminative systems are often more accurate and hence more commonly used, generative classifiers still have a role. They can be more robust to missing data, and can be used to generate synthetic data.

8.6.1.2. Code Example

The code below shows how to train a Nearest Neighbor classifier on the Iris dataset. The Iris dataset is a dataset of 150 observations of iris flowers. There are 3 classes of iris flowers: setosa, versicolor, and virginica. For each observation, there are 4 features: sepal length, sepal width, petal length, and petal width. The goal is to predict the class of iris flower given the 4 features.

The code below uses the scikit-learn library to train a Nearest Neighbor classifier on the Iris dataset. The Nearest Neighbor classifier is a simple classifier that works by finding the training observation that is closest to the test observation, and predicting the class of the closest training observation. The Nearest Neighbor classifier is a discriminative classifier.

There are 5 steps shown in the code below:

Import the dataset: The Iris dataset is included in scikit-learn. We import it using the load_iris function.

Split the dataset into training and test sets: We split the dataset into a training set and a test set. The training set is used to train the classifier, and the test set is used to evaluate the classifier.

Instantiate the classifier: We instantiate the classifier using the KNeighborsClassifier class.

Train the classifier: We train the classifier using the fit method.

Make predictions: We make predictions on the test set using the predict method.

Print the classification report: We evaluate the classifier using the classification_report function.

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report

# 1. Load the data
data = load_iris(as_frame=True)
X    = data['data']
y    = data['target']

# 2. Create a train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# 3. Instantiate a model
model = KNeighborsClassifier()

# 4. Fit a model
model.fit(X_train, y_train)

# 5. Predict on the test set
preds = model.predict(X_test)

# 6. Print classification report
print(classification_report(y_test, preds))

              precision    recall  f1-score   support

           0       1.00      1.00      1.00         8
           1       0.91      1.00      0.95        10
           2       1.00      0.92      0.96        12

    accuracy                           0.97        30
   macro avg       0.97      0.97      0.97        30
weighted avg       0.97      0.97      0.97        30




URL: https://fahadsultan.com/csc272/3_problems_in/42_regression.html


8.7. Regression

Recall that all supervised learning is based on the assumption that there is a relationship between the input variables 
𝑋
 and the output variable 
𝑦
 i.e.

y
=
𝑓
(
X
)

where 
𝑓
 is some unknown function.

𝑋
 here simply is some data that we have as a pd.DataFrame whereas 
𝑦
 here is the target variable, one value for each observation, that we want to predict, as of type pd.Series.

The form of supervised learning we have talked about so far is classification. As discussed previously, in classification, the output variable 
𝑦
 is a discrete target variable e.g. sentiment 
∈
 {positive, neutral or negative}, ring 
∈
 {A, B} or diagnosis 
∈
 {malignant, benign} etc.

The other type of supervised learning that we will talk about in this notebook is called Regression. In regression, the target variable is to predict a continuous target variable i.e. $
𝑦
∈
𝑅
𝑁
$.

For example, predicting the stock price of a publicly listed company, predicting the price of a house in dollars, or predicting the average surface temperature on Earth next year are all examples of regression problems.

Note that the splitting of the data into training and test sets is exactly the same as in classification. The only difference is that the target variable is continuous instead of discrete.

8.7.1. Linear Regression

Linear Regression is the simplest solution to any regression problem.

In linear regression, the relationship between the input variable(s) 
𝑋
 and the output variable 
𝑦
 is assumed to be a linear.

8.7.1.1. Univariate Linear Regression

For simplicity, let’s assume we only have one input variable 
𝑥
 and one output variable 
𝑦
.

Our goal, then is to find a linear function 
𝑓
 that maps 
𝑥
 to 
𝑦
:

𝑦
=
𝑓
(
𝑥
)

Recall that the equation for a straight line is 
𝑓
(
𝑥
)
=
𝑚
𝑥
+
𝑏

where 
𝑚
 is the slope of the line and 
𝑏
 is the y-intercept. Assuming that 
𝑓
 is a linear function, we can write:

𝑦
=
𝑓
(
𝑥
)
=
𝑚
⋅
𝑥
+
𝑏

Expanding this equation for each observation in our dataset, we get:









[
𝑦
1


𝑦
2


⋮


𝑦
𝑛
]
=
𝑚
×
[
𝑥
1


𝑥
2


⋮


𝑥
𝑛
]
+
𝑏

where 
𝑛
 is the number of observations in our dataset.

The goal of linear regression is to find the values of 
𝑚
 and 
𝑏
 that best fit the data.

The cell code below plots y vs x where 
𝑦
=
𝑚
𝑥
+
𝑏
 for a given value of 
𝑚
 and 
𝑏
.

from matplotlib import pyplot as plt 
import pandas as pd 
import seaborn as sns

m = 5
b = 10

df = pd.DataFrame()
df['x'] = range(-5, 6)
df['f (x)'] = m * df['x'] + b

sns.lineplot(x='x', y='f (x)', data=df);

plt.axhline(0, color='black');
plt.axvline(0, color='black');

plt.title("f(x) = mx + b\n\nslope (m) = %s\ny-intercept (b) = %s " % (m, b));
plt.grid()


Now let’s load a dataset and try to find the best values of 
𝑚
 and 
𝑏
 that fit the data.

The code below loads a dataset of Average Land Temperature on each country for each year from 1750 to 2015.

The temperature is in degrees Celsius. Below these temperatures are aggregated and global averages are calculated for each year.

import pandas as pd

data         = pd.read_csv('../data/GlobalLandTemperaturesByCountry.csv')
data['dt']   = pd.to_datetime(data['dt'])
data['year'] = data['dt'].apply(lambda x: x.year)
data['month']= data['dt'].apply(lambda x: x.month)
data         = data.dropna()
data         = data[(data['year'] >= 1900) & (data['month'] == 1)]
avgs         = data.groupby('year').mean()['AverageTemperature']
avgs.name    = 'Average Temperature (C)'

sns.scatterplot(x=avgs.index, y=avgs);


In the code below, the data is split into training and test sets, similar to what we did in the classification examples for Naive Bayes and Nearest Neighbor models.

train_pct = 0.8
train_size = int(train_pct * len(avgs))
train_set = avgs.sample(train_size, random_state=42)
test_set  = avgs[~avgs.index.isin(train_set.index)]

X_train = train_set.index
y_train = train_set

X_test = test_set.index
y_test = test_set

sns.scatterplot(x=X_train, y=y_train, label='Train set');
sns.scatterplot(x=X_test,  y=y_test,  label='Test set');


In the code cell below, two linear functions are plotted against the training data. Both implement the same linear function 
𝑓
(
𝑥
)
=
𝑚
𝑥
+
𝑏
 but with different values of 
𝑚
 and 
𝑏
.

sns.scatterplot(x=X_train, y=y_train,  label='Train Set');

b = -5.5
m = 0.0101
model1 = m * X_train + b
sns.scatterplot(x=X_train, y=model1.values,  label='Model 1');


b = 14.5
m = 0
model2 = m * X_train + b
sns.scatterplot(x=X_train, y=model2.values,  label='Model 2');

8.7.1.2. Errors in Regression

The evaluation of regression models is done similar to classification models. The model is trained on the training set and then evaluated on the test set.

The difference is that the training has an internal evaluation metric that is minimized to find the values of coefficients such as 
𝑚
 and 
𝑏
 that best fit the training data.

For example, model-1 and model-2 above would yield different scores for how well they fit the training data. The model with the lowest score is the one that best fits the training data.

Once the model is trained, the test set is used to evaluate the model.

The internal evaluation metrics for linear regression during training are similar to the ones used in extrinsic evaluation on the test set.

The most common evaluation metrics for linear regression are as follows:

8.7.1.2.1. Residuals

Residuals are the difference between the true values of y and the predicted values of y.

Residual
𝑖
=
𝑦
𝑖
−
𝑦
^
𝑖

where 
𝑦
𝑖
 is the 
𝑖
𝑡
ℎ
 true value of the target variable and 
𝑦
𝑖
^
 is the 
𝑖
𝑡
ℎ
 predicted value of the target variable i.e. $
𝑦
𝑖
^
=
𝑚
𝑥
𝑖
+
𝑏
$

df = pd.DataFrame()
df['y'] = y_train
df['model1'] = model1
df['model2'] = model2

sns.scatterplot(x=X_train, y=y_train);
sns.scatterplot(x=X_train, y=model1.values);
df.apply(lambda x: plt.plot((x.name, x.name), (x['y'], x['model1']), color='red', linewidth=1), axis=1);
plt.legend(['Train set', 'Model 1', 'Residuals']);

sns.scatterplot(x=X_train, y=y_train);
sns.scatterplot(x=X_train, y=model2.values, color='green');
df.apply(lambda x: plt.plot((x.name, x.name), (x['y'], x['model2']), color='red', linewidth=1), axis=1);
plt.legend(['Train set', 'Model 2', 'Residuals']);

8.7.1.2.2. Mean Absolute Error

The Mean Absolute Error (or MAE) is the average of the absolute differences between predictions and actual values. It gives an idea of how wrong the predictions were. The measure gives an idea of the magnitude of the error, but no idea of the direction (e.g. over or under predicting).



𝑀
𝐴
𝐸
=
1
𝑛
∑
𝑖
=
1
𝑛
|
residual
𝑖
|

or



𝑀
𝐴
𝐸
=
1
𝑛
∑
𝑖
=
1
𝑛
|
𝑦
𝑖
−
𝑦
^
𝑖
|

In the code cell below, two linear functions are plotted against the training data. Both implement the same linear function 
𝑓
(
𝑥
)
=
𝑚
𝑥
+
𝑏
 but with different values of 
𝑚
 and 
𝑏
.

sns.scatterplot(x=X_train, y=y_train,  label='Train Set');

def mae(y, y_hat):
    return sum(abs(y - y_hat)) / len(y)

b = -5.5
m = 0.0101
model1 = m * X_train + b
mae_model1 = mae(y_train, model1)
sns.scatterplot(x=X_train, y=model1.values,  label=r'Model 1 (MAE$_{~train}$ = %s)' % round(mae_model1, 2));


b = 14.5
m = 0
model2 = m * X_train + b
mae_model2 = mae(y_train, model2)
sns.scatterplot(x=X_train, y=model2.values,  label=r'Model 2 (MAE$_{~train}$ = %s)' % round(mae_model2, 2));


In order to find the best values of 
𝑚
 and 
𝑏
, we need to define an evaluation metric that we want to minimize.

The code cell below plots model 1 and model 2 against the test set and also calculates the MAE for each model.

sns.scatterplot(x=X_train, y=y_train,  label='Test Set');

def mae(y, y_hat):
    return sum(abs(y - y_hat)) / len(y)

b = -5.5
m = 0.0101
model1 = m * X_test + b
mae_model1 = mae(y_test, model1)
sns.lineplot(x=X_test, y=model1.values, linewidth=3,\
             label=r'Model 1 (MAE$_{~test}$ = %s)' % round(mae_model1, 2), color='orange');


b = 14.5
m = 0
model2 = m * X_test + b
mae_model2 = mae(y_test, model2)
sns.lineplot(x=X_test, y=model2.values, linewidth=3,  \
             label=r'Model 2 (MAE$_{~test}$ = %s)' % round(mae_model2, 2), color='green');

8.7.1.2.3. Mean Squared Error

The Mean Squared Error (or MSE) is much like the mean absolute error in that it provides a gross idea of the magnitude of error.



𝑀
𝑆
𝐸
=
1
𝑛
∑
𝑖
=
1
𝑛
(
Residual
𝑖
)
2

or



𝑀
𝑆
𝐸
=
1
𝑛
∑
𝑖
=
1
𝑛
(
𝑦
𝑖
−
𝑦
^
𝑖
)
2
def mse(y, y_hat):
    return sum((y - y_hat)**2) / len(y)

mse1 = mse(df['y'], df['model1'])
mse2 = mse(df['y'], df['model2'])

print(" MSE model 1: ", round(mse1, 2), "\n", "MSE model 2: ", round(mse2, 2))

 MSE model 1:  0.17 
 MSE model 2:  0.37

8.7.1.2.4. Root Mean Squared Error

Taking the square root of the mean squared error converts the units back to the original units of the output variable and can be meaningful for description and presentation. This is called the Root Mean Squared Error (or RMSE).



𝑅
𝑀
𝑆
𝐸
=
1
𝑛
∑
𝑖
=
1
𝑛
(
Residual
𝑖
)
2

or



𝑅
𝑀
𝑆
𝐸
=
1
𝑛
∑
𝑖
=
1
𝑛
(
𝑦
𝑖
−
𝑦
^
𝑖
)
2
def rmse(y, y_hat):
    return (sum((y - y_hat)**2) / len(y))**(1/2)

mse1 = rmse(df['y'], df['model1'])
mse2 = rmse(df['y'], df['model2'])

print(" RMSE model 1: ", round(mse1, 2), "\n", "RMSE model 2: ", round(mse2, 2))

 RMSE model 1:  0.41 
 RMSE model 2:  0.61

8.7.1.2.5. 
R
2

The 
R
2
 (or R Squared) metric provides an indication of the goodness of fit of a set of predictions to the actual values. In statistical literature, this measure is called the coefficient of determination. This is a value between 0 and 1 for no-fit and perfect fit respectively.

R
2
=
1
−
∑
𝑖
=
1
𝑛
(
𝑦
𝑖
−
𝑦
^
𝑖
)
2
∑
𝑖
=
1
𝑛
(
𝑦
𝑖
−
𝑦
¯
)
2

where 
𝑦
¯
=
1
𝑛
∑
𝑖
=
1
𝑛
𝑦
𝑖
 is the mean of the observed data.

def r2(y, y_hat):
    return 1 - sum((y - y_hat)**2) / sum((y - y.mean())**2)

r2_1 = r2(df['y'], df['model1'])
r2_2 = r2(df['y'], df['model2'])

print(" R2 model 1: ", round(r2_1, 2), "\n", "R2 model 2: ", round(r2_2, 2))

 R2 model 1:  0.36 
 R2 model 2:  -0.4

8.7.2. Multivariate Regression

Multiple linear regression (MLR), also known simply as multiple regression, uses multiple (> 1) input variables (
𝑋
) to predict the outcome of a target variable (
𝑦
∈
𝑅
) by fitting a linear equation to observed data.

𝑦
=
𝑓
(
𝑋
)

Here 
𝑓
 is a linear function of the form:

𝑓
(
𝑋
)
=
𝑋
𝑚
+
𝑏

where 
𝑋
 is a matrix of 
𝑁
 observations and 
𝐷
 features and 
𝑦
 is a vector of 
𝑁
 observations.

			
			
			
			
𝑋
=
[
𝑥
11
	
𝑥
12
	
…
	
𝑥
1
𝐷


𝑥
21
	
𝑥
22
	
…
	
𝑥
2
𝐷


⋮
	
⋮
	
⋱
	
⋮


𝑥
𝑁
1
	
𝑥
𝑁
2
	
…
	
𝑥
𝑁
𝐷
]




𝑦
=
[
𝑦
1


𝑦
2


⋮


𝑦
𝑁
]

where 
𝑚
 is a vector of 
𝐷
 slopes and 
𝑏
 is the y-intercept. i.e.





𝑚
=
[
𝑚
1


𝑚
2


⋮


𝑚
𝐷
]

Putting it all together, we get:





			
			
			
			




[
𝑦
1


𝑦
2


⋮


𝑦
𝑁
]
=
[
𝑥
11
	
𝑥
12
	
…
	
𝑥
1
𝐷


𝑥
21
	
𝑥
22
	
…
	
𝑥
2
𝐷


⋮
	
⋮
	
⋱
	
⋮


𝑥
𝑁
1
	
𝑥
𝑁
2
	
…
	
𝑥
𝑁
𝐷
]
[
𝑚
1


𝑚
2


⋮


𝑚
𝐷
]
+
𝑏

The goal of multiple linear regression is to find the values of 
𝑚
1
,
𝑚
2
,
…
,
𝑚
𝐷
 and 
𝑏
 that best fit the data.




Now let’s load a dataset and try to find the best values of 
𝑚
1
 and 
𝑏
 that fit the data.

The code below uses data from California Housing Dataset to predict the median house value in California districts given the following input variables:

MedInc: Median income in block.

HouseAge: Median house age within a block (measured in years).

AveRooms: Average number of rooms within a block of houses.

AveBedrms: Average number of bedrooms within a block of houses.

Population: Total number of people residing within a block.

AveOccup: Average number of people occupying each house within a block.

Longitude: A measure of how far west a house is; a higher value is farther west.

Latitude: A measure of how far north a house is; a higher value is farther north.

The target variable is:

Median house value for households within a block (measured in US Dollars).

import pandas as pd 
from sklearn import  datasets

california = datasets.fetch_california_housing()
X = pd.DataFrame(california.data, columns=california.feature_names)
y = pd.Series(california.target, name='Price')

X.shape

(20640, 8)

X.head()

	MedInc	HouseAge	AveRooms	AveBedrms	Population	AveOccup	Latitude	Longitude
0	8.3252	41.0	6.984127	1.023810	322.0	2.555556	37.88	-122.23
1	8.3014	21.0	6.238137	0.971880	2401.0	2.109842	37.86	-122.22
2	7.2574	52.0	8.288136	1.073446	496.0	2.802260	37.85	-122.24
3	5.6431	52.0	5.817352	1.073059	558.0	2.547945	37.85	-122.25
4	3.8462	52.0	6.281853	1.081081	565.0	2.181467	37.85	-122.25
y.head()

0    4.526
1    3.585
2    3.521
3    3.413
4    3.422
Name: Price, dtype: float64

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

X_train, X_test, y_train, y_test = train_test_split(X, y)

model = LinearRegression()

model.fit(X_train, y_train)

y_hat = model.predict(X_test)

print("MAE: ",  round(mean_absolute_error(y_test, y_hat), 2))
print("MSE: ",  round(mean_squared_error(y_test, y_hat), 2))
print("RMSE:",  round((mean_squared_error(y_test, y_hat))**(1/2), 2))
print("R2:  ",  round(r2_score(y_test, y_hat), 2))

MAE:  0.53
MSE:  0.51
RMSE: 0.71
R2:   0.62


Choosing between these metrics depends on the specific context of the problem:

Use MAE if you want a metric that’s easy to understand and not influenced much by outliers.

Use RMSE when larger errors should be penalized more heavily and a metric in the same unit as the target variable is desired.

Use MSE when optimizing models since it emphasizes larger errors, making it useful in minimizing those errors during training

results = pd.DataFrame()
results['Actual'] = y_test
results['Prediction'] = y_hat

import seaborn as sns
from matplotlib import pyplot as plt
plt.figure(figsize=(10, 3))
sns.scatterplot(x='Actual', y='Prediction', data=results);

8.7.3. Interpreting the Model

The model we have trained is a linear function of the form:





MedianHouseValue
=
(
𝑚
1
×
MedInc
)
+
(
𝑚
2
×
HouseAge
)
+


(
𝑚
3
×
AveRooms
)
+
(
𝑚
4
×
AveBedrms
)
+


(
𝑚
5
×
Population
)
+
(
𝑚
6
×
AveOccup
)
+


(
𝑚
7
×
Longitude
)
+
(
𝑚
8
×
Latitude
)
+
𝑏

where 
𝑚
1
,
𝑚
2
,
…
,
𝑚
8
 are the slopes and 
𝑏
 is the y-intercept.

The slopes 
𝑚
1
,
𝑚
2
,
…
,
𝑚
8
 tell us how much the target variable changes when the corresponding input variable changes by 1 unit.

The code cell below plots the slopes in decreasing order of magnitude.

weights = pd.Series(model.coef_, index=X.columns)
weights = weights.sort_values(ascending=False)
sns.barplot(x=weights.index, y=weights.values, palette='bwr');
plt.xticks(rotation=90);


The plot indicates that AveBedrms, MedInc and AveRooms have the highest positive relationship with the Price of the house whereas AveRooms, Latitude, Longitude have the strongest negative relationship with the target variable Price of the house.

8.7.4. Polynomial Regression

Polynomial functions are functions that have the form:

𝑓
(
𝑥
)
=
𝑏
+
𝑚
1
𝑥
+
𝑚
2
𝑥
2
+
𝑚
3
𝑥
3
+
.
.
.
+
𝑚
𝑛
𝑥
𝑛

where 
𝑏
,
𝑚
1
,
𝑚
2
,
𝑚
3
,
.
.
.
,
𝑤
𝑛
 are the coefficients of the polynomial function and 
𝑛
 is called the degree of the polynomial. In other words, the degree of a polynomial function is the highest power of the variable in the polynomial function.

Note that the linear function 
𝑓
(
𝑥
)
=
𝑚
𝑥
+
𝑏
 is a special case of the polynomial function. More specifically, a linear function is a polynomial function of degree 1.

Polynomial functions of degree 2 or higher are called non-linear functions. As the degree of the polynomial function increases, the function becomes more flexible and can fit more complex patterns in the data.

If we have only one input variable 
𝑥
 to predict the output variable 
𝑦
, then the polynomial function becomes:

𝑦
=
𝑓
(
𝑥
)
=
𝑏
+
𝑚
1
𝑥
+
𝑚
2
𝑥
2
+
𝑚
3
𝑥
3
+
.
.
.
+
𝑚
𝑛
𝑥
𝑛

In matrix notation, polynomial regression can be written as:

𝑓
(
𝑥
)
=
𝑋
𝑚
+
𝑏

where 
𝑋
 is a matrix of 
𝑁
 observations and each feature is raised to a power from 1 to 
𝐷
.





	
	
		
	
	
	
		
	
					
	
	
		
	




[
𝑦
1


𝑦
2


⋮


𝑦
𝑁
]
=
[
𝑥
1
	
𝑥
1
2
	
𝑥
1
3
	
…
	
𝑥
1
𝐷


𝑥
2
	
𝑥
2
2
	
𝑥
2
3
	
…
	
𝑥
2
𝐷


⋮
	
⋮
	
⋮
	
⋮
	
⋱
	
⋮


𝑥
𝑁
	
𝑥
𝑁
2
	
𝑥
𝑁
3
	
…
	
𝑥
𝑁
𝐷
]
⋅
[
𝑚
1


𝑚
2


⋮


𝑚
𝐷
]
+
𝑏
8.7.4.1. Code Example

Let’s implement polynomial regression on the Average Land Temperature dataset.

import pandas as pd

data         = pd.read_csv('../data/GlobalLandTemperaturesByCountry.csv')
data['dt']   = pd.to_datetime(data['dt'])
data['year'] = data['dt'].apply(lambda x: x.year)
data['month']= data['dt'].apply(lambda x: x.month)
data         = data.dropna()
data         = data[(data['year'] >= 1900) & (data['month'] == 1)]
avgs         = data.groupby('year').mean()['AverageTemperature']
avgs.name    = 'Average Temperature (C)'

sns.scatterplot(x=avgs.index, y=avgs);

X = avgs.index.values.reshape(-1, 1)
y = avgs

annual_means = data.groupby('year').mean()['AverageTemperature'][::10]

X = annual_means.index.values.reshape(-1, 1)
y = annual_means.values

train_set = avgs.sample(int(0.8*len(avgs)), random_state=42).sort_index()
test_set  = avgs.drop(train_set.index).sort_index()

X_train   = train_set.index.values.reshape(-1, 1)
y_train   = train_set.values.reshape(-1, 1)

X_test    = test_set.index.values.reshape(-1, 1)
y_test    = test_set.values.reshape(-1, 1)


Implementing polynomial regression in sklearn is similar to linear regression but with one additional step. We need to transform the input data into a polynomial matrix before fitting the model. In sklearn, this is done using the PolynomialFeatures class.

The code cell below implements polynomial regression of degrees 1, 2 and 5 on the Average Land Temperature dataset.

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

plt.figure(figsize=(5, 5))
plt.scatter(X_train, y_train, label='X', alpha=0.7);

colors = ['orange', 'green', 'red']

for i, degree in enumerate([1, 2, 5]):

    # Create polynomial features for X_train and X_test
    poly         = PolynomialFeatures(degree=degree)
    X_train_poly = poly.fit_transform(X_train)
    X_test_poly  = poly.fit_transform(X_test)

    # Fit a linear regression model to the training data
    model        = LinearRegression()
    model.fit(X_train_poly, y_train)

    # Predict y values for X_test
    y_pred       = model.predict(X_test_poly)
    
    # Plot the predictions
    plt.plot(X_test, y_pred, linewidth=3, label='Degree = %s' % degree, alpha=0.7, color=colors[i]);

plt.legend();


Note that with increasing degree, the polynomial function can fit more complex patterns non-linear trends in the data.

8.7.5. Underfitting vs. Overfitting

This example demonstrates the problems of underfitting and overfitting and how we can use linear regression with polynomial features to approximate nonlinear functions.

The plot shows the function that we want to approximate, which is a part of the cosine function. In addition, the samples from the real function and the approximations of different models are displayed. The models have polynomial features of different degrees. We can see that a linear function (polynomial with degree 1) is not sufficient to fit the training samples. This is called underfitting.

A polynomial of degree 4 approximates the true function almost perfectly. However, for higher degrees the model will overfit the training data, i.e. it learns the noise of the training data. This is called overfitting. We evaluate quantitatively overfitting / underfitting by using cross-validation.

We calculate the mean squared error (MSE) on the validation set, the higher, the less likely the model generalizes correctly from the training data.

import matplotlib.pyplot as plt
import numpy as np

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures


def true_fun(X):
    return np.cos(1.5 * np.pi * X)

np.random.seed(0)

n_samples = 30
degrees = [1, 4, 15]

X = np.sort(np.random.rand(n_samples))
y = true_fun(X) + np.random.randn(n_samples) * 0.1

ax = sns.scatterplot(x=X, y=y, label='Samples');
ax.set(xlabel='X', ylabel='y', title = r'$y = cos(1.5 \pi  x) + \epsilon$');

sns.lineplot(x=X, y=true_fun(X), label='True function', color='blue');

plt.figure(figsize=(14, 5))
for i in range(len(degrees)):
    ax = plt.subplot(1, len(degrees), i + 1)
    plt.setp(ax, xticks=(), yticks=())

    polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False)
    linear_regression = LinearRegression()
    pipeline = Pipeline(
        [
            ("polynomial_features", polynomial_features),
            ("linear_regression", linear_regression),
        ]
    )
    pipeline.fit(X[:, np.newaxis], y)

    # Evaluate the models using crossvalidation
    scores = cross_val_score(
        pipeline, X[:, np.newaxis], y, scoring="neg_mean_squared_error", cv=10
    )

    X_test = np.linspace(0, 1, 100)
    plt.scatter(X, y, edgecolor="b", s=20, label="Samples")
    plt.plot(X_test, true_fun(X_test), label="True function")
    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label="Model %s (degree = %s)" % (i+1, degrees[i]))
    plt.xlabel("x")
    plt.ylabel("y")
    plt.xlim((0, 1))
    plt.ylim((-2, 2))
    plt.legend(loc="best")
    plt.title(
        "MSE = {:.2} +/- {:.2}".format(
            round(-scores.mean(), 2), round(scores.std(), 2)
        )
    )

plt.suptitle("Polynomial Regression with increasing degrees, leading to overfitting", fontsize=14);
plt.show()


Ideally, you want to strike a balance between underfitting (high training error, high testing error) and overfitting (low training error, high testing error) by picking a model complexity (number of parameters) that generalizes well to unseen data.

Note that model complexity here refers to the number of parameters in the model. For example, univariate linear regression model has 2 parameters (slope and y-intercept) whereas a polynomial regression model of degree 2 has 3 parameters (slope, y-intercept and coefficient of 
𝑥
2
).



URL: https://fahadsultan.com/csc272/3_problems_in/20_unsupervised.html


9. Unsupervised Learning



In contrast to supervised learning, in unsupervised learning there are no available labels in the dataset. The goal of unsupervised learning is to “infer” labels. This is done by learning the structure of the data by analyzing how different observations (rows) relate to one another.

In the figure below, the top row shows the inputs to supervised and unsupervised learning problems. Note that points in the top right panel are colored based on available labels. In contrast, the panel in top left shows the same data, but without labels.




The bottom row shows the outputs of the two problems. Note that supervised learning (right column) uses the labels to infer the underlying distributions so that a new incoming point could be categorized into one of the two labels. Unsupervised learning, in contrast, infers the structure of the data, and assigns labels to the data based on the inferred structure. This structure is captured by relative proximity or distances of observations to each other, not too differently from nearest neighbor.

Note that in unsupervised learning, there is no need to split the data into training and test sets. This is because there are no labels to predict. Instead, the goal is to learn the structure of the data, and then use this structure to infer labels for new data points.



URL: https://fahadsultan.com/csc272/3_problems_in/21_clustering.html


9.1. Clustering

Clustering is the most well-known unsupervised learning technique. The goal of clustering is to discover groups in observations. The groups are called clusters.

The data points in the same cluster are similar to each other, compared to points in different clusters, which are relatively dissimilar.

There are many clustering algorithms. In this notebook, we will focus on two of them:

One that requires the number of clusters (
𝑘
) to be specified: K-means.

And another that does NOT require the number of clusters to be specified: DBSCAN.

To compare the performance of the clustering algorithms, in the code below we will use the same six datasets capturing a wide variety of patterns and structures.

import pandas as pd 
from matplotlib import pyplot as plt
import seaborn as sns 

url = "https://raw.githubusercontent.com/fahadsultan/csc272/main/data/clusters/"

fnames = ["aniso", "blobs", "no_structure", "noisy_circles", "noisy_moons", "varied"]

datasets = {}

fig, axs = plt.subplots(1, len(fnames), figsize=(17, 3))
for i, fname in enumerate(fnames):
    df = pd.read_csv(url + fname + ".csv", index_col=0)
    df.columns = ['x1', 'x2']
    ax = sns.scatterplot(data=df, x='x1', y='x2', ax=axs[i]);
    ax.set(title=fname)
    datasets[fname] = df


Note that the data sets are not labeled. Also note that unsupervised learning algorithms do not work only with 2-dimensional data but with data of any dimensionality. Here we use 2-dimensional data only to be able to visualize the results.

9.1.1. K-means

The k-means algorithm is a simple and popular clustering algorithm. It is an iterative algorithm that partitions the data points into a pre-specified 
𝑘
 number of clusters.

The algorithm works as follows:

Start: Select 
𝑘
 random points as the initial centroids.

Update Cluster Assignments: Assign each data point to the cluster with the nearest centroid.

Update Cluster Centers: Update the centroids of the clusters by taking the average of the data points in each cluster.

Repeat steps 2 and 3 until the centroids do not change.

The animation below visualizes the algorithm:

The algorithm is guaranteed to converge to a result. However, the result may not be the optimal one.

Because of random initialization, the algorithm converges to different results on different runs. Such algorithms or processes, where there is an element of randomness but with some bounds of predictability, are called stochastic algorithms or processes.

9.1.1.1. Before Clustering

Let’s try the k-means algorithm on the blobs dataset first. Note that the raw data has just two features (x1 and x2) but no labels.

X = datasets['blobs']
X.head()

	x1	x2
0	-5.730354	-7.583286
1	1.942992	1.918875
2	6.829682	1.164871
3	-2.901306	7.550771
4	5.841093	1.565094

The code below plots the raw data as a scatter plot.

sns.scatterplot(data=X, x='x1', y='x2');
plt.title("Blobs dataset");


Note that there are clearly three clusters in the data where the points within each cluster are closer to each other compared to points across clusters.

9.1.1.2. Clustering

We will use the KMeans class from the sklearn.cluster module.

The constructor of the KMeans class takes the number of clusters 
𝑘
 as input.

The KMeans class has a fit() method that takes the data as input and runs the k-means algorithm on it.

After we fit the model to the data, we can use the .labels_ attribute to get the discovered labels of the clusters assigned to each data point.

Below we add a third feature label to the data, which is the cluster label assigned to each data point by the k-means algorithm.

from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=3, random_state=0)
kmeans.fit(X)
X['label'] = kmeans.labels_

X.head()

	x1	x2	label
0	-5.730354	-7.583286	1
1	1.942992	1.918875	2
2	6.829682	1.164871	2
3	-2.901306	7.550771	0
4	5.841093	1.565094	2
9.1.1.3. After Clustering

The code below plots the data again, but this time with the cluster labels.

import seaborn as sns 
sns.scatterplot(data=X, x='x1', y='x2', hue='label', palette='viridis');


Note that the k-means algorithm has perfectly discovered the three blobs in the data.

9.1.1.4. Limitations of K-means

K-means is a simple and popular clustering algorithm. However, it has some limitations:

It requires the number of clusters 
𝑘
 to be specified. If the number of clusters is not known in advance, then we need to try different values of 
𝑘
 and select the one that gives the best results.

It is sensitive to the initial random selection of centroids. The algorithm may converge to different results on different runs.

Since k-means is reliant on averages, it is sensitive to outliers. Outliers can significantly affect the location of the centroids and hence the clusters.

Most importantly, k-means does not work well with clusters of different sizes and densities. It assumes that the clusters are spherical and of similar size.

To illustrate this limitation, let’s try the k-means algorithm on a dataset that does not satisfy the assumptions of the algorithm.

X = datasets['noisy_circles']
print(X.head())
sns.scatterplot(data=X, x='x1', y='x2');

kmeans = KMeans(n_clusters=2, random_state=0)
kmeans.fit(X)
sns.scatterplot(data=X, x='x1', y='x2', hue=kmeans.labels_, palette='viridis');

         x1        x2
0 -0.469276  0.210118
1 -0.164164  0.986075
2 -0.471454  0.019974
3 -0.670347 -0.657977
4 -0.826468  0.594071


Note how the k-means algorithm fails to discover the two clusters in the data. This is because the clusters are a) not spherical and b) of different sizes.

Such failures of a clustering algorithm can only be detected by either visualizing the results or computing internal cluster validation metrics such as the silhouette score.

9.1.1.4.1. Silhouette Score

The Silhouette Score is calculated using the mean intra-cluster distance (
𝑎
) and the mean nearest-cluster distance (
𝑏
) for each sample. The Silhouette Coefficient for a sample is

Silhouette Coefficient
=
(
𝑏
−
𝑎
)
max
(
𝑎
,
𝑏
)

where 
𝑏
 is the distance between a sample and the nearest cluster that the sample is not a part of.

Note that Silhouette Coefficient is only defined if number of labels is 2 <= n_labels <= n_samples - 1.

sklearn.metrics.silhouette_score function returns the mean Silhouette Coefficient over all samples. To obtain the values for each sample, use silhouette_samples.

The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar.

A score of 1 indicates that the object is far away from the neighboring clusters. A score of 0 indicates that the object is close to the decision boundary between two neighboring clusters. A score of -1 indicates that the object may have been assigned to the wrong cluster.

from sklearn.metrics import silhouette_score

silhouette_score(X, kmeans.labels_)

0.355318252897544

9.1.2. DBSCAN

DBSCAN stands for Density-Based Spatial Clustering of Applications with Noise. It is a density-based clustering algorithm. It is a popular clustering algorithm because it does not require the number of clusters to be specified. It can discover clusters of arbitrary shapes. It can also identify outliers in the data.

The algorithm has two parameters:

𝜖
: maximum distance between two points for them to be considered as in the same neighborhood.

𝑚
: minimum number of points required to form a dense region.

The algorithm works as follows:

It starts with an arbitrary point in the data set that has not been visited.

It finds all of the points in the neighborhood of the point, using a distance measure 
𝜖
.

If there are at least 
𝑚
 points in the neighborhood, it starts a cluster with the initial point as its first member. It also visits all of the points in the neighborhood and adds them to the cluster.

If there are less than 
𝑚
 points in the neighborhood, the point is labeled as noise.

If a point is part of a cluster, its neighborhood is also part of that cluster. Hence, all of the points in the neighborhood are added to the cluster.

The algorithm repeats steps 1 to 5 until all of the points have been visited.




Note that despite the random initialization, DBSCAN is a deterministic algorithm. That is, it always produces the same result on the same data set.

9.1.2.1. Before Clustering
X = datasets['noisy_circles']
print(X.head())
sns.scatterplot(data=X, x='x1', y='x2');

         x1        x2
0 -0.469276  0.210118
1 -0.164164  0.986075
2 -0.471454  0.019974
3 -0.670347 -0.657977
4 -0.826468  0.594071

9.1.2.2. Clustering

DBSCAN is implemented in the DBSCAN class from the sklearn.cluster module.

The constructor of the DBSCAN class takes the two parameters 
𝜖
 and 
𝑚
 as input.

Similar to KMeans, the DBSCAN class has a fit() method that takes the data as input and runs the DBSCAN algorithm on it.

After we fit the model to the data, we can use the .labels_ attribute to get the discovered labels of the clusters assigned to each data point.

from sklearn.cluster import DBSCAN

dbscan = DBSCAN(eps=0.2, min_samples=5)
dbscan.fit(X)
X['label'] = dbscan.labels_

9.1.2.3. After clustering

The code below plots the data again, but this time with the cluster labels.

print(X.head())

sns.scatterplot(data=X, x='x1', y='x2', hue='label', palette='viridis');

         x1        x2  label
0 -0.469276  0.210118      0
1 -0.164164  0.986075      1
2 -0.471454  0.019974      0
3 -0.670347 -0.657977      1
4 -0.826468  0.594071      1


Just because DBSCAN does better than k-means on the circles dataset, it does not mean that DBSCAN is always better than k-means. Each clustering algorithm has its own strengths and weaknesses.

9.1.3. Comparing DBSCAN and K-means
fnames = ["aniso", "blobs", "no_structure", "noisy_circles", "noisy_moons", "varied"]

datasets = {}

fig, axs = plt.subplots(3, len(fnames), figsize=(17, 10))

for i, fname in enumerate(fnames):
    df = pd.read_csv(url + fname + ".csv", index_col=0)
    df.columns = ['x1', 'x2']
    kmeans = KMeans(n_clusters=3, random_state=0)
    kmeans.fit(df)
    kmeans_labels = kmeans.labels_
    dbscan = DBSCAN(eps=0.2, min_samples=5)
    dbscan.fit(df)
    dbscan_labels = dbscan.labels_
    
    ax = sns.scatterplot(data=df, x='x1', y='x2', ax=axs[0][i]);
    ax.set(title=fname)
    ax = sns.scatterplot(data=df, x='x1', y='x2', hue=kmeans_labels, ax=axs[1][i],  palette='viridis');
    ax = sns.scatterplot(data=df, x='x1', y='x2', hue=dbscan_labels, ax=axs[2][i],  palette='viridis');

    axs[0][0].set(title=fname, ylabel='No Clustering')
    axs[1][0].set(ylabel='KMeans (k=3)')
    axs[2][0].set(ylabel='DBSCAN (eps=0.2, min_samples=5)')


Note that the two algorithms work better on different datasets.

Furthermore, the parameters of the two algorithms (
𝑘
 for nearest neighbor and 
𝜖
 and 
𝑚
 for DBSCAN) need to be tuned to get the best results for an individual datasets.

9.1.4. Limitations of Clustering

Note that not all data sets are suitable for clustering. Some data sets do not have a well-defined cluster structure.

For example, below we try the k-means algorithm on the sentiments dataset. We know that the data set has three classes: positive, negative, and neutral. However, the k-means algorithm fails to discover the three classes. This is because the data set does not have a well-defined cluster structure.

import pandas as pd
data = pd.read_csv('https://raw.githubusercontent.com/fahadsultan/csc272/main/data/chat_dataset.csv')
data.head()

	message	sentiment
0	I really enjoyed the movie	positive
1	The food was terrible	negative
2	I'm not sure how I feel about this	neutral
3	The service was excellent	positive
4	I had a bad experience	negative
vocab = ' '.join(data['message'].values).lower().split()
vocab = list(set(vocab))

bow = pd.DataFrame(0, columns=vocab, index=data.index)
for word in vocab:
    bow[word] = data['message'].apply(lambda x: x.lower().split().count(word))

kmeans = KMeans(n_clusters=3, random_state=0)

def l2_norm(x):
    return (sum(x**2))**(1/2)

bow = bow.apply(lambda x: x/l2_norm(x), axis=1)
kmeans.fit(bow)

data['label'] = kmeans.labels_

data['label'].value_counts()

1    201
2    199
0    184
Name: label, dtype: int64

data.groupby(['label', 'sentiment']).size()

label  sentiment
0      negative      50
       neutral       72
       positive      62
1      negative      69
       neutral       70
       positive      62
2      negative      28
       neutral      117
       positive      54
dtype: int64

9.1.5. Relationship between Clusters and Labels

Please take caution in comparing the discovered clusters with any available labels for a dataset.

In clustering, the label ‘values’ are arbitrary. For example, if we have a dataset with three classes, we can label them as 0, 1, and 2 or as 1, 2, and 3 or as 100, 200, and 300.

from sklearn.datasets import load_iris

data = load_iris(as_frame=True)
X    = data['data']

kmeans = KMeans(n_clusters=3, random_state=0)
kmeans.fit(X)
cluster_labels = kmeans.labels_

y    = data['target']

sum(cluster_labels == y)/len(y)

0.24

fig, axs = plt.subplots(1, 2, figsize=(10, 5))

ax = sns.scatterplot(data=X, \
                x='sepal length (cm)', \
                y='sepal width (cm)', \
                hue=y, \
                palette='viridis', ax=axs[0]);
ax.set(title="True labels");

ax = sns.scatterplot(data=X, \
                x='sepal length (cm)', \
                y='sepal width (cm)', \
                hue=cluster_labels, \
                palette='viridis', ax=axs[1]);
ax.set(title="Cluster labels");

9.1.6. Silhouette Score to identify 
𝑘
, 
𝜖
 and min_samples
X = datasets['noisy_circles']
ax = sns.scatterplot(data=X, x='x1', y='x2');
ax.set(title="Noisy Circles");

plt.figure();
X = datasets['blobs']
ax = sns.scatterplot(data=X, x='x1', y='x2');
ax.set(title="Blobs");

 
9.1.7. Plotting Silhouette score to find k
sscores = {'noisy_circles':[], 'blobs':[]}
ks = [2, 3, 4, 5, 10, 15]
url = "https://raw.githubusercontent.com/fahadsultan/csc272/main/data/clusters/"

for name in ['noisy_circles', 'blobs']:

    X = pd.read_csv(url + name + ".csv", index_col=0)

    for k in ks:
        kmeans = KMeans(n_clusters=k, random_state=0)
        kmeans.fit(X)
        score = silhouette_score(X, kmeans.labels_)
        sscores[name].append(score)

ax = sns.lineplot(x=ks, y=sscores['noisy_circles'], marker='s');
ax = sns.lineplot(x=ks, y=sscores['blobs'], marker='s');
ax.set(xlabel='k', ylabel='silhouette_score');

plt.grid()
plt.legend(['noisy_circles', 'blobs']);
plt.title('Because K-Means does not work for `noisy circles` data, \nSilhouette Score never reaches close to 1 for any `k`');

from matplotlib import pyplot as plt
from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score


epsilons = [0.1, 0.2, 0.5, 1, 2, 5]

fig, axs = plt.subplots(1, len(epsilons), figsize=(17, 3))


X = datasets['blobs']
sscores = []
for i, e in enumerate(epsilons):
    dbscan = DBSCAN(eps=e, min_samples=2)
    dbscan.fit(X[['x1', 'x2']])
    score = silhouette_score(X, dbscan.labels_)
    sscores.append(score)
    # sns.scatterplot(data=X, x='x1', y='x2', hue=dbscan.labels_)
    axs[i].scatter(X['x1'], X['x2'], c=dbscan.labels_)
    axs[i].set_title("epsilon = "+ str(e))

plt.figure();
ax = sns.lineplot(x=epsilons, y=sscores, marker='s');
ax.set(xlabel='eps', ylabel='Silhouette Score');

plt.grid()
plt.legend(['noisy_circles']);
plt.title('Because K-Means does not work for `noisy circles` data, \nSilhouette Score never reaches close to 1 for any `k`');

 
sscores = {'noisy_circles':[], 'blobs':[]}
epsilons = [0.2, 0.3, 0.4, 0.5, 1.0, 1.5]
# epsilons = [0.1, 0.15, 0.2]

X = datasets['blobs']
# del X['label']
for e in epsilons:
    dbscan = DBSCAN(eps=e, min_samples=2)
    dbscan.fit(X)
    print(name, e, len(set(dbscan.labels_)))
    score = silhouette_score(X, dbscan.labels_)
    sscores[name].append(score)

# ax = sns.lineplot(x=epsilons, y=sscores['noisy_circles'], marker='s');
ax = sns.lineplot(x=epsilons, y=sscores['blobs'], marker='s');
ax.set(xlabel='eps', ylabel='Silhouette Score');

plt.grid()
plt.legend(['noisy_circles', 'blobs']);
plt.title('For Circles dataset, Silhouette Score never reaches close to 1 for either KMeans or DBSCAN');




URL: https://fahadsultan.com/csc272/3_problems_in/30_reinforcement.html


10. Reinforcement Learning



URL: https://fahadsultan.com/csc272/4_models/graphical.html


11. Graphical Models

Specifying a joint distribution over even a handful of random variables can become very daunting very fast. When we consider the fact that a typical medical- diagnosis problem has dozens or even hundreds of relevant attributes, the problem appears completely intractable.

Probabilistic graphical models, also known as Bayesian networks, provide a framework for exploiting structure in complex distributions so they can be described compactly, and in a way that allows them to be constructed and utilized effectively.

Probabilistic graphical models use a graph-based representation as the basis for compactly encoding a complex distribution over a high-dimensional space. In this graphical representation, the nodes (or ovals) correspond to the variables in our domain, and the edges correspond to direct probabilistic dependencies between the connected variables.

Popular examples of probabilistic graphical models include Naive Bayes, Markov Chains and Hidden Markov Models.



URL: https://fahadsultan.com/csc272/4_models/41_naivebayes.html


11.1. Naive Bayes

Naive Bayes are a set of models for supervised learning based on applying Bayes’ theorem with a “naive” assumption.

Naive Bayes models are generative and probabilistic.

Generative models are ones that make an assumption about how the data is generated. Probabilistic models are ones that make an assumption about the probability distribution of the data. A probabilistic models tell us the probability of the observation being in each class. This full distribution over the classes can be useful information for downstream decisions; avoiding making discrete decisions early on can be useful when combining system

11.1.1. Bayes’ Theorem

Recall Bayes’ Theorem from probability theory:

𝑃
𝑜
𝑠
𝑡
𝑒
𝑟
𝑖
𝑜
𝑟
=
𝐿
𝑖
𝑘
𝑒
𝑙
𝑖
ℎ
𝑜
𝑜
𝑑
⋅
𝑃
𝑟
𝑖
𝑜
𝑟
𝐸
𝑣
𝑖
𝑑
𝑒
𝑛
𝑐
𝑒

or

𝑃
(
𝑦
|
𝑋
)
=
𝑃
(
𝑋
|
𝑦
)
𝑃
(
𝑦
)
𝑃
(
𝑋
)

Here 
𝑋
 is multi-dimensional input data (say Bag of Words representation of a set of documents) whereas 
𝑦
 is the corresponding set of labels (say each document’s sentiment).

11.1.2. Naive Assumption

Note that the likelihood term 
𝑃
(
𝑋
|
𝑦
)
 in the Bayes theorem can get very complicated to compute, for high dimensional data where X may be composed of many features.

Naive Bayes’ makes a simplifying naive assumption that the features are independent of each other, given the class. In other words,



𝑃
(
𝑋
|
𝑦
)
=
∏
𝑖
=
1
𝐷
𝑃
(
𝑥
𝑖
|
𝑦
)

This is a naive assumption, since in most cases, features are not independent. This is especially not true for text data, where the presence of a word in a document is highly correlated with the presence of other words in the document. Hence the popular adage in NLP: “You shall know a word by the company it keeps” by Firth.

However, the naive assumption makes the computation of the likelihood term tractable and still remarkably yields great results in practice.

Naive Bayes model is a generative model since it makes an assumption about how the data is generated. It assumes that the data is generated by first sampling a class 
𝑦
 from the prior distribution 
𝑃
(
𝑦
)
 and then sampling each feature 
𝑥
𝑖
 from the likelihood distribution 
𝑃
(
𝑥
𝑖
|
𝑦
)
.

11.1.3. Bag of Words (BOW) as Multinomial Data

Recall that multinomial distribution counts the number of times an outcome occurs when there are k-possible outcomes and N independent trials that can be used to capture probability of counts for rolling a k-sided die n times. Also recall that BOW representation of text is a count of how many times a word occurs in a document.

BOW representation, therefore, can be modeled using the multinomial distibution. Here, instead of k-sided die, imagine a 
|
𝑉
|
 sided die where 
𝑉
 is the set of all unique words (vocabulary) and 
|
𝑉
|
 is the size of the vocabulary. Each word in the vocabulary, in other words, is a possible outcome and count of a word in all documents constitutes a binomial distribution of that word.

So we can use the multinomial distribution to model the likelihood term 
𝑃
(
𝑥
𝑖
|
𝑦
)
 in the prediction rule.

However, the Naive Assumption in the Naive Bayes model alleviates the need to estimate the joint distribution of the features. Instead, we can estimate the parameters of the binomial distribution for each feature independently and simply multiply the probabilities of each feature to get the likelihood term.

The parameters of the binomial distribution are estimated using Maximum Likelihood Estimation (MLE) or Maximum A Posteriori (MAP) estimation.

11.1.4. Multinomial Naive Bayes Classifier

Naive Bayes classifier is simply a probabilistic classifier where the prediction 
𝑦
^
 is the class 
𝑦
 that maximizes the posterior probability 
𝑃
(
𝑦
|
𝑋
)
 i.e.



𝑦
^
=
argmax
𝑦
𝑃
(
𝑦
|
𝑋
)

In other words, if y=0 maximizes P(y | X), then the predicted class is 0. Otherwise, if y=1 maximizes P(y | X), then the predicted class is 1.

P(y | X) is proportional to P(X | y)P(y), as per the Bayes’ theorem. In other words, we can ignore the denominator P(X) since it is the same for all classes (values of 
y
).

So, we can also write the prediction rule as:



𝑦
^
=
argmax
𝑦
𝑃
(
𝑋
|
𝑦
)
⋅
𝑃
(
𝑦
)

If we make the naive assumption that the features are independent of each other, given the class, then we can write the prediction rule as:





𝑦
^
=
argmax
𝑦
∏
𝑖
=
1
𝐷
𝑃
(
𝑥
𝑖
|
𝑦
)
⋅
𝑃
(
𝑦
)

Based on our conversation on logarithms, when we apply log to the equation above, two things happen:

Due to log being a monotonically increasing function, the outcome of argmax remains unchanged.

All multiplications in the equation become additions, making our lives easier.

Therefore, we can write the prediction rule as:





𝑦
^
=
argmax
𝑦
∑
𝑖
=
1
𝐷
log
⁡
𝑃
(
𝑥
𝑖
|
𝑦
)
+
log
⁡
𝑃
(
𝑦
)

Now, let’s look at the two terms in the equation above:

𝑃
(
𝑦
)
 is the prior probability of the class 
𝑦
. It is estimated as the fraction of documents in the training set that belong to class 
𝑦
.

𝑃
(
𝑥
𝑖
|
𝑦
)
 is the likelihood term. It is the sum of the log probabilities of each 
𝑥
𝑖
 
𝑋
 given the class 
𝑦
.

If, for example, we are trying to predict the sentiment of the document “Furman University” then the likelihood term would be 
log
 
𝑃
(
Furman
|
𝑦
)
+
log
 
𝑃
(
University
|
𝑦
)
.

Similarly, likelihood term for the document “Greeenville is in SC” would be 
log
 
𝑃
(
Greenville
|
𝑦
)
+
log
 
𝑃
(
is
|
𝑦
)
+
log
 
𝑃
(
in
|
𝑦
)
+
log
 
𝑃
(
SC
|
𝑦
)
.



URL: https://fahadsultan.com/csc272/4_models/generalized.html


12. Generalized Linear Model



URL: https://fahadsultan.com/csc272/4_models/nn.html


13. Neural Networks



URL: https://fahadsultan.com/csc272/5_algos/em.html


14. Expectation Maximization



URL: https://fahadsultan.com/csc272/5_algos/gradient_descent.html


15. Gradient Descent



URL: https://fahadsultan.com/csc272/5_algos/backprop.html


16. Backpropogation



URL: https://fahadsultan.com/csc272/6_problems_with/10_representation.html


17. Representation Matters



URL: https://fahadsultan.com/csc272/6_problems_with/40_feedback.html


18. Positive Feedback Loops



URL: https://fahadsultan.com/csc272/6_problems_with/20_boundaries.html


19. Decision Boundaries



URL: https://fahadsultan.com/csc272/6_problems_with/30_incentives.html


20. Majority Rule



URL: https://fahadsultan.com/csc272/7_applications/40_recommendation.html


21. Recommendation Engine
print('Hello world')

Hello world




